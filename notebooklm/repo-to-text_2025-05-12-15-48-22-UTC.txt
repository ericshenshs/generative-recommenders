<repo-to-text>
Directory: generative-recommenders

Directory Structure:
<directory_structure>
.
├── .gitignore

</directory_structure>

<content full_path="CODE_OF_CONDUCT.md">
# Code of Conduct

## Our Pledge

In the interest of fostering an open and welcoming environment, we as
contributors and maintainers pledge to make participation in our project and
our community a harassment-free experience for everyone, regardless of age, body
size, disability, ethnicity, sex characteristics, gender identity and expression,
level of experience, education, socio-economic status, nationality, personal
appearance, race, religion, or sexual identity and orientation.

## Our Standards

Examples of behavior that contributes to creating a positive environment
include:

* Using welcoming and inclusive language
* Being respectful of differing viewpoints and experiences
* Gracefully accepting constructive criticism
* Focusing on what is best for the community
* Showing empathy towards other community members

Examples of unacceptable behavior by participants include:

* The use of sexualized language or imagery and unwelcome sexual attention or
advances
* Trolling, insulting/derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or electronic
address, without explicit permission
* Other conduct which could reasonably be considered inappropriate in a
professional setting

## Our Responsibilities

Project maintainers are responsible for clarifying the standards of acceptable
behavior and are expected to take appropriate and fair corrective action in
response to any instances of unacceptable behavior.

Project maintainers have the right and responsibility to remove, edit, or
reject comments, commits, code, wiki edits, issues, and other contributions
that are not aligned to this Code of Conduct, or to ban temporarily or
permanently any contributor for other behaviors that they deem inappropriate,
threatening, offensive, or harmful.

## Scope

This Code of Conduct applies within all project spaces, and it also applies when
an individual is representing the project or its community in public spaces.
Examples of representing a project or community include using an official
project e-mail address, posting via an official social media account, or acting
as an appointed representative at an online or offline event. Representation of
a project may be further defined and clarified by project maintainers.

This Code of Conduct also applies outside the project spaces when there is a
reasonable belief that an individual's behavior may have a negative impact on
the project or its community.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported by contacting the project team at <opensource-conduct@meta.com>. All
complaints will be reviewed and investigated and will result in a response that
is deemed necessary and appropriate to the circumstances. The project team is
obligated to maintain confidentiality with regard to the reporter of an incident.
Further details of specific enforcement policies may be posted separately.

Project maintainers who do not follow or enforce the Code of Conduct in good
faith may face temporary or permanent repercussions as determined by other
members of the project's leadership.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,
available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html

[homepage]: https://www.contributor-covenant.org

For answers to common questions about this code of conduct, see
https://www.contributor-covenant.org/faq

</content>

<content full_path="LICENSE">

                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright [yyyy] [name of copyright owner]

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.

</content>

<content full_path="requirements.txt">
absl-py==2.1.0
astunparse==1.6.3
certifi==2024.2.2
charset-normalizer==3.3.2
contourpy==1.2.1
cycler==0.12.1
fbgemm-gpu==0.6.0
filelock==3.13.4
flatbuffers==24.3.25
fonttools==4.51.0
fsspec==2024.3.1
fvcore==0.1.5.post20221221
gast==0.5.4
gin-config==0.5.0
google-pasta==0.2.0
grpcio==1.62.1
h5py==3.11.0
iopath==0.1.10
joblib==1.4.0
keras==3.2.0
kiwisolver==1.4.5
libclang==18.1.1
Markdown==3.6
markdown-it-py==3.0.0
MarkupSafe==2.1.5
matplotlib==3.8.4
mdurl==0.1.2
ml-dtypes==0.3.2
mpmath==1.3.0
namex==0.0.7
networkx==3.3
numpy==1.26.4
nvidia-cublas-cu12==12.1.3.1
nvidia-cuda-cupti-cu12==12.1.105
nvidia-cuda-nvrtc-cu12==12.1.105
nvidia-cuda-runtime-cu12==12.1.105
nvidia-cudnn-cu12==8.9.2.26
nvidia-cufft-cu12==11.0.2.54
nvidia-curand-cu12==10.3.2.106
nvidia-cusolver-cu12==11.4.5.107
nvidia-cusparse-cu12==12.1.0.106
nvidia-nccl-cu12==2.19.3
nvidia-nvjitlink-cu12==12.4.127
nvidia-nvtx-cu12==12.1.105
opt-einsum==3.3.0
optree==0.11.0
packaging==24.0
pandas==2.2.1
pillow==10.3.0
portalocker==2.8.2
protobuf==4.25.3
Pygments==2.17.2
pyparsing==3.1.2
python-dateutil==2.9.0.post0
pytz==2024.1
PyYAML==6.0.1
rich==13.7.1
scipy==1.13.0
six==1.16.0
sympy==1.12
tabulate==0.9.0
tensorboard==2.16.2
tensorboard-data-server==0.7.2
tensorflow==2.16.1
tensorflow-io-gcs-filesystem==0.36.0
tensorrt==8.6.1.post1
tensorrt-bindings==8.6.1
tensorrt-libs==8.6.1
termcolor==2.4.0
threadpoolctl==3.4.0
torch==2.2.2
torch_tensorrt==2.2.0
torchaudio==2.2.2
torchvision==0.17.2
triton==2.2.0
typing_extensions==4.11.0
tzdata==2024.1
wrapt==1.16.0
yacs==0.1.8

</content>

<content full_path="run_fractal_expansion.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# pyre-unsafe

"""
Run fractal expansion introduced in https://arxiv.org/abs/1901.08910.
Implementation adapted from the scripts used to generate MovieLens-1B
(https://grouplens.org/datasets/movielens/movielens-1b/).
"""

# Generate a 3B dataset (takes around 50 minutes):
# python run_fractal_expansion.py --input-csv-file tmp/ml-20m/ratings.csv --write-dataset True --output-prefix tmp/ml-3b/

import csv
import logging
import os
import pickle

from dataclasses import dataclass

import click
import matplotlib.pyplot as plt

import numpy as np
import pandas as pd
import scipy.linalg
import skimage.transform as transform
from scipy import sparse
from scipy.sparse import linalg
from sklearn.utils import shuffle
from tqdm import tqdm


logging.basicConfig()
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)


@dataclass
class SparseMatrixMetadata:
    num_interactions: int = 0
    num_rows: int = 0
    num_cols: int = 0


def _dropout_sparse_coo_matrix(
    sparse_matrix, rate, min_dropout_rate=0.05, max_dropout_rate=0.99
):
    assert min_dropout_rate <= max_dropout_rate
    sampling_rate = 1.0 - rate

    sampled_fraction = min(
        max(sampling_rate, 1.0 - max_dropout_rate), 1.0 - min_dropout_rate
    )
    if sampled_fraction != sampling_rate:
        logger.warning(
            f"Desired sampling rate {sampling_rate} clipped to {sampled_fraction}."
        )
    num_sampled = min(
        max(int(sparse_matrix.nnz * sampled_fraction), 1), sparse_matrix.nnz
    )
    sampled_indices = np.random.choice(
        sparse_matrix.nnz, size=num_sampled, replace=False
    )
    return sparse.coo_matrix(
        (
            sparse_matrix.data[sampled_indices],
            (sparse_matrix.row[sampled_indices], sparse_matrix.col[sampled_indices]),
        ),
        shape=sparse_matrix.shape,
    )


def shuffle_sparse_matrix(
    sparse_matrix, dropout_rate=0.0, min_dropout_rate=0.05, max_dropout_rate=0.99
):
    """
    Shuffle sparse matrix encoded as a SciPy csr matrix.
    """

    assert dropout_rate >= 0.0 and dropout_rate <= 1.0
    (num_rows, num_cols) = sparse_matrix.shape
    shuffled_rows = shuffle(np.arange(num_rows))
    shuffled_cols = shuffle(np.arange(num_cols))
    sparse_matrix = _dropout_sparse_coo_matrix(
        sparse_matrix, dropout_rate, min_dropout_rate, max_dropout_rate
    )
    new_row = np.take(shuffled_rows, sparse_matrix.row)
    new_col = np.take(shuffled_cols, sparse_matrix.col)
    return sparse.csr_matrix(
        (sparse_matrix.data, (new_row, new_col)), shape=(num_rows, num_cols)
    )


def graph_reduce(usv, num_rows, num_cols):
    """Apply algorithm 2 in https://arxiv.org/pdf/1901.08910.pdf."""

    def _closest_column_orthogonal_matrix(matrix):
        return np.matmul(
            matrix, np.linalg.inv(scipy.linalg.sqrtm(np.matmul(matrix.T, matrix)))
        )

    u, s, v = usv
    k = min(num_rows, num_cols)
    u_random_proj = transform.resize(u[:, :k], (num_rows, k))
    v_random_proj = transform.resize(v[:k, :], (k, num_cols))
    u_random_proj_orth = _closest_column_orthogonal_matrix(u_random_proj)
    v_random_proj_orth = _closest_column_orthogonal_matrix(v_random_proj.T).T
    return np.matmul(u_random_proj_orth, np.matmul(np.diag(s[:k]), v_random_proj_orth))


def rescale(matrix, rescale_w_abs=False):
    """Rescale all values of the matrix into [0, 1]."""
    if rescale_w_abs:
        abs_matrix = np.abs(matrix.copy())
        return abs_matrix / abs_matrix.max()
    else:
        out = (matrix - matrix.min()) / (matrix.max() - matrix.min())
        assert out.min() >= 0 and out.max() <= 1
        return out


def _compute_row_block(
    i, left_matrix, right_matrix, indices_out_path, remove_empty_rows
):
    """Compute row block of expansion for row i of the left_matrix."""

    kron_blocks = []
    num_rows = 0
    num_removed_rows = 0
    num_interactions = 0

    for j in range(left_matrix.shape[1]):
        dropout_rate = 1.0 - left_matrix[i, j]
        kron_block = shuffle_sparse_matrix(right_matrix, dropout_rate).tocsr()
        num_interactions += kron_block.nnz
        kron_blocks.append(kron_block)
        logger.info(f"Kronecker block ({i}, {j}) processed.")

    rows_to_write = sparse.hstack(kron_blocks).tocsr()
    logger.info("Writing dataset row by row.")

    # Write Kronecker product line per line.
    filepath = f"{indices_out_path}_{i}.csv"
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    with open(filepath, "w", newline="") as file:
        writer = csv.writer(file)
        for k in range(right_matrix.shape[0]):
            items_to_write = rows_to_write.getrow(k).indices
            ratings_to_write = rows_to_write.getrow(k).data
            num = items_to_write.shape[0]
            if remove_empty_rows and (not num):
                logger.info(f"Removed empty output row {i * left_matrix.shape[0] + k}.")
                num_removed_rows += 1
                continue
            num_rows += 1
            writer.writerow(
                [
                    i * right_matrix.shape[0] + k,
                    ",".join([str(x) for x in items_to_write]),
                    ",".join([str(x) for x in ratings_to_write]),
                ]
            )
            if k % 100000 == 0:
                logger.info(f"Done producing data set row {k}.")

    num_cols = rows_to_write.shape[1]
    metadata = SparseMatrixMetadata(
        num_interactions=num_interactions, num_rows=num_rows, num_cols=num_cols
    )
    logger.info(
        f"Done with left matrix row {i}, {num_interactions} interactions written in shard, {num_removed_rows} rows removed in shard."
    )
    return (num_removed_rows, metadata)


def build_randomized_kronecker(
    left_matrix,
    right_matrix,
    indices_out_path,
    metadata_out_path=None,
    remove_empty_rows=True,
):
    """Compute randomized Kronecker product and dump it on the fly based on https://arxiv.org/pdf/1901.08910.pdf."""
    logger.info(f"Writing item sequences to pickle files {metadata_out_path}.")

    num_rows = 0
    num_removed_rows = 0
    num_cols = left_matrix.shape[1] * right_matrix.shape[1]
    num_interactions = 0

    filepath = f"{indices_out_path}_users.csv"
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    with open(filepath, "w", newline="") as file:
        writer = csv.writer(file)
        for i in tqdm(range(left_matrix.shape[0])):
            (shard_num_removed_rows, shard_metadata) = _compute_row_block(
                i, left_matrix, right_matrix, indices_out_path, remove_empty_rows
            )
            writer.writerow([i, shard_metadata.num_rows])
            file.flush()
            num_rows += shard_metadata.num_rows
            num_removed_rows += shard_num_removed_rows
            num_interactions += shard_metadata.num_interactions

    logger.info(f"{num_interactions / num_rows} average sequence length")
    logger.info(f"{num_interactions} total interactions written.")
    logger.info(f"{num_removed_rows} total rows removed.")

    metadata = SparseMatrixMetadata(
        num_interactions=num_interactions, num_rows=num_rows, num_cols=num_cols
    )
    if metadata_out_path is not None:
        logger.info(f"Writing metadata file to {metadata_out_path}")
        with open(metadata_out_path, "wb") as output_file:
            pickle.dump(metadata, output_file)
    return metadata


def _preprocess_movie_lens(ratings_df, binary=False):
    """
    Filters out users with less than three distinct timestamps.
    """

    def _create_index(df, colname):
        value_set = sorted(set(df[colname].values))
        num_unique = len(value_set)
        return dict(zip(value_set, range(num_unique)))

    if not binary:
        ratings_df["data"] = ratings_df["rating"]
    else:
        ratings_df["data"] = 1.0
    ratings_df["binary_data"] = 1.0
    num_timestamps = ratings_df[["userId", "timestamp"]].groupby("userId").nunique()
    ratings_df["numberOfTimestamps"] = ratings_df["userId"].apply(
        lambda x: num_timestamps["timestamp"][x]
    )
    ratings_df = ratings_df[ratings_df["numberOfTimestamps"] > 2]
    user_id_to_user_idx = _create_index(ratings_df, "userId")
    item_id_to_item_idx = _create_index(ratings_df, "movieId")
    ratings_df["row"] = ratings_df["userId"].apply(lambda x: user_id_to_user_idx[x])
    ratings_df["col"] = ratings_df["movieId"].apply(lambda x: item_id_to_item_idx[x])
    return ratings_df


def normalize(matrix):
    norm_matrix = matrix.copy()
    norm_matrix.data -= norm_matrix.mean()
    max_val = norm_matrix.max()
    min_val = norm_matrix.min()
    norm_matrix.data /= max(abs(max_val), abs(min_val))
    return norm_matrix


def plot_distribution(user_wise_sum, item_wise_sum, s, title_prefix, normalized=False):
    y_label = "rating sums" if normalized else "number of ratings"
    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))
    ax1.loglog(
        np.arange(len(user_wise_sum)) + 1,
        np.sort(user_wise_sum)[::-1],
        linestyle="-",
        color="blue",
        marker="",
    )
    ax1.set_title(f"{title_prefix} matrix user-wise rating sums")
    ax1.set_xlabel("User rank")
    ax1.set_ylabel(y_label)
    ax1.grid(True)
    ax2.loglog(
        np.arange(len(item_wise_sum)) + 1,
        np.sort(item_wise_sum)[::-1],
        linestyle="-",
        color="green",
        marker="",
    )
    ax2.set_title(f"{title_prefix} matrix item-wise rating sums")
    ax2.set_xlabel("Item rank")
    ax2.set_ylabel(y_label)
    ax2.grid(True)
    ax3.loglog(
        np.arange(len(s)) + 1, np.sort(s)[::-1], linestyle="-", color="red", marker=""
    )
    ax3.set_title(f"{title_prefix} matrix singular values")
    ax3.set_xlabel("Singular value Rank")
    ax3.set_ylabel("Magnitude")
    ax3.grid(True)
    plt.tight_layout()
    plt.show()


def visualize_distribution(mat, reduced_mat, s, reduced_s, normalized=False):
    user_wise_sum = np.asarray(mat.sum(axis=1)).flatten()
    item_wise_sum = np.asarray(mat.sum(axis=0)).flatten()
    assert len(user_wise_sum) == mat.shape[0]
    assert len(item_wise_sum) == mat.shape[1]
    plot_distribution(
        user_wise_sum, item_wise_sum, s, title_prefix="Original", normalized=normalized
    )

    reduced_user_wise_sum = np.asarray(reduced_mat.sum(axis=1)).flatten()
    reduced_item_wise_sum = np.asarray(reduced_mat.sum(axis=0)).flatten()
    assert len(reduced_user_wise_sum) == reduced_mat.shape[0]
    assert len(reduced_item_wise_sum) == reduced_mat.shape[1]
    plot_distribution(
        reduced_user_wise_sum,
        reduced_item_wise_sum,
        reduced_s,
        title_prefix="Reduced",
        normalized=normalized,
    )

    expanded_s = np.einsum("i,j->ij", reduced_s, s).flatten()
    expanded_user_wise_sum = np.einsum("ij,k->ik", reduced_mat, user_wise_sum).flatten()
    expanded_item_wise_sum = np.einsum("ij,k->jk", reduced_mat, item_wise_sum).flatten()
    assert len(expanded_user_wise_sum) == reduced_mat.shape[0] * mat.shape[0]
    assert len(expanded_item_wise_sum) == reduced_mat.shape[1] * mat.shape[1]
    plot_distribution(
        expanded_user_wise_sum,
        expanded_item_wise_sum,
        expanded_s,
        title_prefix="Expanded",
        normalized=normalized,
    )


def expand_dataset(
    ratings_matrix,
    binary_ratings_matrix,
    num_users,
    num_items,
    reduced_num_rows,
    reduced_num_cols,
    rescale_w_abs,
    visualize,
    write_dataset,
    output_prefix,
):
    k = min(reduced_num_rows, reduced_num_cols)
    norm_rating_matrix = normalize(ratings_matrix)
    (u, s, v) = linalg.svds(
        norm_rating_matrix, k=k, maxiter=None, return_singular_vectors=True
    )

    logger.info(
        f"Creating reduced rating matrix (size {reduced_num_rows}, {reduced_num_cols})"
    )
    reduced_matrix = graph_reduce((u, s, v), reduced_num_rows, reduced_num_cols)
    norm_reduced_matrix = normalize(reduced_matrix)
    (_, s_reduce, _) = linalg.svds(
        norm_reduced_matrix, k=k - 1, maxiter=None, return_singular_vectors=True
    )
    reduced_matrix = rescale(reduced_matrix, rescale_w_abs=rescale_w_abs)
    logger.info(f"largest singular value of the reduced matrix is {s_reduce[-1]}")
    logger.info(
        f"Sampling rate mean is {reduced_matrix.mean()}, var is {reduced_matrix.var()}, min is {reduced_matrix.min()}, max is {reduced_matrix.max()}"
    )
    samples = reduced_matrix.sum() * ratings_matrix.nnz
    logger.info(
        f"Expected number of synthetic samples: {samples}, sparsity is {samples / (num_users * num_items * reduced_num_rows * reduced_num_cols)}, average seqlen is {samples / (num_users * reduced_num_rows)}"
    )

    if visualize:
        s = linalg.svds(
            norm_rating_matrix, k=20 * k, maxiter=None, return_singular_vectors=False
        )
        visualize_distribution(
            norm_rating_matrix, norm_reduced_matrix, s, s_reduce, normalized=True
        )
        visualize_distribution(
            binary_ratings_matrix, reduced_matrix, s, s_reduce, normalized=False
        )
    if write_dataset:
        output_file = (
            output_prefix + str(reduced_num_rows) + "x" + str(reduced_num_cols)
        )
        output_file_metadata = None

        logger.info(f"Creating synthetic dataset and dumping to {output_file}.")
        build_randomized_kronecker(
            left_matrix=reduced_matrix,
            right_matrix=ratings_matrix.tocoo(),
            indices_out_path=output_file,
            metadata_out_path=output_file_metadata,
        )


@click.command()
@click.option(
    "--random-seed",
    type=int,
    default=0,
)
@click.option(
    "--input-csv-file",
    type=str,
    default="ratings.csv",
)
@click.option(
    "--output-prefix",
    type=str,
    default="",
)
@click.option(
    "--num-row-multiplier",
    type=int,
    default=16,
)
@click.option(
    "--num-col-multiplier",
    type=int,
    default=32,
)
@click.option(
    "--visualize",
    type=bool,
    default=False,
)
@click.option(
    "--write-dataset",
    type=bool,
    default=False,
)
def main(
    random_seed: int,
    input_csv_file: str,
    output_prefix: str,
    num_row_multiplier: int,
    num_col_multiplier: int,
    visualize: bool,
    write_dataset: bool,
):
    np.random.seed(random_seed)

    logger.info(f"Loading and preprocessing MovieLens-20m from {input_csv_file}")
    with open(input_csv_file, "r") as infile:
        ratings_df = pd.read_csv(infile, sep=",", header=0)
    ratings_df = _preprocess_movie_lens(ratings_df, binary=False)
    num_ratings = len(ratings_df)
    num_users = len(set(ratings_df["row"].values))
    num_items = len(set(ratings_df["col"].values))
    logger.info(
        f"number of ratings of input dataset is {num_ratings}, number of users is {num_users}, number of items is {num_items}, sparsity is {num_ratings / (num_users * num_items)}, average seqlen is {num_ratings / num_users}"
    )

    ratings_matrix = sparse.csr_matrix(
        (
            ratings_df["data"].values,
            (ratings_df["row"].values, ratings_df["col"].values),
        ),
        shape=(num_users, num_items),
    )
    binary_ratings_matrix = sparse.csr_matrix(
        (
            ratings_df["binary_data"].values,
            (ratings_df["row"].values, ratings_df["col"].values),
        ),
        shape=(num_users, num_items),
    )
    expand_dataset(
        ratings_matrix=ratings_matrix,
        binary_ratings_matrix=binary_ratings_matrix,
        num_users=num_users,
        num_items=num_items,
        reduced_num_rows=num_row_multiplier,
        reduced_num_cols=num_col_multiplier,
        rescale_w_abs=False,
        visualize=visualize,
        write_dataset=write_dataset,
        output_prefix=output_prefix,
    )


if __name__ == "__main__":
    main()

</content>

<content full_path="README.md">

# Why Clone?

The cloned repository is for understanding the original repository. We use Cursor IDE to add comments to the codebase, keeping the original codebase unchanged.


## Code Structure

The repository is organized as follows. And we use Cursor IDE to add comments to each file.

- [x] `configs/`: Configuration files for different datasets and model variants
  - `amzn-books/`: Configuration for Amazon Books dataset
  - `ml-1m/`: Configuration for MovieLens 1M dataset
  - `ml-3b/`: Configuration for MovieLens 3B dataset
  - `ml-20m/`: Configuration for MovieLens 20M dataset
- `generative_recommenders/`: Core model implementation
  - `data/`: Dataset loading, preprocessing and data utilities
  - `indexing/`: Efficient indexing and retrieval implementations
  - `modeling/`: Core model architectures and components
    - `sequential/`: Sequential model architectures and components for processing user interaction sequences
      - `losses/`: Loss functions and metrics for training sequential recommendation models
  - `ops/`: Custom CUDA/Triton operators for performance optimization
  - `rails/`: Guardrails and safety mechanisms for model deployment
  - `trainer/`: Training loop, optimization and evaluation code
    - [x] `data_loader.py`: Data loading and preprocessing
    - [x] `trainer.py`: Main training loop logic
  - `common.py`: Common utilities
- `main.py`: Main entry point for training and evaluation
- `preprocess_public_data.py`: Preprocess the public data
- [x] `README.md`: This file

# Generative Recommenders

Repository hosting code for ``Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations`` (https://arxiv.org/abs/2402.17152, ICML'24) and related code, where we demonstrate that the ubiquitously used classical deep learning recommendation paradigm (DLRMs) can be reformulated as a generative modeling problem (Generative Recommenders or GRs) to overcome known compute scaling bottlenecks, propose efficient algorithms such as HSTU and M-FALCON to accelerate training and inference for large-scale sequential models by 10x-1000x, and demonstrate scaling law for the first-time in deployed, billion-user scale recommendation systems.

Currently only code for reproducing public experiments listed in the paper (Section 4.1.1) and Triton kernels for efficiency experiments (Section 4.2) are included. We plan to add integration code needed for throughput/performance benchmarks at a later point in time.

## Getting started

### Public experiments

To reproduce the public experiments (traditional sequential recommender setting, Section 4.1.1) on MovieLens and Amazon Reviews in the paper, please follow these steps:

#### Install dependencies.

(Optional) Use conda to install dependencies.

```bash
conda create --name generative-recommenders python=3.10 pytorch
conda activate generative-recommenders
```

Install PyTorch based on official instructions. Then,

```bash
pip3 install gin-config absl-py scikit-learn scipy matplotlib numpy apex hypothesis pandas fbgemm_gpu iopath tensorboard
```

#### Download and preprocess data.

```bash
mkdir -p tmp/ && python3 preprocess_public_data.py
```

#### Run model training.

A GPU with 24GB or more HBM should work for most datasets.

```bash
CUDA_VISIBLE_DEVICES=0 python3 main.py --gin_config_file=configs/ml-1m/hstu-sampled-softmax-n128-large-final.gin --master_port=12345
```

Other configurations are included in configs/ml-1m, configs/ml-20m, and configs/amzn-books to make reproducing these experiments easier.

#### Verify results.

By default we write experimental logs to exps/. We can launch tensorboard with something like the following:

```bash
tensorboard --logdir ~/generative-recommenders/exps/ml-1m-l200/ --port 24001 --bind_all
tensorboard --logdir ~/generative-recommenders/exps/ml-20m-l200/ --port 24001 --bind_all
tensorboard --logdir ~/generative-recommenders/exps/amzn-books-l50/ --port 24001 --bind_all
```

With the provided configuration (.gin) files, you should be able to reproduce the following results (verified as of 04/15/2024):

**MovieLens-1M (ML-1M)**:

| Method        | HR@10            | NDCG@10         | HR@50           | NDCG@50         | HR@200          | NDCG@200        |
| ------------- | ---------------- | ----------------| --------------- | --------------- | --------------- | --------------- |
| SASRec        | 0.2853           | 0.1603          | 0.5474          | 0.2185          | 0.7528          | 0.2498          |
| BERT4Rec      | 0.2843 (-0.4%)   | 0.1537 (-4.1%)  |                 |                 |                 |                 |
| GRU4Rec       | 0.2811 (-1.5%)   | 0.1648 (+2.8%)  |                 |                 |                 |                 |
| HSTU          | 0.3097 (+8.6%)   | 0.1720 (+7.3%)  | 0.5754 (+5.1%)  | 0.2307 (+5.6%)  | 0.7716 (+2.5%)  | 0.2606 (+4.3%)  |
| HSTU-large    | **0.3294 (+15.5%)**  | **0.1893 (+18.1%)** | **0.5935 (+8.4%)**  | **0.2481 (+13.5%)** | **0.7839 (+4.1%)**  | **0.2771 (+10.9%)** |

**MovieLens-20M (ML-20M)**:

| Method        | HR@10            | NDCG@10         | HR@50           | NDCG@50         | HR@200          | NDCG@200        |
| ------------- | ---------------- | --------------- | --------------- | --------------- | --------------- | --------------- |
| SASRec        | 0.2889           | 0.1621          | 0.5503          | 0.2199          | 0.7661          | 0.2527          |
| BERT4Rec      | 0.2816 (-2.5%)   | 0.1703 (+5.1%)  |                 |                 |                 |                 |
| GRU4Rec       | 0.2813 (-2.6%)   | 0.1730 (+6.7%)  |                 |                 |                 |                 |
| HSTU          | 0.3273 (+13.3%)  | 0.1895 (+16.9%) | 0.5889 (+7.0%)  | 0.2473 (+12.5%) | 0.7952 (+3.8%)  | 0.2787 (+10.3%) |
| HSTU-large    | **0.3556 (+23.1%)**  | **0.2098 (+29.4%)** | **0.6143 (+11.6%)** | **0.2671 (+21.5%)** | **0.8074 (+5.4%)**  | **0.2965 (+17.4%)** |

**Amazon Reviews (Books)**:

| Method        | HR@10            | NDCG@10         | HR@50           | NDCG@50         | HR@200          | NDCG@200        |
| ------------- | ---------------- | ----------------|---------------- | --------------- | --------------- | --------------- |
| SASRec        | 0.0306           | 0.0164          | 0.0754          | 0.0260          | 0.1431          | 0.0362          |
| HSTU          | 0.0416 (+36.4%)  | 0.0227 (+39.3%) | 0.0957 (+27.1%) | 0.0344 (+32.3%) | 0.1735 (+21.3%) | 0.0461 (+27.7%) |
| HSTU-large    | **0.0478 (+56.7%)**  | **0.0262 (+60.7%)** | **0.1082 (+43.7%)** | **0.0393 (+51.2%)** | **0.1908 (+33.4%)** | **0.0517 (+43.2%)** |

for all three tables above, the ``SASRec`` rows are based on [Self-Attentive Sequential Recommendation](https://arxiv.org/abs/1808.09781) but with the original binary cross entropy loss
replaced with sampled softmax losses proposed in [Revisiting Neural Retrieval on Accelerators](https://arxiv.org/abs/2306.04039). These rows are reproducible with ``configs/*/sasrec-*-final.gin``.
The ``BERT4Rec`` and ``GRU4Rec`` rows are based on results reported by [Turning Dross Into Gold Loss: is BERT4Rec really better than SASRec?](https://arxiv.org/abs/2309.07602) -
note that the comparison slightly favors these two, due to them using full negatives whereas the other rows used 128/512 sampled negatives. The ``HSTU`` and ``HSTU-large`` rows are based on [Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations](https://arxiv.org/abs/2402.17152); in particular, HSTU rows utilize identical configurations as SASRec. ``HSTU`` and ``HSTU-large`` results can be reproduced with ``configs/*/hstu-*-final.gin``.


#### Use synthetic dataset MovieLens-3B
We support generating synthetic dataset with fractal expansion introduced in https://arxiv.org/abs/1901.08910. This allows us to expand the current 20 million real-world ratings from ML-20M to 3 billion.

To download the pre-generated synthetic dataset:

```bash
pip3 install gdown
mkdir -p tmp/ && cd tmp/
gdown https://drive.google.com/uc?id=1-jZ6k0el7e7PyFnwqMLfqUTRh_Qdumt-
unzip ml-3b.zip && rm ml-3b.zip
```

To generate the synthetic dataset on your own:

```bash
python run_fractal_expansion.py --input-csv-file tmp/ml-20m/ratings.csv --write-dataset True --output-prefix tmp/ml-3b/
```

### Efficiency experiments

``ops/triton`` currently contains triton kernels needed for efficiency experiments (forward pass). More code (incl integration glue code) to be added at a later point in time. If it's urgent, please feel free to open PRs.


## License
This codebase is Apache 2.0 licensed, as found in the [LICENSE](LICENSE) file.


## Contributors
The overall project is made possible thanks to the joint work from many technical contributors (listed in alphabetical order):

Adnan Akhundov, Bugra Akyildiz, Shabab Ayub, Alex Bao, Renqin Cai, Jennifer Cao, Xuan Cao, Guoqiang Jerry Chen, Lei Chen, Sean Chen, Xianjie Chen, Huihui Cheng, Weiwei Chu, Ted Cui, Shiyan Deng, Nimit Desai, Fei Ding, Shilin Ding, Francois Fagan, Lu Fang, Leon Gao, Zhaojie Gong, Fangda Gu, Liang Guo, Liz Guo, Jeevan Gyawali, Yuchen Hao, Daisy Shi He, Michael Jiayuan He, Samuel Hsia, Jie Hua, Yanzun Huang, Hongyi Jia, Rui Jian, Jian Jin, Rahul Kindi, Changkyu Kim, Yejin Lee, Fu Li, Han Li, Hong Li, Shen Li, Rui Li, Wei Li, Zhijing Li, Lucy Liao, Xueting Liao, Emma Lin, Hao Lin, Chloe Liu, Jingzhou Liu, Xing Liu, Xingyu Liu, Kai Londenberg, Yinghai Lu, Liang Luo, Linjian Ma, Matt Ma, Yun Mao, Bert Maher, Ajit Mathews, Matthew Murphy, Satish Nadathur, Min Ni, Jongsoo Park, Jing Qian, Lijing Qin, Alex Singh, Timothy Shi,  Yu Shi, Dennis van der Staay, Xiao Sun, Colin Taylor, Shin-Yeh Tsai, Rohan Varma, Omkar Vichare, Alyssa Wang, Pengchao Wang, Shengzhi Wang, Wenting Wang, Xiaolong Wang, Yueming Wang, Zhiyong Wang, Wei Wei, Bin Wen, Carole-Jean Wu, Yanhong Wu, Eric Xu, Bi Xue, Hong Yan, Zheng Yan, Chao Yang, Junjie Yang, Wen-Yun Yang, Zimeng Yang, Chunxing Yin, Daniel Yin, Yiling You, Jiaqi Zhai, Keke Zhai, Yanli Zhao, Zhuoran Zhao, Hui Zhang, Jingjing Zhang, Lu Zhang, Lujia Zhang, Na Zhang, Rui Zhang, Xiong Zhang, Ying Zhang, Zhiyun Zhang, Charles Zheng, Erheng Zhong, Xin Zhuang.

For the initial paper describing the Generative Recommender problem formulation and the algorithms used, including HSTU and M-FALCON, please refer to ``Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations`` (https://proceedings.mlr.press/v235/zhai24a.html, ICML'24), [poster](https://tinyurl.com/gr-icml24), slides (to be added). More documentations, including an extended technical report, will follow later.

</content>

<content full_path="CONTRIBUTING.md">
# Contributing to facebookresearch/generative-recommenders
We want to make contributing to this project as easy and transparent as
possible.

## Our Development Process
We may periodically sync internal changes to this project, although we do not
currently expect this to be a frequent process except for major changes.

## Pull Requests
We actively welcome your pull requests.

1. Fork the repo and create your branch from `main`.
2. If you've added code that should be tested, add tests.
3. If you've changed APIs, update the documentation.
4. Ensure the test suite passes.
5. Make sure your code lints.
6. If you haven't already, complete the Contributor License Agreement ("CLA").

## Contributor License Agreement ("CLA")
In order to accept your pull request, we need you to submit a CLA. You only need
to do this once to work on any of Meta's open source projects.

Complete your CLA here: <https://code.facebook.com/cla>

## Issues
We use GitHub issues to track public bugs. Please ensure your description is
clear and has sufficient instructions to be able to reproduce the issue.

Meta has a [bounty program](https://bugbounty.meta.com/) for the safe
disclosure of security bugs. In those cases, please go through the process
outlined on that page and do not file a public issue.

## Coding Style  
* 2 spaces for indentation rather than tabs
* 80 character line length
* ...

## License
By contributing to facebookresearch/generative-recommenders, you agree that your contributions will be licensed
under the LICENSE file in the root directory of this source tree.

</content>

<content full_path="preprocess_public_data.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# pyre-unsafe

"""
Usage: mkdir -p tmp/ && python3 preprocess_public_data.py
"""

from generative_recommenders.data.preprocessor import get_common_preprocessors


def main() -> None:
    get_common_preprocessors()["ml-1m"].preprocess_rating()
    get_common_preprocessors()["ml-20m"].preprocess_rating()
    # get_common_preprocessors()["ml-1b"].preprocess_rating()
    get_common_preprocessors()["amzn-books"].preprocess_rating()


if __name__ == "__main__":
    main()

</content>

<content full_path="main.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# pyre-unsafe

"""
Main entry point for model training. Please refer to README.md for usage instructions.
"""

import logging
import os

from typing import List, Optional

os.environ["TF_CPP_MIN_LOG_LEVEL"] = "1"  # Hide excessive tensorflow debug messages
import sys

import fbgemm_gpu  # noqa: F401, E402
import gin

import torch
import torch.multiprocessing as mp

from absl import app, flags
from generative_recommenders.trainer.train import train_fn

logging.basicConfig(stream=sys.stdout, level=logging.INFO)


def delete_flags(FLAGS, keys_to_delete: List[str]) -> None:  # pyre-ignore [2]
    keys = [key for key in FLAGS._flags()]
    for key in keys:
        if key in keys_to_delete:
            delattr(FLAGS, key)


delete_flags(flags.FLAGS, ["gin_config_file", "master_port"])
flags.DEFINE_string("gin_config_file", None, "Path to the config file.")
flags.DEFINE_integer("master_port", 12355, "Master port.")
FLAGS = flags.FLAGS  # pyre-ignore [5]


def mp_train_fn(
    rank: int,
    world_size: int,
    master_port: int,
    gin_config_file: Optional[str],
) -> None:
    if gin_config_file is not None:
        # Hack as absl doesn't support flag parsing inside multiprocessing.
        logging.info(f"Rank {rank}: loading gin config from {gin_config_file}")
        gin.parse_config_file(gin_config_file)

    train_fn(rank, world_size, master_port)


def _main(argv) -> None:  # pyre-ignore [2]
    world_size = torch.cuda.device_count()

    mp.set_start_method("forkserver")
    mp.spawn(
        mp_train_fn,
        args=(world_size, FLAGS.master_port, FLAGS.gin_config_file),
        nprocs=world_size,
        join=True,
    )


def main() -> None:
    app.run(_main)


if __name__ == "__main__":
    main()

</content>

<content full_path="generative_recommenders/common.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

#!/usr/bin/env python3

# pyre-strict

import dataclasses

from dataclasses import dataclass
from enum import Enum, unique
from typing import Any, Dict, List, Optional, Tuple, Union

import torch

# @manual=//triton:triton
import triton

# @manual=//triton:triton
from triton.runtime.autotuner import Autotuner

try:
    from hammer.ops.triton.utils import (
        NamedSpecType,
        register_tritoncc_specs,
        SpecType,
        triton_autotune,
        VersionedSpec,
    )
    from hammer.utils import HammerKernel, is_dev_mode, set_dev_mode, set_verbose_level
except ImportError:
    SpecType = Union[Tuple[str, int], Tuple[str, int, bool], int, str]
    NamedSpecType = Dict[str, SpecType]

    @dataclass
    class VersionedSpec:
        """
        spec: a dict that maps each argument name to a spec
        version: the version of the spec
        """

        spec: NamedSpecType = dataclasses.field(default_factory=dict)
        version: str = ""
        default_values: Dict[str, Any] = dataclasses.field(default_factory=dict)

    # pyre-ignore[2,3]
    def register_tritoncc_specs(func, versioned_specs):
        return func

    # pyre-ignore
    def triton_autotune(
        configs: List[triton.Config],
        key: List[str],
        # pyre-ignore
        prune_configs_by=None,
        # pyre-ignore
        reset_to_zero=None,
        # pyre-ignore
        restore_value=None,
        warmup: int = 25,
        rep: int = 100,
    ):
        # pyre-ignore
        def decorator(fn):
            return Autotuner(
                fn,
                fn.arg_names,
                configs,
                key,
                reset_to_zero,
                restore_value,
                pre_hook=None,
                post_hook=None,
                prune_configs_by=prune_configs_by,
                warmup=warmup,
                rep=rep,
            )

        return decorator

    DEV_MODE: bool = False
    VERBOSE_LEVEL: int = 0

    def set_dev_mode(val: bool) -> None:
        global DEV_MODE
        DEV_MODE = val

    def is_dev_mode() -> bool:
        global DEV_MODE
        return DEV_MODE

    def set_verbose_level(level: int) -> None:
        global VERBOSE_LEVEL
        VERBOSE_LEVEL = level

    def get_verbose_level() -> int:
        global VERBOSE_LEVEL
        return VERBOSE_LEVEL

    @unique
    class HammerKernel(Enum):
        TRITON = "TRITON"
        PYTORCH = "PYTORCH"
        CUDA = "CUDA"
        TRITON_CC = "TRITON_CC"


class GRModuleBase(torch.nn.Module):
    _is_inference: bool
    _use_triton_cc: bool
    _custom_kernel: bool
    _hammer_kernel: Optional[HammerKernel] = None

    def __init__(
        self,
        is_inference: bool,
        use_triton_cc: bool = True,
        custom_kernel: bool = True,
        hammer_kernel: Optional[HammerKernel] = None,
    ) -> None:
        super().__init__()
        self._is_inference = is_inference
        self._use_triton_cc = use_triton_cc
        self._custom_kernel = custom_kernel
        self._hammer_kernel = hammer_kernel

    def hammer_kernel(self) -> HammerKernel:
        kernel = self._hammer_kernel
        if kernel is not None:
            return kernel
        if self._custom_kernel:
            if self._is_inference and self._use_triton_cc:
                return HammerKernel.TRITON_CC
            else:
                return HammerKernel.TRITON
        else:
            return HammerKernel.PYTORCH

    # pyre-ignore[2]
    def recursive_setattr(self, name: str, value: Any) -> None:
        for _, module in self.named_modules():
            if hasattr(module, name):
                setattr(module, name, value)

    @property
    def predict_mode(self) -> bool:
        return self._is_inference

    @property
    def eval_mode(self) -> bool:
        return (not self._is_inference) and (not self.training)

    @property
    def train_mode(self) -> bool:
        return (not self._is_inference) and self.training


def generate_sparse_seq_len(
    size: int,
    max_seq_len: int,
    sparsity: float,
    device: torch.device,
) -> torch.Tensor:
    if sparsity == 0.0:
        return torch.zeros(size=(size,), device=device, dtype=torch.int)
    elif sparsity == 1.0:
        return torch.ones(size=(size,), device=device, dtype=torch.int) * max_seq_len
    elif sparsity >= 0.5:
        min_seq_len: int = int((2 * sparsity - 1.0) * max_seq_len)
        return torch.randint(
            low=min_seq_len,
            high=max_seq_len,
            size=(size,),
            device=device,
            dtype=torch.int,
        )
    else:
        min_seq_len: int = 0
        max_seq_len: int = int(2 * sparsity * max_seq_len)
        return torch.randint(
            low=min_seq_len,
            high=max_seq_len,
            size=(size,),
            device=device,
            dtype=torch.int,
        )


def apply_sampling(
    lengths: torch.Tensor,
    alpha: float,
    max_seq_len: int,
) -> torch.Tensor:
    threshold = int(max_seq_len ** (alpha / 2))
    no_sample_prob = (max_seq_len**alpha) / torch.pow(lengths, 2)
    users_to_sample = torch.logical_and(
        lengths > threshold,
        torch.rand_like(no_sample_prob) < 1 - no_sample_prob,
    )
    lengths = torch.where(users_to_sample, threshold, lengths)
    return lengths


nv_gpu_unavailable: Tuple[bool, str] = (
    not torch.cuda.is_available() or torch.cuda.device_count() == 0,
    "CUDA is not available or no GPUs detected",
)
nv_gpu_available: bool = not nv_gpu_unavailable[0]


amd_gpu_unavailable: Tuple[bool, str] = (
    not torch.version.hip,
    "AMD HIP not available or no GPUs detected",
)
amd_gpu_available: bool = not amd_gpu_unavailable[0]

gpu_unavailable: Tuple[bool, str] = (
    not nv_gpu_available and not amd_gpu_available,
    "CUDA/HIP is not available or no GPUs detected",
)

gpu_available: bool = not gpu_unavailable[0]


def switch_to_contiguous_if_needed(x: torch.Tensor) -> torch.Tensor:
    if not torch.jit.is_scripting() and torch.compiler.is_compiling():
        # Tell Dynamo this data-dependent value is in the range (0, 10**9)
        torch._check(x.size(0) > 0)
        torch._check(x.size(0) < 10**9)
    if x.stride(-1) == 1:
        return x
    return x.contiguous()


@torch.fx.wrap
def prev_power_of_2(x: int) -> int:
    if torch.compiler.is_compiling():
        # Re-write to make Dynamo happy
        x_tensor = torch.scalar_tensor(x, dtype=torch.int64)  # type: ignore[arg-type]
        x_tensor_orig = x_tensor.clone()
        out = triton.next_power_of_2(x_tensor)  # type: ignore[arg-type]
        return int(torch.where(torch.lt(x_tensor_orig, out), out // 2, out).item())  # type: ignore[return-value]
    else:
        out = triton.next_power_of_2(x)
        return out // 2 if out > x else out


STATIC_MAX_SEQ_LEN = -1
L2_STATIC_MAX_SEQ_LEN = -1
USE_RUNTIME_MAX_SEQ_LEN = True


def set_static_max_seq_lens(max_seq_len: int, l2_max_seq_len: int) -> None:
    global STATIC_MAX_SEQ_LEN
    global L2_STATIC_MAX_SEQ_LEN
    STATIC_MAX_SEQ_LEN = max_seq_len
    L2_STATIC_MAX_SEQ_LEN = l2_max_seq_len


def get_static_max_seq_lens() -> Tuple[int, int]:
    return STATIC_MAX_SEQ_LEN, L2_STATIC_MAX_SEQ_LEN


def set_use_runtime_max_seq_len(use_runtime_max_seq_len: bool) -> None:
    global USE_RUNTIME_MAX_SEQ_LEN
    USE_RUNTIME_MAX_SEQ_LEN = use_runtime_max_seq_len


def use_runtime_max_seq_len() -> bool:
    return USE_RUNTIME_MAX_SEQ_LEN


def autotune_max_seq_len(runtime_max_seq_len: int) -> int:
    if use_runtime_max_seq_len():
        return prev_power_of_2(runtime_max_seq_len)
    else:
        max_seq_len, l2_max_seq_len = get_static_max_seq_lens()
        assert (
            max_seq_len > 0 and l2_max_seq_len > 0
        ), "max_seq_len and l2_max_seq_len must be greater than 0"
        return l2_max_seq_len if runtime_max_seq_len <= l2_max_seq_len else max_seq_len

</content>

<content full_path="generative_recommenders/modeling/similarity_module.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import abc
from typing import Optional

import torch

from generative_recommenders.rails.similarities.module import SimilarityModule


class SequentialEncoderWithLearnedSimilarityModule(torch.nn.Module):
    """
    Interface enabling using various similarity functions (besides inner products)
    as part of a sequential encoder/decoder.

    See rails/ for more details.
    """

    def __init__(
        self,
        ndp_module: SimilarityModule,
    ) -> None:
        super().__init__()

        self._ndp_module: SimilarityModule = ndp_module

    @abc.abstractmethod
    def debug_str(
        self,
    ) -> str:
        pass

    def similarity_fn(
        self,
        query_embeddings: torch.Tensor,
        item_ids: torch.Tensor,
        item_embeddings: Optional[torch.Tensor] = None,
        **kwargs,
    ) -> torch.Tensor:
        torch._assert(
            len(query_embeddings.size()) == 2, "len(query_embeddings.size()) must be 2"
        )
        torch._assert(len(item_ids.size()) == 2, "len(item_ids.size()) must be 2")
        if item_embeddings is None:
            item_embeddings = self.get_item_embeddings(item_ids)
        torch._assert(
            len(item_embeddings.size()) == 3, "len(item_embeddings.size()) must be 3"
        )

        return self._ndp_module(
            query_embeddings=query_embeddings,  # (B, query_embedding_dim)
            item_embeddings=item_embeddings,  # (1/B, X, item_embedding_dim)
            item_ids=item_ids,
            **kwargs,
        )

</content>

<content full_path="generative_recommenders/modeling/__init__.py">

</content>

<content full_path="generative_recommenders/modeling/initialization.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# pyre-unsafe

import torch


def truncated_normal(x: torch.Tensor, mean: float, std: float) -> torch.Tensor:
    with torch.no_grad():
        size = x.shape
        tmp = x.new_empty(size + (4,)).normal_()
        valid = (tmp < 2) & (tmp > -2)
        ind = valid.max(-1, keepdim=True)[1]
        x.data.copy_(tmp.gather(-1, ind).squeeze(-1))
        x.data.mul_(std).add_(mean)
        return x


def init_mlp_xavier_weights_zero_bias(m: torch.nn.Module) -> None:
    if isinstance(m, torch.nn.Linear):
        torch.nn.init.xavier_uniform(m.weight)
        if getattr(m, "bias", None) is not None:
            m.bias.data.fill_(0.0)

</content>

<content full_path="generative_recommenders/modeling/similarity_utils.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# pyre-unsafe

from typing import List, Optional, Tuple

import gin
import torch

from generative_recommenders.rails.similarities.dot_product_similarity_fn import (
    DotProductSimilarity,
)
from generative_recommenders.rails.similarities.layers import SwiGLU
from generative_recommenders.rails.similarities.mol.item_embeddings_fn import (
    RecoMoLItemEmbeddingsFn,
)
from generative_recommenders.rails.similarities.mol.query_embeddings_fn import (
    RecoMoLQueryEmbeddingsFn,
)
from generative_recommenders.rails.similarities.mol.similarity_fn import (
    MoLSimilarity,
    SoftmaxDropoutCombiner,
)


def init_mlp_xavier_weights_zero_bias(m) -> None:
    if isinstance(m, torch.nn.Linear):
        torch.nn.init.xavier_uniform(m.weight)
        if getattr(m, "bias", None) is not None:
            m.bias.data.fill_(0.0)


@gin.configurable
def create_mol_interaction_module(
    query_embedding_dim: int,
    item_embedding_dim: int,
    dot_product_dimension: int,
    query_dot_product_groups: int,
    item_dot_product_groups: int,
    temperature: float,
    query_dropout_rate: float,
    query_hidden_dim: int,
    item_dropout_rate: float,
    item_hidden_dim: int,
    gating_query_hidden_dim: int,
    gating_qi_hidden_dim: int,
    gating_item_hidden_dim: int,
    softmax_dropout_rate: float,
    bf16_training: bool,
    gating_query_fn: bool = True,
    gating_item_fn: bool = True,
    dot_product_l2_norm: bool = True,
    query_nonlinearity: str = "geglu",
    item_nonlinearity: str = "geglu",
    uid_dropout_rate: float = 0.5,
    uid_embedding_hash_sizes: Optional[List[int]] = None,
    uid_embedding_level_dropout: bool = False,
    gating_combination_type: str = "glu_silu",
    gating_item_dropout_rate: float = 0.0,
    gating_qi_dropout_rate: float = 0.0,
    eps: float = 1e-6,
) -> Tuple[MoLSimilarity, str]:
    """
    Gin wrapper for creating MoL learned similarity.
    """
    mol_module = MoLSimilarity(
        query_embedding_dim=query_embedding_dim,
        item_embedding_dim=item_embedding_dim,
        dot_product_dimension=dot_product_dimension,
        query_dot_product_groups=query_dot_product_groups,
        item_dot_product_groups=item_dot_product_groups,
        temperature=temperature,
        dot_product_l2_norm=dot_product_l2_norm,
        query_embeddings_fn=RecoMoLQueryEmbeddingsFn(
            query_embedding_dim=query_embedding_dim,
            query_dot_product_groups=query_dot_product_groups,
            dot_product_dimension=dot_product_dimension,
            dot_product_l2_norm=dot_product_l2_norm,
            proj_fn=lambda input_dim, output_dim: (
                torch.nn.Sequential(
                    torch.nn.Dropout(p=query_dropout_rate),
                    SwiGLU(
                        in_features=input_dim,
                        out_features=query_hidden_dim,
                    ),
                    torch.nn.Linear(
                        in_features=query_hidden_dim,
                        out_features=output_dim,
                    ),
                ).apply(init_mlp_xavier_weights_zero_bias)
            ),
            eps=eps,
        ),
        item_embeddings_fn=RecoMoLItemEmbeddingsFn(
            item_embedding_dim=item_embedding_dim,
            item_dot_product_groups=item_dot_product_groups,
            dot_product_dimension=dot_product_dimension,
            dot_product_l2_norm=dot_product_l2_norm,
            proj_fn=lambda input_dim, output_dim: (
                torch.nn.Sequential(
                    torch.nn.Dropout(p=item_dropout_rate),
                    SwiGLU(in_features=input_dim, out_features=item_hidden_dim),
                    torch.nn.Linear(
                        in_features=item_hidden_dim,
                        out_features=output_dim,
                    ),
                ).apply(init_mlp_xavier_weights_zero_bias)
            ),
            eps=eps,
        ),
        gating_query_only_partial_fn=lambda input_dim, output_dim: (
            torch.nn.Sequential(
                torch.nn.Linear(
                    in_features=input_dim,
                    out_features=gating_query_hidden_dim,
                ),
                torch.nn.SiLU(),
                torch.nn.Linear(
                    in_features=gating_query_hidden_dim,
                    out_features=output_dim,
                    bias=False,
                ),
            ).apply(init_mlp_xavier_weights_zero_bias)
            if gating_query_fn
            else None
        ),
        gating_item_only_partial_fn=lambda input_dim, output_dim: (
            torch.nn.Sequential(
                torch.nn.Dropout(p=gating_item_dropout_rate),
                torch.nn.Linear(
                    in_features=input_dim,
                    out_features=gating_item_hidden_dim,
                ),
                torch.nn.SiLU(),
                torch.nn.Linear(
                    in_features=gating_item_hidden_dim,
                    out_features=output_dim,
                    bias=False,
                ),
            ).apply(init_mlp_xavier_weights_zero_bias)
            if gating_item_fn
            else None
        ),
        gating_qi_partial_fn=lambda input_dim, output_dim: (
            torch.nn.Sequential(
                torch.nn.Dropout(p=gating_qi_dropout_rate),
                torch.nn.Linear(
                    in_features=input_dim,
                    out_features=gating_qi_hidden_dim,
                ),
                torch.nn.SiLU(),
                torch.nn.Linear(
                    in_features=gating_qi_hidden_dim,
                    out_features=output_dim,
                ),
            ).apply(init_mlp_xavier_weights_zero_bias)
            if gating_qi_hidden_dim > 0
            else torch.nn.Sequential(
                torch.nn.Dropout(p=gating_qi_dropout_rate),
                torch.nn.Linear(
                    in_features=input_dim,
                    out_features=output_dim,
                ),
            ).apply(init_mlp_xavier_weights_zero_bias)
        ),
        gating_combination_type=gating_combination_type,
        gating_normalization_fn=lambda _: SoftmaxDropoutCombiner(
            dropout_rate=softmax_dropout_rate, eps=1e-6
        ),
        eps=eps,
        autocast_bf16=bf16_training,
    )
    interaction_module_debug_str = (
        f"MoL-{query_dot_product_groups}x{item_dot_product_groups}x{dot_product_dimension}"
        + f"-t{temperature}-d{softmax_dropout_rate}"
        + f"{'-l2' if dot_product_l2_norm else ''}"
        + f"-q{query_hidden_dim}d{query_dropout_rate}{query_nonlinearity}"
        + f"-i{item_hidden_dim}d{item_dropout_rate}{item_nonlinearity}"
        + (f"-gq{gating_query_hidden_dim}" if gating_query_fn else "")
        + (
            f"-gi{gating_item_hidden_dim}d{gating_item_dropout_rate}"
            if gating_item_fn
            else ""
        )
        + f"-gqi{gating_qi_hidden_dim}d{gating_qi_dropout_rate}-x-{gating_combination_type}"
    )
    return mol_module, interaction_module_debug_str


@gin.configurable
def get_similarity_function(
    module_type: str,
    query_embedding_dim: int,
    item_embedding_dim: int,
    bf16_training: bool = False,
    activation_checkpoint: bool = False,
) -> Tuple[torch.nn.Module, str]:
    """
    Factory function to create a similarity module based on the specified type.

    This function creates modules that compute similarity scores between query and item embeddings:
    - DotProduct: Simple dot product between query and item vectors
    - MoL (Mixture of Learners): More sophisticated similarity with multiple learned components

    Args:
        module_type: Type of similarity function ("DotProduct" or "MoL")
        query_embedding_dim: Dimension of query embeddings (user/context representations)
        item_embedding_dim: Dimension of item embeddings (must match query dimension)
        bf16_training: Whether to use bfloat16 precision during training
        activation_checkpoint: Whether to use gradient checkpointing

    Returns:
        Tuple of:
        - interaction_module: The similarity module (torch.nn.Module)
        - interaction_module_debug_str: Debug string describing the module configuration

    The similarity module is a key component that:
    1. Takes query embeddings (representing user context/interests)
    2. Takes item embeddings (representing candidate items)
    3. Computes similarity scores between them
    4. Used to rank items for recommendations

    Two types supported:
    1. DotProduct:
       - Simple inner product between query and item vectors
       - Fast and memory efficient
       - Works well for basic recommendation tasks
       
    2. MoL (Mixture of Learners):
       - More sophisticated learned similarity function
       - Uses multiple dot product "experts"
       - Has learned gating to combine expert outputs
       - Better captures complex similarity patterns
       - More parameters but potentially better accuracy
    """
    if module_type == "DotProduct":
        # DotProductSimilarity forward function:
        #
        # Takes query embeddings [B, Q, D] and item embeddings [B, N, D] where:
        # - B is batch size
        # - Q is number of queries (usually 1)
        # - N is number of items
        # - D is embedding dimension
        #
        # Returns similarity scores [B, Q, N] computed as:
        # - Dot product between each query-item pair
        # - Efficiently implemented as batch matrix multiplication (bmm)
        # - Higher scores indicate greater similarity
        #
        interaction_module = DotProductSimilarity()
        interaction_module_debug_str = "DotProduct"

    elif module_type == "MoL":
        # MoL (Mixture of Learners) similarity function:
        #
        # A more sophisticated similarity function that:
        # 1. Projects query and item embeddings through multiple "expert" networks
        # 2. Computes multiple dot products between projected embeddings
        # 3. Uses learned gating to combine expert outputs
        # 4. Returns weighted combination as final similarity score
        #
        # Key components:
        # - Multiple dot product "experts" capture different similarity patterns
        # - Learned gating determines how to combine expert outputs
        # - Projection networks transform embeddings before dot products
        # - Dropout and normalization for regularization
        #
        # Advantages:
        # - Can learn complex similarity patterns
        # - More expressive than simple dot product
        # - Gating allows dynamic expert weighting
        # - Multiple experts capture different aspects
        #
        # Disadvantages:
        # - More parameters to learn
        # - Higher computational cost
        # - May overfit on small datasets
        # - Requires careful tuning of hyperparameters
        interaction_module, interaction_module_debug_str = (
            create_mol_interaction_module(
                query_embedding_dim=query_embedding_dim,
                item_embedding_dim=item_embedding_dim,
                bf16_training=bf16_training,
            )
        )
    else:
        raise ValueError(f"Unknown interaction_module_type {module_type}")
    return interaction_module, interaction_module_debug_str

</content>

<content full_path="generative_recommenders/modeling/sequential/sasrec.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# pyre-unsafe

"""
Implements SASRec (Self-Attentive Sequential Recommendation, https://arxiv.org/abs/1808.09781, ICDM'18).

Compared with the original paper which used BCE loss, this implementation is modified so that
we can utilize a Sampled Softmax loss proposed in Revisiting Neural Retrieval on Accelerators
(https://arxiv.org/abs/2306.04039, KDD'23) and Turning Dross Into Gold Loss: is BERT4Rec really
better than SASRec? (https://arxiv.org/abs/2309.07602, RecSys'23), where the authors showed
sampled softmax loss to significantly improved SASRec model quality.
"""

from typing import Dict, Optional, Tuple

import torch
import torch.nn.functional as F

from generative_recommenders.modeling.sequential.embedding_modules import (
    EmbeddingModule,
)
from generative_recommenders.modeling.sequential.input_features_preprocessors import (
    InputFeaturesPreprocessorModule,
)
from generative_recommenders.modeling.sequential.output_postprocessors import (
    OutputPostprocessorModule,
)
from generative_recommenders.modeling.sequential.utils import get_current_embeddings
from generative_recommenders.modeling.similarity_module import (
    SequentialEncoderWithLearnedSimilarityModule,
)
from generative_recommenders.rails.similarities.module import SimilarityModule


class StandardAttentionFF(torch.nn.Module):
    def __init__(
        self,
        embedding_dim: int,
        hidden_dim: int,
        activation_fn: str,
        dropout_rate: float,
    ) -> None:
        super().__init__()

        assert (
            activation_fn == "relu" or activation_fn == "gelu"
        ), f"Invalid activation_fn {activation_fn}"

        self._conv1d = torch.nn.Sequential(
            torch.nn.Conv1d(
                in_channels=embedding_dim,
                out_channels=hidden_dim,
                kernel_size=1,
            ),
            torch.nn.GELU() if activation_fn == "gelu" else torch.nn.ReLU(),
            torch.nn.Dropout(p=dropout_rate),
            torch.nn.Conv1d(
                in_channels=hidden_dim,
                out_channels=embedding_dim,
                kernel_size=1,
            ),
            torch.nn.Dropout(p=dropout_rate),
        )

    def forward(self, inputs: torch.Tensor) -> torch.Tensor:
        # Conv1D requires (B, D, N)
        return self._conv1d(inputs.transpose(-1, -2)).transpose(-1, -2) + inputs


class SASRec(SequentialEncoderWithLearnedSimilarityModule):
    """
    Implements SASRec (Self-Attentive Sequential Recommendation, https://arxiv.org/abs/1808.09781, ICDM'18).

    Compared with the original paper which used BCE loss, this implementation is modified so that
    we can utilize a Sampled Softmax loss proposed in Revisiting Neural Retrieval on Accelerators
    (https://arxiv.org/abs/2306.04039, KDD'23) and Turning Dross Into Gold Loss: is BERT4Rec really
    better than SASRec? (https://arxiv.org/abs/2309.07602, RecSys'23), where the authors showed
    sampled softmax loss to significantly improved SASRec model quality.
    """

    def __init__(
        self,
        max_sequence_len: int,
        max_output_len: int,
        embedding_dim: int,
        num_blocks: int,
        num_heads: int,
        ffn_hidden_dim: int,
        ffn_activation_fn: str,
        ffn_dropout_rate: float,
        embedding_module: EmbeddingModule,
        similarity_module: SimilarityModule,
        input_features_preproc_module: InputFeaturesPreprocessorModule,
        output_postproc_module: OutputPostprocessorModule,
        activation_checkpoint: bool = False,
        verbose: bool = False,
    ) -> None:
        super().__init__(ndp_module=similarity_module)

        self._embedding_module: EmbeddingModule = embedding_module
        self._embedding_dim: int = embedding_dim
        self._item_embedding_dim: int = embedding_module.item_embedding_dim
        self._max_sequence_length: int = max_sequence_len + max_output_len
        self._input_features_preproc: InputFeaturesPreprocessorModule = (
            input_features_preproc_module
        )
        self._output_postproc: OutputPostprocessorModule = output_postproc_module
        self._activation_checkpoint: bool = activation_checkpoint
        self._verbose: bool = verbose

        self.attention_layers = torch.nn.ModuleList()
        self.forward_layers = torch.nn.ModuleList()
        self._num_blocks: int = num_blocks
        self._num_heads: int = num_heads
        self._ffn_hidden_dim: int = ffn_hidden_dim
        self._ffn_activation_fn: str = ffn_activation_fn
        self._ffn_dropout_rate: float = ffn_dropout_rate

        for _ in range(num_blocks):
            self.attention_layers.append(
                torch.nn.MultiheadAttention(
                    embed_dim=self._embedding_dim,
                    num_heads=num_heads,
                    dropout=ffn_dropout_rate,
                    batch_first=True,
                )
            )
            self.forward_layers.append(
                StandardAttentionFF(
                    embedding_dim=self._embedding_dim,
                    hidden_dim=ffn_hidden_dim,
                    activation_fn=ffn_activation_fn,
                    dropout_rate=self._ffn_dropout_rate,
                )
            )

        self.register_buffer(
            "_attn_mask",
            torch.triu(
                torch.ones(
                    (self._max_sequence_length, self._max_sequence_length),
                    dtype=torch.bool,
                ),
                diagonal=1,
            ),
        )
        self.reset_state()

    def reset_state(self) -> None:
        for name, params in self.named_parameters():
            if (
                "_input_features_preproc" in name
                or "_embedding_module" in name
                or "_output_postproc" in name
            ):
                if self._verbose:
                    print(f"Skipping initialization for {name}")
                continue
            try:
                torch.nn.init.xavier_normal_(params.data)
                if self._verbose:
                    print(
                        f"Initialize {name} as xavier normal: {params.data.size()} params"
                    )
            except:
                if self._verbose:
                    print(f"Failed to initialize {name}: {params.data.size()} params")

    def get_item_embeddings(self, item_ids: torch.Tensor) -> torch.Tensor:
        return self._embedding_module.get_item_embeddings(item_ids)

    def debug_str(self) -> str:
        return (
            f"SASRec-d{self._item_embedding_dim}-b{self._num_blocks}-h{self._num_heads}"
            + "-"
            + self._input_features_preproc.debug_str()
            + "-"
            + self._output_postproc.debug_str()
            + f"-ffn{self._ffn_hidden_dim}-{self._ffn_activation_fn}-d{self._ffn_dropout_rate}"
            + f"{'-ac' if self._activation_checkpoint else ''}"
        )

    def _run_one_layer(
        self,
        i: int,
        user_embeddings: torch.Tensor,
        valid_mask: torch.Tensor,
    ) -> torch.Tensor:
        Q = F.layer_norm(
            user_embeddings,
            normalized_shape=(self._embedding_dim,),
            eps=1e-8,
        )
        mha_outputs, _ = self.attention_layers[i](
            query=Q,
            key=user_embeddings,
            value=user_embeddings,
            attn_mask=self._attn_mask,
        )
        user_embeddings = self.forward_layers[i](
            F.layer_norm(
                Q + mha_outputs,
                normalized_shape=(self._embedding_dim,),
                eps=1e-8,
            )
        )
        user_embeddings *= valid_mask
        return user_embeddings

    def generate_user_embeddings(
        self,
        past_lengths: torch.Tensor,
        past_ids: torch.Tensor,
        past_embeddings: torch.Tensor,
        past_payloads: Dict[str, torch.Tensor],
    ) -> torch.Tensor:
        """
        Args:
            past_ids: (B, N,) x int

        Returns:
            (B, N, D,) x float
        """
        past_lengths, user_embeddings, valid_mask = self._input_features_preproc(
            past_lengths=past_lengths,
            past_ids=past_ids,
            past_embeddings=past_embeddings,
            past_payloads=past_payloads,
        )

        for i in range(len(self.attention_layers)):
            if self._activation_checkpoint:
                user_embeddings = torch.utils.checkpoint.checkpoint(
                    self._run_one_layer,
                    i,
                    user_embeddings,
                    valid_mask,
                    use_reentrant=False,
                )
            else:
                user_embeddings = self._run_one_layer(i, user_embeddings, valid_mask)

        return self._output_postproc(user_embeddings)

    def forward(
        self,
        past_lengths: torch.Tensor,
        past_ids: torch.Tensor,
        past_embeddings: torch.Tensor,
        past_payloads: Dict[str, torch.Tensor],
        batch_id: Optional[int] = None,
    ) -> torch.Tensor:
        """
        Args:
            past_ids: [B, N] x int64 where the latest engaged ids come first. In
                particular, [:, 0] should correspond to the last engaged values.
            past_ratings: [B, N] x int64.
            past_timestamps: [B, N] x int64.

        Returns:
            encoded_embeddings of [B, N, D].
        """
        encoded_embeddings = self.generate_user_embeddings(
            past_lengths,
            past_ids,
            past_embeddings,
            past_payloads,
        )
        return encoded_embeddings

    def encode(
        self,
        past_lengths: torch.Tensor,
        past_ids: torch.Tensor,  # [B, N] x int64
        past_embeddings: torch.Tensor,
        past_payloads: Dict[str, torch.Tensor],
    ) -> torch.Tensor:
        encoded_seq_embeddings = self.generate_user_embeddings(
            past_lengths, past_ids, past_embeddings, past_payloads
        )  # [B, N, D]
        return get_current_embeddings(
            lengths=past_lengths, encoded_embeddings=encoded_seq_embeddings
        )

    def predict(
        self,
        past_ids: torch.Tensor,
        past_ratings: torch.Tensor,
        past_timestamps: torch.Tensor,
        next_timestamps: torch.Tensor,
        target_ids: torch.Tensor,
        batch_id: Optional[int] = None,
    ) -> torch.Tensor:
        return self.interaction(
            self.encode(
                past_ids, past_ratings, past_timestamps, next_timestamps
            ),  # pyre-ignore [6]
            target_ids,
        )  # [B, X]

</content>

<content full_path="generative_recommenders/modeling/sequential/encoder_utils.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# pyre-unsafe

import gin
from generative_recommenders.modeling.sequential.embedding_modules import (
    EmbeddingModule,
)
from generative_recommenders.modeling.sequential.hstu import HSTU
from generative_recommenders.modeling.sequential.input_features_preprocessors import (
    InputFeaturesPreprocessorModule,
)
from generative_recommenders.modeling.sequential.output_postprocessors import (
    OutputPostprocessorModule,
)
from generative_recommenders.modeling.sequential.sasrec import SASRec
from generative_recommenders.modeling.similarity_module import (
    SequentialEncoderWithLearnedSimilarityModule,
)
from generative_recommenders.rails.similarities.module import SimilarityModule


@gin.configurable
def sasrec_encoder(
    max_sequence_length: int,
    max_output_length: int,
    embedding_module: EmbeddingModule,
    similarity_module: SimilarityModule,
    input_preproc_module: InputFeaturesPreprocessorModule,
    output_postproc_module: OutputPostprocessorModule,
    activation_checkpoint: bool,
    verbose: bool,
    ffn_hidden_dim: int = 64,
    ffn_activation_fn: str = "relu",
    ffn_dropout_rate: float = 0.2,
    num_blocks: int = 2,
    num_heads: int = 1,
) -> SequentialEncoderWithLearnedSimilarityModule:
    return SASRec(
        embedding_module=embedding_module,
        max_sequence_len=max_sequence_length,
        max_output_len=max_output_length,
        embedding_dim=embedding_module.item_embedding_dim,
        ffn_hidden_dim=ffn_hidden_dim,
        ffn_activation_fn=ffn_activation_fn,
        ffn_dropout_rate=ffn_dropout_rate,
        num_blocks=num_blocks,
        num_heads=num_heads,
        similarity_module=similarity_module,  # pyre-ignore [6]
        input_features_preproc_module=input_preproc_module,
        output_postproc_module=output_postproc_module,
        activation_checkpoint=activation_checkpoint,
        verbose=verbose,
    )


@gin.configurable
def hstu_encoder(
    max_sequence_length: int,
    max_output_length: int,
    embedding_module: EmbeddingModule,
    similarity_module: SimilarityModule,
    input_preproc_module: InputFeaturesPreprocessorModule,
    output_postproc_module: OutputPostprocessorModule,
    activation_checkpoint: bool,
    verbose: bool,
    num_blocks: int = 2,
    num_heads: int = 1,
    dqk: int = 64,
    dv: int = 64,
    linear_dropout_rate: float = 0.0,
    attn_dropout_rate: float = 0.0,
    normalization: str = "rel_bias",
    linear_config: str = "uvqk",
    linear_activation: str = "silu",
    concat_ua: bool = False,
    enable_relative_attention_bias: bool = True,
) -> SequentialEncoderWithLearnedSimilarityModule:
    return HSTU(
        embedding_module=embedding_module,
        similarity_module=similarity_module,  # pyre-ignore [6]
        input_features_preproc_module=input_preproc_module,
        output_postproc_module=output_postproc_module,
        max_sequence_len=max_sequence_length,
        max_output_len=max_output_length,
        embedding_dim=embedding_module.item_embedding_dim,
        num_blocks=num_blocks,
        num_heads=num_heads,
        attention_dim=dqk,
        linear_dim=dv,
        linear_dropout_rate=linear_dropout_rate,
        attn_dropout_rate=attn_dropout_rate,
        linear_config=linear_config,
        linear_activation=linear_activation,
        normalization=normalization,
        concat_ua=concat_ua,
        enable_relative_attention_bias=enable_relative_attention_bias,
        verbose=verbose,
    )


@gin.configurable
def get_sequential_encoder(
    module_type: str,
    max_sequence_length: int,
    max_output_length: int,
    embedding_module: EmbeddingModule,
    interaction_module: SimilarityModule,
    input_preproc_module: InputFeaturesPreprocessorModule,
    output_postproc_module: OutputPostprocessorModule,
    verbose: bool,
    activation_checkpoint: bool = False,
) -> SequentialEncoderWithLearnedSimilarityModule:
    """
    Factory function to create a sequential encoder model based on the specified module type.
    
    A factory function is a design pattern that provides an interface for creating objects
    without explicitly specifying their exact classes. Key aspects:

    1. Encapsulation: Hides object creation logic from the client code
       - Client code doesn't need to know implementation details
       - Creation logic is centralized in one place
       
    2. Flexibility: Allows runtime decisions about which class to instantiate
       - Can choose different implementations based on parameters
       - Easy to add new types without changing client code
       
    3. Consistency: Ensures objects are created in a standardized way
       - Enforces proper initialization and configuration
       - Reduces duplicate object creation code
       
    In this case, this factory function:
    - Takes configuration parameters as input
    - Decides whether to create SASRec or HSTU model
    - Configures the model with provided modules and parameters
    - Returns a fully initialized sequential encoder

    The sequential encoder processes user interaction sequences to generate recommendations:
    1. Takes sequence of user-item interactions as input
    2. Processes through transformer-based architecture
    3. Generates predictions for next items
    
    Args:
        module_type: Type of transformer architecture ("SASRec" or "HSTU")
        max_sequence_length: Maximum length of input sequences
        max_output_length: Maximum length of output predictions
        embedding_module: Handles conversion of item IDs to embeddings
        interaction_module: Core transformer layers for sequence processing
        input_preproc_module: Adds positional embeddings to inputs
        output_postproc_module: Normalizes output embeddings
        verbose: Whether to print model architecture details
        activation_checkpoint: Whether to use gradient checkpointing
        
    Returns:
        SequentialEncoderWithLearnedSimilarityModule: Configured encoder model
        
    The model processes sequences by:
    1. Converting item IDs to embeddings via embedding_module
    2. Adding positional information via input_preproc_module  
    3. Passing through transformer layers in interaction_module
    4. Normalizing outputs via output_postproc_module
    5. Computing similarity scores for next-item prediction
    
    Supports two architectures:
    - SASRec: Self-Attention based Sequential Recommendation
    - HSTU: Hierarchical Sequential Transformer Unit
      
    The two supported encoder architectures are:

    1. SASRec (Self-Attention based Sequential Recommendation):
       - Classic transformer architecture for sequential recommendation
       - Uses self-attention to capture item-item relationships
       - Each layer has:
         - Multi-head self attention
         - Position-wise feed forward network
         - Layer normalization and residual connections
       - Processes full sequence at once
       - Good for capturing long-range dependencies
       - Memory efficient compared to RNN approaches

    2. HSTU (Hierarchical Sequential Transformer Unit):
       - Novel hierarchical transformer architecture
       - Processes sequence in hierarchical chunks
       - Each chunk processed by:
         - Local self-attention within chunk
         - Global attention across chunk summaries
         - Hierarchical position embeddings
       - Benefits:
         - More efficient than full attention
         - Can handle longer sequences
         - Maintains both local and global context
         - Reduces memory and compute requirements
       - Especially suited for long user histories

    Both encoders:
    - Take preprocessed item sequences as input
    - Apply positional embeddings
    - Process through transformer layers
    - Generate contextualized representations
    - Output embeddings for next-item prediction
    - Support flexible sequence lengths
    - Can be trained end-to-end

    The key difference is in how they process sequences:
    - SASRec uses standard full self-attention
    - HSTU uses hierarchical attention for better efficiency
    """
    if module_type == "SASRec":
        model = sasrec_encoder(
            max_sequence_length=max_sequence_length,
            max_output_length=max_output_length,
            embedding_module=embedding_module,
            similarity_module=interaction_module,
            input_preproc_module=input_preproc_module,
            output_postproc_module=output_postproc_module,
            activation_checkpoint=activation_checkpoint,
            verbose=verbose,
        )
    elif module_type == "HSTU":
        model = hstu_encoder(
            max_sequence_length=max_sequence_length,
            max_output_length=max_output_length,
            embedding_module=embedding_module,
            similarity_module=interaction_module,
            input_preproc_module=input_preproc_module,
            output_postproc_module=output_postproc_module,
            activation_checkpoint=activation_checkpoint,
            verbose=verbose,
        )
    else:
        raise ValueError(f"Unsupported module_type {module_type}")
    return model

</content>

<content full_path="generative_recommenders/modeling/sequential/__init__.py">

</content>

<content full_path="generative_recommenders/modeling/sequential/features.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# pyre-unsafe

from typing import Dict, NamedTuple, Optional, Tuple

import torch


class SequentialFeatures(NamedTuple):
    """
    Represents a user's historical interactions with items.
    Contains:
    - past_lengths: (B,) Length of each user's history
    - past_ids: (B, N,) Movie IDs in user histories
    - past_embeddings: (B, N, D) Embeddings for each movie in user history
    - past_payloads: Dict[str, torch.Tensor] Additional metadata about each interaction
    """
    # (B,) x int64. Requires past_lengths[i] > 0 \forall i.
    past_lengths: torch.Tensor
    # (B, N,) x int64. 0 denotes valid ids.
    past_ids: torch.Tensor
    # (B, N, D) x float.
    past_embeddings: Optional[torch.Tensor]
    # Implementation-specific payloads.
    # e.g., past timestamps, past event_types (e.g., clicks, likes), etc.
    past_payloads: Dict[str, torch.Tensor]


def movielens_seq_features_from_row(
    row: Dict[str, torch.Tensor],
    device: int,
    max_output_length: int,
) -> Tuple[SequentialFeatures, torch.Tensor, torch.Tensor]:
    """Converts a row of MovieLens data into sequential features format.

    Args:
        row (Dict[str, torch.Tensor]): Dictionary containing MovieLens data tensors:
            - history_lengths: (B,) Length of each user's history
            - historical_ids: (B, N) Movie IDs in user histories
            - historical_ratings: (B, N) Ratings in user histories
            - historical_timestamps: (B, N) Timestamps in user histories
            - target_ids: (B,) Target movie IDs to predict
            - target_ratings: (B,) Target ratings to predict
            - target_timestamps: (B,) Target timestamps
        device (int): GPU device ID to place tensors on
        max_output_length (int): If > 0, pad histories to this length and include target

    Returns:
        Tuple containing:
        - SequentialFeatures: Named tuple with user history data
        - target_ids: (B, 1) Target movie IDs
        - target_ratings: (B, 1) Target ratings
    """
    historical_lengths = row["history_lengths"].to(device)  # [B]
    historical_ids = row["historical_ids"].to(device)  # [B, N]
    historical_ratings = row["historical_ratings"].to(device)
    historical_timestamps = row["historical_timestamps"].to(device)
    target_ids = row["target_ids"].to(device).unsqueeze(1)  # [B, 1]
    target_ratings = row["target_ratings"].to(device).unsqueeze(1)
    target_timestamps = row["target_timestamps"].to(device).unsqueeze(1)
    if max_output_length > 0:
        B = historical_lengths.size(0)
        historical_ids = torch.cat(
            [
                historical_ids,
                torch.zeros(
                    (B, max_output_length), dtype=historical_ids.dtype, device=device
                ),
            ],
            dim=1,
        )
        historical_ratings = torch.cat(
            [
                historical_ratings,
                torch.zeros(
                    (B, max_output_length),
                    dtype=historical_ratings.dtype,
                    device=device,
                ),
            ],
            dim=1,
        )
        historical_timestamps = torch.cat(
            [
                historical_timestamps,
                torch.zeros(
                    (B, max_output_length),
                    dtype=historical_timestamps.dtype,
                    device=device,
                ),
            ],
            dim=1,
        )
        historical_timestamps.scatter_(
            dim=1,
            index=historical_lengths.view(-1, 1),
            src=target_timestamps.view(-1, 1),
        )
        # print(f"historical_ids.size()={historical_ids.size()}, historical_timestamps.size()={historical_timestamps.size()}")
    features = SequentialFeatures(
        past_lengths=historical_lengths,
        past_ids=historical_ids,
        past_embeddings=None,
        past_payloads={
            "timestamps": historical_timestamps,
            "ratings": historical_ratings,
        },
    )
    return features, target_ids, target_ratings

</content>

<content full_path="generative_recommenders/modeling/sequential/output_postprocessors.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# pyre-unsafe

import abc

import torch
import torch.nn.functional as F


class OutputPostprocessorModule(torch.nn.Module):
    @abc.abstractmethod
    def debug_str(self) -> str:
        pass

    @abc.abstractmethod
    def forward(
        self,
        output_embeddings: torch.Tensor,
    ) -> torch.Tensor:
        pass


class L2NormEmbeddingPostprocessor(OutputPostprocessorModule):
    def __init__(
        self,
        embedding_dim: int,
        eps: float = 1e-6,
    ) -> None:
        super().__init__()
        self._embedding_dim: int = embedding_dim
        self._eps: float = eps

    def debug_str(self) -> str:
        return "l2"

    def forward(
        self,
        output_embeddings: torch.Tensor,
    ) -> torch.Tensor:
        output_embeddings = output_embeddings[..., : self._embedding_dim]
        return output_embeddings / torch.clamp(
            torch.linalg.norm(output_embeddings, ord=None, dim=-1, keepdim=True),
            min=self._eps,
        )


class LayerNormEmbeddingPostprocessor(OutputPostprocessorModule):
    def __init__(
        self,
        embedding_dim: int,
        eps: float = 1e-6,
    ) -> None:
        super().__init__()
        self._embedding_dim: int = embedding_dim
        self._eps: float = eps

    def debug_str(self) -> str:
        return "ln"

    def forward(
        self,
        output_embeddings: torch.Tensor,
    ) -> torch.Tensor:
        output_embeddings = output_embeddings[..., : self._embedding_dim]
        return F.layer_norm(
            output_embeddings,
            normalized_shape=(self._embedding_dim,),
            eps=self._eps,
        )

</content>

<content full_path="generative_recommenders/modeling/sequential/utils.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# pyre-unsafe

import torch


def batch_gather_embeddings(
    rowwise_indices: torch.Tensor,
    embeddings: torch.Tensor,
) -> torch.Tensor:
    """
    Args:
        rowwise_indices: (B, N) x int, where each entry is in [0, X).
        embeddings: (B, X, D,) x float.

    Returns:
        (B, N, D,) x float, embeddings corresponding to rowwise_indices.
    """
    _, N = rowwise_indices.size()
    B, X, D = embeddings.size()
    flattened_indices = (
        rowwise_indices
        + torch.arange(
            start=0,
            end=B,
            step=1,
            dtype=rowwise_indices.dtype,
            device=rowwise_indices.device,
        )
        .unsqueeze(1)
        .expand(-1, N)
        * X
    )
    return embeddings.view(-1, D)[flattened_indices, :].reshape(
        rowwise_indices.size() + (D,)
    )


def batch_scatter_embeddings(
    dst_embeddings: torch.Tensor,
    rowwise_indices: torch.Tensor,
    src_embeddings: torch.Tensor,
) -> None:
    """
    Args:
        dst_embeddings: (B, N, D,) x float.
        rowwise_indices: (B,) x int, where each entry is in [0, N - 1).
        source_embeddings: (B, D,) x float.
    """
    B, N, D = dst_embeddings.size()
    flattened_indices = rowwise_indices + torch.arange(
        start=0,
        end=B * N,
        step=N,
        dtype=rowwise_indices.dtype,
        device=rowwise_indices.device,
    )
    dst_embeddings.view(B * N, D)[flattened_indices, :] = src_embeddings


def get_current_embeddings(
    lengths: torch.Tensor,
    encoded_embeddings: torch.Tensor,
) -> torch.Tensor:
    """
    Args:
        lengths: (B,) x int
        seq_embeddings: (B, N, D,) x float

    Returns:
        (B, D,) x float, where [i, :] == encoded_embeddings[i, lengths[i] - 1, :]
    """
    B, N, D = encoded_embeddings.size()
    flattened_offsets = (lengths - 1) + torch.arange(
        start=0, end=B, step=1, dtype=lengths.dtype, device=lengths.device
    ) * N
    return encoded_embeddings.reshape(-1, D)[flattened_offsets, :].reshape(B, D)


def jagged_or_dense_repeat_interleave_dim0(
    x: torch.Tensor, lengths: torch.Tensor, repeats: int
) -> torch.Tensor:
    if len(x.size()) == 3:
        return x.repeat_interleave(repeats, dim=0)
    else:
        assert len(x.size()) == 2, f"x.size() = {x.size()}"
        padded_x = torch.ops.fbgemm.jagged_to_padded_dense(
            values=x,
            offsets=[torch.ops.fbgemm.asynchronous_complete_cumsum(lengths)],
            max_lengths=[lengths.max()],
            padding_value=0.0,
        )
        lengths = lengths.repeat_interleave(repeats, dim=0)
        return torch.ops.fbgemm.dense_to_jagged(
            padded_x.repeat_interleave(repeats, dim=0),
            [torch.ops.fbgemm.asynchronous_complete_cumsum(lengths)],
        )[0]


def jagged_or_dense_index_select_dim0(
    x: torch.Tensor, lengths: torch.Tensor, indices: torch.Tensor
) -> torch.Tensor:
    if len(x.size()) == 3:
        return x[indices, :, :]
    else:
        assert len(x.size()) == 2, f"x.size() = {x.size()}"
        padded_x = torch.ops.fbgemm.jagged_to_padded_dense(
            values=x,
            offsets=[torch.ops.fbgemm.asynchronous_complete_cumsum(lengths)],
            max_lengths=[lengths.max()],
            padding_value=0.0,
        )
        return torch.ops.fbgemm.dense_to_jagged(
            padded_x[indices, :],
            [torch.ops.fbgemm.asynchronous_complete_cumsum(lengths[indices])],
        )[0]

</content>

<content full_path="generative_recommenders/modeling/sequential/embedding_modules.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# pyre-unsafe

import abc

import torch

from generative_recommenders.modeling.initialization import truncated_normal


class EmbeddingModule(torch.nn.Module):
    @abc.abstractmethod
    def debug_str(self) -> str:
        pass

    @abc.abstractmethod
    def get_item_embeddings(self, item_ids: torch.Tensor) -> torch.Tensor:
        pass

    @property
    @abc.abstractmethod
    def item_embedding_dim(self) -> int:
        pass


class LocalEmbeddingModule(EmbeddingModule):
    def __init__(
        self,
        num_items: int,
        item_embedding_dim: int,
    ) -> None:
        """
        LocalEmbeddingModule provides direct item ID to embedding lookup, 
        where each item has its own unique embedding.

        This is different from CategoricalEmbeddingModule which maps items to category embeddings,
        where multiple items that belong to the same category share the same embedding
        by first looking up the category ID for each item,
        then mapping those category IDs to shared embeddings.

        Key differences:
        - LocalEmbeddingModule: Each item ID maps to a unique embedding vector
        - CategoricalEmbeddingModule: Items are grouped by categories, items in same category share embedding
        
        Args:
            num_items: Number of unique items in the dataset
            item_embedding_dim: Dimension of the embedding vectors
        """
        super().__init__()

        self._item_embedding_dim: int = item_embedding_dim
        self._item_emb = torch.nn.Embedding(
            # Add 1 to num_items to account for padding_idx=0, which will be initialized to zeros.
            # The embedding layer maps item IDs to dense vector representations.
            #
            # Add 1 to num_items to reserve index 0 for padding. Valid item IDs will start at 1.
            # This ensures padding_idx=0 has its own dedicated index and doesn't overlap with real items.
            #
            # If we do not use padding:
            # - Cannot batch sequences of different lengths together efficiently
            # - No way to mask out/ignore invalid positions in sequences
            # - Model will try to learn embeddings for padding positions
            # - Gradient updates will be incorrect due to padding contributing
            # - Memory wasted storing/computing embeddings for padding tokens
            # - More complex logic needed to handle variable length sequences
            #
            # We reserve index 0 for padding to handle variable length sequences:
            # - When batching sequences of different lengths, shorter sequences are padded to match longest
            # - Padding tokens should not contribute to model predictions or loss
            # - By using padding_idx=0, the embeddings at index 0 are initialized to zero vectors and won't be updated during training
            # - This effectively makes padding tokens "invisible" to the model's computations
            # 
            # torch.nn.Embedding provides several advantages over a static dictionary:
            # - Embeddings are learnable parameters updated during training
            # - Supports automatic differentiation and backpropagation 
            # - Optimized for efficient parallel lookup on GPU
            # - Handles padding and out-of-bounds indices gracefully
            #
            # Args:
            #   num_items: Number of items in the dataset
            #   item_embedding_dim: Dimension of the item embedding vectors
            #   padding_idx: Index to be initialized to zeros (0 in this case)
            #
            # Reference: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html
            #
            num_embeddings=num_items + 1,
            # Dimension of each embedding vector 
            embedding_dim=item_embedding_dim,
            # Index 0 is used for padding and will be initialized to zeros
            # If specified, the entries at padding_idx do not contribute to the gradient; 
            # therefore, the embedding vector at padding_idx is not updated during training, 
            # i.e. it remains as a fixed “pad”. For a newly constructed Embedding, 
            # the embedding vector at padding_idx will default to all zeros, 
            # but can be updated to another value to be used as the padding vector.
            padding_idx=0
        )
        self.reset_params()

    def debug_str(self) -> str:
        return f"local_emb_d{self._item_embedding_dim}"

    def reset_params(self) -> None:
        for name, params in self.named_parameters():
            if "_item_emb" in name:
                print(
                    f"Initialize {name} as truncated normal: {params.data.size()} params"
                )
                truncated_normal(params, mean=0.0, std=0.02)
            else:
                print(f"Skipping initializing params {name} - not configured")

    def get_item_embeddings(self, item_ids: torch.Tensor) -> torch.Tensor:
        return self._item_emb(item_ids)

    @property
    def item_embedding_dim(self) -> int:
        return self._item_embedding_dim


class CategoricalEmbeddingModule(EmbeddingModule):
    def __init__(
        self,
        num_items: int,
        item_embedding_dim: int,
        item_id_to_category_id: torch.Tensor,
    ) -> None:
        super().__init__()

        self._item_embedding_dim: int = item_embedding_dim
        self._item_emb: torch.nn.Embedding = torch.nn.Embedding(
            num_items + 1, item_embedding_dim, padding_idx=0
        )
        self.register_buffer("_item_id_to_category_id", item_id_to_category_id)
        self.reset_params()

    def debug_str(self) -> str:
        return f"cat_emb_d{self._item_embedding_dim}"

    def reset_params(self) -> None:
        for name, params in self.named_parameters():
            if "_item_emb" in name:
                print(
                    f"Initialize {name} as truncated normal: {params.data.size()} params"
                )
                truncated_normal(params, mean=0.0, std=0.02)
            else:
                print(f"Skipping initializing params {name} - not configured")

    def get_item_embeddings(self, item_ids: torch.Tensor) -> torch.Tensor:
        item_ids = self._item_id_to_category_id[(item_ids - 1).clamp(min=0)] + 1
        return self._item_emb(item_ids)

    @property
    def item_embedding_dim(self) -> int:
        return self._item_embedding_dim

</content>

<content full_path="generative_recommenders/modeling/sequential/input_features_preprocessors.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# pyre-unsafe

import abc
import math
from typing import Dict, Tuple

import torch

from generative_recommenders.modeling.initialization import truncated_normal


class InputFeaturesPreprocessorModule(torch.nn.Module):
    @abc.abstractmethod
    def debug_str(self) -> str:
        pass

    @abc.abstractmethod
    def forward(
        self,
        past_lengths: torch.Tensor,
        past_ids: torch.Tensor,
        past_embeddings: torch.Tensor,
        past_payloads: Dict[str, torch.Tensor],
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        pass


class LearnablePositionalEmbeddingInputFeaturesPreprocessor(
    InputFeaturesPreprocessorModule
):
    """
    Input features preprocessor that adds learnable positional embeddings to item embeddings to capture sequential order.

    Positional embeddings are crucial because transformer-based models have no inherent way to 
    understand sequence order. Without position information, the model would treat sequences like 
    ["movie1", "movie2", "movie3"] and ["movie2", "movie1", "movie3"] as identical, since 
    self-attention operates on unordered sets.
    
    The positional order is encoded through learnable embeddings:
    1. Each position index (0 to max_sequence_len-1) maps to a unique embedding vector
    2. The position embeddings are initialized using truncated normal distribution
    3. During forward pass, position embeddings are added to the item embeddings
    4. The combined embeddings allow the model to distinguish item order

    The position embeddings are added (element-wise sum) to the item embeddings, not concatenated.
    This preserves the embedding dimension while allowing the model to learn position-specific 
    modifications to each item embedding dimension.

    For example, if we have:
    - Item embedding: [0.1, 0.2, 0.3]
    - Position embedding for pos 0: [0.01, -0.02, 0.04] 
    
    The final embedding would be:
    [0.11, 0.18, 0.34] = [0.1 + 0.01, 0.2 + (-0.02), 0.3 + 0.04]

    This additive approach:
    - Maintains same embedding dimension throughout network
    - Allows position information to modulate each item embedding dimension
    - Is computationally efficient compared to concatenation
    - Follows standard practice in transformer architectures

    For example, given sequence [item1, item2, item3]:
    - Position 0 embedding is added to item1 embedding
    - Position 1 embedding is added to item2 embedding  
    - Position 2 embedding is added to item3 embedding

    This creates position-aware representations where:
    - Same item at different positions has different final embeddings
    - Model can learn position-specific patterns and biases
    - Relative positions are preserved through unique embedding combinations

    The positional embeddings:
    - Assign a unique learnable vector to each position in the sequence
    - Allow the model to distinguish between items appearing at different positions
    - Help capture temporal patterns and dependencies in user behavior
    - Enable the model to learn position-specific item preferences

    For example, in movie recommendations:
    - Recent movies may be more relevant for next-item prediction
    - The order of movies watched can indicate evolving user tastes
    - Some movies are more likely to be watched as first-time vs follow-up content

    The embeddings are learned during training to capture meaningful position information
    that helps improve recommendation accuracy.
    
    This module:
    - Creates learnable embeddings for each position in the sequence
    - Adds position embeddings to item embeddings after scaling
    - Applies dropout for regularization
    - Masks out padding tokens (id=0)
    
    Args:
        max_sequence_len: Maximum length of input sequences
        embedding_dim: Dimension of item and position embeddings
        dropout_rate: Dropout probability applied to embeddings
        
    The forward pass:
    1. Scales item embeddings by sqrt(embedding_dim)
    2. Adds position embeddings based on sequence position
    3. Applies dropout
    4. Masks out padding tokens
    5. Returns sequence lengths, processed embeddings, and attention mask
    
    References:
        - Vaswani et al. "Attention Is All You Need" (2017)
        - Position embeddings help model learn order dependencies
        - Scaling by sqrt(dim) helps maintain variance after adding embeddings
        - Dropout prevents overfitting on position patterns
    """
    def __init__(
        self,
        max_sequence_len: int,
        embedding_dim: int,
        dropout_rate: float,
    ) -> None:
        super().__init__()

        self._embedding_dim: int = embedding_dim
        self._pos_emb: torch.nn.Embedding = torch.nn.Embedding(
            max_sequence_len,
            self._embedding_dim,
        )
        self._dropout_rate: float = dropout_rate
        self._emb_dropout = torch.nn.Dropout(p=dropout_rate)
        self.reset_state()

    def debug_str(self) -> str:
        return f"posi_d{self._dropout_rate}"

    def reset_state(self) -> None:
        truncated_normal(
            self._pos_emb.weight.data,
            mean=0.0,
            std=math.sqrt(1.0 / self._embedding_dim),
        )

    def forward(
        self,
        past_lengths: torch.Tensor,  # [B] sequence lengths
        past_ids: torch.Tensor,      # [B, N] item ids
        past_embeddings: torch.Tensor,  # [B, N, D] item embeddings
        past_payloads: Dict[str, torch.Tensor],  # Additional payload tensors
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Forward pass that adds positional embeddings to item embeddings.
        
        The item embeddings and positional embeddings are added in the body of this 
        function using:
        user_embeddings = past_embeddings * (self._embedding_dim**0.5) + self._pos_emb(...)
        
        This creates position-aware embeddings by:
        1. Scaling item embeddings by sqrt(embedding_dim)
        2. Adding learned positional embeddings based on sequence position
        
        Args:
            past_lengths: Tensor of sequence lengths for each batch [B]
            past_ids: Tensor of item IDs for each position [B, N] 
            past_embeddings: Tensor of item embeddings [B, N, D]
            past_payloads: Additional payload tensors
            
        Returns:
            Tuple of:
            - past_lengths: Original sequence lengths [B]
            - user_embeddings: Position-aware embeddings [B, N, D]
              where:
              - B is the batch size (number of sequences)
              - N is the sequence length (number of items per sequence)
              - D is the embedding dimension size
              Called "user_embeddings" because they represent the user's historical
              sequence of items with positional information added. These embeddings
              capture both the items and their order in the user's history.
            - valid_mask: Binary mask for valid (non-padding) positions [B, N, 1]
        """
        B, N = past_ids.size()
        D = past_embeddings.size(-1)

        user_embeddings = past_embeddings * (self._embedding_dim**0.5) + self._pos_emb(
            torch.arange(N, device=past_ids.device).unsqueeze(0).repeat(B, 1)
        )
        # The dropout layer randomly zeros out elements of user_embeddings with probability dropout_rate
        # This means some positions in the sequence will have their embeddings set to 0
        # For example with dropout_rate=0.1:
        # - ~90% of embeddings will keep their original values
        # - ~10% of embeddings will be set to 0 
        # This helps prevent overfitting by forcing the model to not rely too heavily on any single position
        # The dropout is applied independently to each element in each embedding vector
        # During inference/evaluation, dropout is disabled and all embeddings are scaled by (1-dropout_rate)
        user_embeddings = self._emb_dropout(user_embeddings)

        # Create mask for valid (non-padding) positions
        # past_ids contains item IDs, where 0 is reserved as the padding token
        # When loading sequences, shorter sequences are padded with 0s to match max length
        # For example, if max length is 5, a sequence of length 3 would be:
        # [item1, item2, item3, 0, 0]
        # The valid_mask will be: [1.0, 1.0, 1.0, 0.0, 0.0]
        valid_mask = (past_ids != 0).unsqueeze(-1).float()  # [B, N, 1]
        
        # Zero out embeddings at padding positions by multiplying with valid_mask
        user_embeddings *= valid_mask

        # Return values:
        # past_lengths: Original sequence lengths tensor [B] indicating number of actual items
        #   For example: If sequences are [[1,2,3,0,0], [1,2,0,0,0]]
        #   past_lengths would be [3, 2] 
        #   This is determined when loading the data, counting non-zero items
        # user_embeddings: Position-aware item embeddings with dropout applied [B, N, D]
        #   - Combines item embeddings and positional embeddings
        #   - Scaled by sqrt(embedding_dim) 
        #   - Has dropout applied for regularization
        #   - Masked to zero out padding positions
        # valid_mask: Binary mask tensor [B, N, 1] where:
        #   - 1.0 indicates valid item positions (past_ids != 0)
        #   - 0.0 indicates padding positions (past_ids == 0)
        #   - Used to zero out embeddings at padding positions
        return past_lengths, user_embeddings, valid_mask


class LearnablePositionalEmbeddingRatedInputFeaturesPreprocessor(
    InputFeaturesPreprocessorModule
):
    def __init__(
        self,
        max_sequence_len: int,
        item_embedding_dim: int,
        dropout_rate: float,
        rating_embedding_dim: int,
        num_ratings: int,
    ) -> None:
        super().__init__()

        self._embedding_dim: int = item_embedding_dim + rating_embedding_dim
        self._pos_emb: torch.nn.Embedding = torch.nn.Embedding(
            max_sequence_len,
            self._embedding_dim,
        )
        self._dropout_rate: float = dropout_rate
        self._emb_dropout = torch.nn.Dropout(p=dropout_rate)
        self._rating_emb: torch.nn.Embedding = torch.nn.Embedding(
            num_ratings,
            rating_embedding_dim,
        )
        self.reset_state()

    def debug_str(self) -> str:
        return f"posir_d{self._dropout_rate}"

    def reset_state(self) -> None:
        truncated_normal(
            self._pos_emb.weight.data,
            mean=0.0,
            std=math.sqrt(1.0 / self._embedding_dim),
        )
        truncated_normal(
            self._rating_emb.weight.data,
            mean=0.0,
            std=math.sqrt(1.0 / self._embedding_dim),
        )

    def forward(
        self,
        past_lengths: torch.Tensor,
        past_ids: torch.Tensor,
        past_embeddings: torch.Tensor,
        past_payloads: Dict[str, torch.Tensor],
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        B, N = past_ids.size()

        user_embeddings = torch.cat(
            [past_embeddings, self._rating_emb(past_payloads["ratings"].int())],
            dim=-1,
        ) * (self._embedding_dim**0.5) + self._pos_emb(
            torch.arange(N, device=past_ids.device).unsqueeze(0).repeat(B, 1)
        )
        user_embeddings = self._emb_dropout(user_embeddings)

        valid_mask = (past_ids != 0).unsqueeze(-1).float()  # [B, N, 1]
        user_embeddings *= valid_mask
        return past_lengths, user_embeddings, valid_mask


class CombinedItemAndRatingInputFeaturesPreprocessor(InputFeaturesPreprocessorModule):
    def __init__(
        self,
        max_sequence_len: int,
        item_embedding_dim: int,
        dropout_rate: float,
        num_ratings: int,
    ) -> None:
        super().__init__()

        self._embedding_dim: int = item_embedding_dim
        # Due to [item_0, rating_0, item_1, rating_1, ...]
        self._pos_emb: torch.nn.Embedding = torch.nn.Embedding(
            max_sequence_len * 2,
            self._embedding_dim,
        )
        self._dropout_rate: float = dropout_rate
        self._emb_dropout = torch.nn.Dropout(p=dropout_rate)
        self._rating_emb: torch.nn.Embedding = torch.nn.Embedding(
            num_ratings,
            item_embedding_dim,
        )
        self.reset_state()

    def debug_str(self) -> str:
        return f"combir_d{self._dropout_rate}"

    def reset_state(self) -> None:
        truncated_normal(
            self._pos_emb.weight.data,
            mean=0.0,
            std=math.sqrt(1.0 / self._embedding_dim),
        )
        truncated_normal(
            self._rating_emb.weight.data,
            mean=0.0,
            std=math.sqrt(1.0 / self._embedding_dim),
        )

    def get_preprocessed_ids(
        self,
        past_lengths: torch.Tensor,
        past_ids: torch.Tensor,
        past_embeddings: torch.Tensor,
        past_payloads: Dict[str, torch.Tensor],
    ) -> torch.Tensor:
        """
        Returns (B, N * 2,) x int64.
        """
        B, N = past_ids.size()
        return torch.cat(
            [
                past_ids.unsqueeze(2),  # (B, N, 1)
                past_payloads["ratings"].to(past_ids.dtype).unsqueeze(2),
            ],
            dim=2,
        ).reshape(B, N * 2)

    def get_preprocessed_masks(
        self,
        past_lengths: torch.Tensor,
        past_ids: torch.Tensor,
        past_embeddings: torch.Tensor,
        past_payloads: Dict[str, torch.Tensor],
    ) -> torch.Tensor:
        """
        Returns (B, N * 2,) x bool.
        """
        B, N = past_ids.size()
        return (past_ids != 0).unsqueeze(2).expand(-1, -1, 2).reshape(B, N * 2)

    def forward(
        self,
        past_lengths: torch.Tensor,
        past_ids: torch.Tensor,
        past_embeddings: torch.Tensor,
        past_payloads: Dict[str, torch.Tensor],
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        B, N = past_ids.size()
        D = past_embeddings.size(-1)

        user_embeddings = torch.cat(
            [
                past_embeddings,  # (B, N, D)
                self._rating_emb(past_payloads["ratings"].int()),
            ],
            dim=2,
        ) * (self._embedding_dim**0.5)
        user_embeddings = user_embeddings.view(B, N * 2, D)
        user_embeddings = user_embeddings + self._pos_emb(
            torch.arange(N * 2, device=past_ids.device).unsqueeze(0).repeat(B, 1)
        )
        user_embeddings = self._emb_dropout(user_embeddings)

        valid_mask = (
            self.get_preprocessed_masks(
                past_lengths,
                past_ids,
                past_embeddings,
                past_payloads,
            )
            .unsqueeze(2)
            .float()
        )  # (B, N * 2, 1,)
        user_embeddings *= valid_mask
        return past_lengths * 2, user_embeddings, valid_mask

</content>

<content full_path="generative_recommenders/modeling/sequential/hstu.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# pyre-unsafe

"""
Implements HSTU (Hierarchical Sequential Transduction Unit) in
Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations
(https://arxiv.org/abs/2402.17152, ICML'24).
"""

import abc
import math
from typing import Callable, Dict, List, Optional, Tuple, Union

import torch
import torch.nn.functional as F

from generative_recommenders.modeling.sequential.embedding_modules import (
    EmbeddingModule,
)
from generative_recommenders.modeling.sequential.input_features_preprocessors import (
    InputFeaturesPreprocessorModule,
)
from generative_recommenders.modeling.sequential.output_postprocessors import (
    OutputPostprocessorModule,
)
from generative_recommenders.modeling.sequential.utils import get_current_embeddings
from generative_recommenders.modeling.similarity_module import (
    SequentialEncoderWithLearnedSimilarityModule,
)
from generative_recommenders.rails.similarities.module import SimilarityModule


TIMESTAMPS_KEY = "timestamps"


class RelativeAttentionBiasModule(torch.nn.Module):
    @abc.abstractmethod
    def forward(
        self,
        all_timestamps: torch.Tensor,
    ) -> torch.Tensor:
        """
        Args:
            all_timestamps: [B, N] x int64
        Returns:
            torch.float tensor broadcastable to [B, N, N]
        """
        pass


class RelativePositionalBias(RelativeAttentionBiasModule):
    def __init__(self, max_seq_len: int) -> None:
        super().__init__()

        self._max_seq_len: int = max_seq_len
        self._w = torch.nn.Parameter(
            torch.empty(2 * max_seq_len - 1).normal_(mean=0, std=0.02),
        )

    def forward(
        self,
        all_timestamps: torch.Tensor,
    ) -> torch.Tensor:
        del all_timestamps
        n: int = self._max_seq_len
        t = F.pad(self._w[: 2 * n - 1], [0, n]).repeat(n)
        t = t[..., :-n].reshape(1, n, 3 * n - 2)
        r = (2 * n - 1) // 2
        return t[..., r:-r]


class RelativeBucketedTimeAndPositionBasedBias(RelativeAttentionBiasModule):
    """
    Bucketizes timespans based on ts(next-item) - ts(current-item).
    """

    def __init__(
        self,
        max_seq_len: int,
        num_buckets: int,
        bucketization_fn: Callable[[torch.Tensor], torch.Tensor],
    ) -> None:
        super().__init__()

        self._max_seq_len: int = max_seq_len
        self._ts_w = torch.nn.Parameter(
            torch.empty(num_buckets + 1).normal_(mean=0, std=0.02),
        )
        self._pos_w = torch.nn.Parameter(
            torch.empty(2 * max_seq_len - 1).normal_(mean=0, std=0.02),
        )
        self._num_buckets: int = num_buckets
        self._bucketization_fn: Callable[[torch.Tensor], torch.Tensor] = (
            bucketization_fn
        )

    def forward(
        self,
        all_timestamps: torch.Tensor,
    ) -> torch.Tensor:
        """
        Args:
            all_timestamps: (B, N).
        Returns:
            (B, N, N).
        """
        B = all_timestamps.size(0)
        N = self._max_seq_len
        t = F.pad(self._pos_w[: 2 * N - 1], [0, N]).repeat(N)
        t = t[..., :-N].reshape(1, N, 3 * N - 2)
        r = (2 * N - 1) // 2

        # [B, N + 1] to simplify tensor manipulations.
        ext_timestamps = torch.cat(
            [all_timestamps, all_timestamps[:, N - 1 : N]], dim=1
        )
        # causal masking. Otherwise [:, :-1] - [:, 1:] works
        bucketed_timestamps = torch.clamp(
            self._bucketization_fn(
                ext_timestamps[:, 1:].unsqueeze(2) - ext_timestamps[:, :-1].unsqueeze(1)
            ),
            min=0,
            max=self._num_buckets,
        ).detach()
        rel_pos_bias = t[:, :, r:-r]
        rel_ts_bias = torch.index_select(
            self._ts_w, dim=0, index=bucketed_timestamps.view(-1)
        ).view(B, N, N)
        return rel_pos_bias + rel_ts_bias


HSTUCacheState = Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]


def _hstu_attention_maybe_from_cache(
    num_heads: int,
    attention_dim: int,
    linear_dim: int,
    q: torch.Tensor,
    k: torch.Tensor,
    v: torch.Tensor,
    cached_q: Optional[torch.Tensor],
    cached_k: Optional[torch.Tensor],
    delta_x_offsets: Optional[Tuple[torch.Tensor, torch.Tensor]],
    x_offsets: torch.Tensor,
    all_timestamps: Optional[torch.Tensor],
    invalid_attn_mask: torch.Tensor,
    rel_attn_bias: RelativeAttentionBiasModule,
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    B: int = x_offsets.size(0) - 1
    n: int = invalid_attn_mask.size(-1)
    if delta_x_offsets is not None:
        padded_q, padded_k = cached_q, cached_k
        flattened_offsets = delta_x_offsets[1] + torch.arange(
            start=0,
            end=B * n,
            step=n,
            device=delta_x_offsets[1].device,
            dtype=delta_x_offsets[1].dtype,
        )
        assert isinstance(padded_q, torch.Tensor)
        assert isinstance(padded_k, torch.Tensor)
        padded_q = (
            padded_q.view(B * n, -1)
            .index_copy_(
                dim=0,
                index=flattened_offsets,
                source=q,
            )
            .view(B, n, -1)
        )
        padded_k = (
            padded_k.view(B * n, -1)
            .index_copy_(
                dim=0,
                index=flattened_offsets,
                source=k,
            )
            .view(B, n, -1)
        )
    else:
        padded_q = torch.ops.fbgemm.jagged_to_padded_dense(
            values=q, offsets=[x_offsets], max_lengths=[n], padding_value=0.0
        )
        padded_k = torch.ops.fbgemm.jagged_to_padded_dense(
            values=k, offsets=[x_offsets], max_lengths=[n], padding_value=0.0
        )

    qk_attn = torch.einsum(
        "bnhd,bmhd->bhnm",
        padded_q.view(B, n, num_heads, attention_dim),
        padded_k.view(B, n, num_heads, attention_dim),
    )
    if all_timestamps is not None:
        qk_attn = qk_attn + rel_attn_bias(all_timestamps).unsqueeze(1)
    qk_attn = F.silu(qk_attn) / n
    qk_attn = qk_attn * invalid_attn_mask.unsqueeze(0).unsqueeze(0)
    attn_output = torch.ops.fbgemm.dense_to_jagged(
        torch.einsum(
            "bhnm,bmhd->bnhd",
            qk_attn,
            torch.ops.fbgemm.jagged_to_padded_dense(v, [x_offsets], [n]).reshape(
                B, n, num_heads, linear_dim
            ),
        ).reshape(B, n, num_heads * linear_dim),
        [x_offsets],
    )[0]
    return attn_output, padded_q, padded_k


class SequentialTransductionUnitJagged(torch.nn.Module):
    def __init__(
        self,
        embedding_dim: int,
        linear_hidden_dim: int,
        attention_dim: int,
        dropout_ratio: float,
        attn_dropout_ratio: float,
        num_heads: int,
        linear_activation: str,
        relative_attention_bias_module: Optional[RelativeAttentionBiasModule] = None,
        normalization: str = "rel_bias",
        linear_config: str = "uvqk",
        concat_ua: bool = False,
        epsilon: float = 1e-6,
        max_length: Optional[int] = None,
    ) -> None:
        """Sequential Transduction Unit with Jagged Tensor Support.

        This module implements a sequential processing unit that can handle variable-length sequences
        using jagged tensors. It combines linear projections and multi-head attention mechanisms.

        Args:
            embedding_dim (int): Dimension of input embeddings
            linear_hidden_dim (int): Hidden dimension for linear projections
            attention_dim (int): Dimension of attention mechanism
            dropout_ratio (float): Dropout probability for regularization
            attn_dropout_ratio (float): Dropout probability specific to attention
            num_heads (int): Number of attention heads
            linear_activation (str): Activation function for linear layers
            relative_attention_bias_module (Optional[RelativeAttentionBiasModule]): 
                Module for relative position attention bias
            normalization (str): Type of normalization to use, defaults to "rel_bias"
            linear_config (str): Configuration for linear projections, defaults to "uvqk"
            concat_ua (bool): Whether to concatenate update and attention outputs
            epsilon (float): Small constant for numerical stability
            max_length (Optional[int]): Maximum sequence length, if any
        """
        super().__init__()
        self._embedding_dim: int = embedding_dim
        self._linear_dim: int = linear_hidden_dim
        self._attention_dim: int = attention_dim
        self._dropout_ratio: float = dropout_ratio
        self._attn_dropout_ratio: float = attn_dropout_ratio
        self._num_heads: int = num_heads
        self._rel_attn_bias: Optional[RelativeAttentionBiasModule] = (
            relative_attention_bias_module
        )
        self._normalization: str = normalization
        self._linear_config: str = linear_config
        if self._linear_config == "uvqk":
            # The uvqk config combines 4 projection matrices into a single parameter matrix:
            # u: Update projection - maps input to hidden state (linear_hidden_dim * num_heads)
            # v: Value projection - maps input to values for attention (linear_hidden_dim * num_heads)
            # q: Query projection - maps input to queries for attention (attention_dim * num_heads)
            # k: Key projection - maps input to keys for attention (attention_dim * num_heads)
            #
            # The shape is [embedding_dim, (2*linear_hidden_dim + 2*attention_dim)*num_heads] because:
            # - Input has shape [embedding_dim]
            # - We need 4 projection matrices:
            #   - u and v: Each maps to linear_hidden_dim * num_heads
            #     -> 2 * linear_hidden_dim * num_heads total
            #   - q and k: Each maps to attention_dim * num_heads
            #     -> 2 * attention_dim * num_heads total
            # - Total output dim is sum of all projections:
            #   (2*linear_hidden_dim + 2*attention_dim) * num_heads
            #
            self._uvqk: torch.nn.Parameter = torch.nn.Parameter(
                torch.empty(
                    (
                        embedding_dim,
                        linear_hidden_dim * 2 * num_heads
                        + attention_dim * num_heads * 2,
                    )
                ).normal_(mean=0, std=0.02),
            )
        else:
            raise ValueError(f"Unknown linear_config {self._linear_config}")
        self._linear_activation: str = linear_activation
        self._concat_ua: bool = concat_ua
        self._o = torch.nn.Linear(
            in_features=linear_hidden_dim * num_heads * (3 if concat_ua else 1),
            out_features=embedding_dim,
        )
        torch.nn.init.xavier_uniform_(self._o.weight)
        self._eps: float = epsilon

    def _norm_input(self, x: torch.Tensor) -> torch.Tensor:
        return F.layer_norm(x, normalized_shape=[self._embedding_dim], eps=self._eps)

    def _norm_attn_output(self, x: torch.Tensor) -> torch.Tensor:
        return F.layer_norm(
            x, normalized_shape=[self._linear_dim * self._num_heads], eps=self._eps
        )

    def forward(  # pyre-ignore [3]
        self,
        x: torch.Tensor,
        x_offsets: torch.Tensor,
        all_timestamps: Optional[torch.Tensor],
        invalid_attn_mask: torch.Tensor,
        delta_x_offsets: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
        cache: Optional[HSTUCacheState] = None,
        return_cache_states: bool = False,
    ):
        """
        Args:
            x: (\sum_i N_i, D) x float.
            x_offsets: (B + 1) x int32.
            all_timestamps: optional (B, N) x int64.
            invalid_attn_mask: (B, N, N) x float, each element in {0, 1}.
            delta_x_offsets: optional 2-tuple ((B,) x int32, (B,) x int32).
                For the 1st element in the tuple, each element is in [0, x_offsets[-1]). For the
                2nd element in the tuple, each element is in [0, N).
            cache: Optional 4-tuple of (v, padded_q, padded_k, output) from prior runs,
                where all except padded_q, padded_k are jagged.
        Returns:
            x' = f(x), (\sum_i N_i, D) x float.
            
        # This method implements the forward pass of the HSTU (Hierarchical Sequential Transduction Unit) layer
        #
        # Args:
        #   x: Input tensor containing item embeddings, shape (sum_i N_i, D) where:
        #      - N_i is sequence length for batch i
        #      - D is embedding dimension
        #      - Flattened across batches
        #
        #   x_offsets: Tensor of size (B+1) containing offsets to index into x
        #              Used to identify sequence boundaries in the flattened input
        #
        #   all_timestamps: Optional tensor of timestamps for each item, shape (B, N)
        #                  Used for temporal attention patterns
        #
        #   invalid_attn_mask: Binary mask of shape (B, N, N) where:
        #                      - 1 indicates positions that should not attend to each other
        #                      - 0 indicates valid attention pairs
        #                      Used to prevent attending to padding or future items
        #
        #   delta_x_offsets: Optional tuple of tensors for incremental decoding:
        #                    - First tensor: Indices into x for new items, shape (B,)
        #                    - Second tensor: Position indices for new items, shape (B,)
        #
        #   cache: Optional cache state from previous forward passes, containing:
        #          - Cached value (v) projections
        #          - Cached query (q) projections 
        #          - Cached key (k) projections
        #          - Cached layer outputs
        #          Used for efficient incremental decoding
        #
        #   return_cache_states: Whether to return cache tensors for future forward passes
        #
        # Returns:
        #   Transformed embeddings with same shape as input x
        #   Optional cache state if return_cache_states is True
        """
        n: int = invalid_attn_mask.size(-1)
        cached_q = None
        cached_k = None
        if delta_x_offsets is not None:
            # In this case, for all the following code, x, u, v, q, k become restricted to
            # [delta_x_offsets[0], :].
            assert cache is not None
            x = x[delta_x_offsets[0], :]
            cached_v, cached_q, cached_k, cached_outputs = cache

        normed_x = self._norm_input(x)

        if self._linear_config == "uvqk":
            batched_mm_output = torch.mm(normed_x, self._uvqk)
            if self._linear_activation == "silu":
                batched_mm_output = F.silu(batched_mm_output)
            elif self._linear_activation == "none":
                batched_mm_output = batched_mm_output
            u, v, q, k = torch.split(
                batched_mm_output,
                [
                    self._linear_dim * self._num_heads,
                    self._linear_dim * self._num_heads,
                    self._attention_dim * self._num_heads,
                    self._attention_dim * self._num_heads,
                ],
                dim=1,
            )
        else:
            raise ValueError(f"Unknown self._linear_config {self._linear_config}")

        if delta_x_offsets is not None:
            v = cached_v.index_copy_(dim=0, index=delta_x_offsets[0], source=v)

        B: int = x_offsets.size(0) - 1
        if self._normalization == "rel_bias" or self._normalization == "hstu_rel_bias":
            assert self._rel_attn_bias is not None
            attn_output, padded_q, padded_k = _hstu_attention_maybe_from_cache(
                num_heads=self._num_heads,
                attention_dim=self._attention_dim,
                linear_dim=self._linear_dim,
                q=q,
                k=k,
                v=v,
                cached_q=cached_q,
                cached_k=cached_k,
                delta_x_offsets=delta_x_offsets,
                x_offsets=x_offsets,
                all_timestamps=all_timestamps,
                invalid_attn_mask=invalid_attn_mask,
                rel_attn_bias=self._rel_attn_bias,
            )
        elif self._normalization == "softmax_rel_bias":
            if delta_x_offsets is not None:
                B = x_offsets.size(0) - 1
                padded_q, padded_k = cached_q, cached_k
                flattened_offsets = delta_x_offsets[1] + torch.arange(
                    start=0,
                    end=B * n,
                    step=n,
                    device=delta_x_offsets[1].device,
                    dtype=delta_x_offsets[1].dtype,
                )
                assert padded_q is not None
                assert padded_k is not None
                padded_q = (
                    padded_q.view(B * n, -1)
                    .index_copy_(
                        dim=0,
                        index=flattened_offsets,
                        source=q,
                    )
                    .view(B, n, -1)
                )
                padded_k = (
                    padded_k.view(B * n, -1)
                    .index_copy_(
                        dim=0,
                        index=flattened_offsets,
                        source=k,
                    )
                    .view(B, n, -1)
                )
            else:
                padded_q = torch.ops.fbgemm.jagged_to_padded_dense(
                    values=q, offsets=[x_offsets], max_lengths=[n], padding_value=0.0
                )
                padded_k = torch.ops.fbgemm.jagged_to_padded_dense(
                    values=k, offsets=[x_offsets], max_lengths=[n], padding_value=0.0
                )

            qk_attn = torch.einsum("bnd,bmd->bnm", padded_q, padded_k)
            if self._rel_attn_bias is not None:
                qk_attn = qk_attn + self._rel_attn_bias(all_timestamps)
            qk_attn = F.softmax(qk_attn / math.sqrt(self._attention_dim), dim=-1)
            qk_attn = qk_attn * invalid_attn_mask
            attn_output = torch.ops.fbgemm.dense_to_jagged(
                torch.bmm(
                    qk_attn,
                    torch.ops.fbgemm.jagged_to_padded_dense(v, [x_offsets], [n]),
                ),
                [x_offsets],
            )[0]
        else:
            raise ValueError(f"Unknown normalization method {self._normalization}")

        attn_output = (
            attn_output
            if delta_x_offsets is None
            else attn_output[delta_x_offsets[0], :]
        )
        if self._concat_ua:
            a = self._norm_attn_output(attn_output)
            o_input = torch.cat([u, a, u * a], dim=-1)
        else:
            o_input = u * self._norm_attn_output(attn_output)

        new_outputs = (
            self._o(
                F.dropout(
                    o_input,
                    p=self._dropout_ratio,
                    training=self.training,
                )
            )
            + x
        )

        if delta_x_offsets is not None:
            new_outputs = cached_outputs.index_copy_(
                dim=0, index=delta_x_offsets[0], source=new_outputs
            )

        if return_cache_states and delta_x_offsets is None:
            v = v.contiguous()

        return new_outputs, (v, padded_q, padded_k, new_outputs)


class HSTUJagged(torch.nn.Module):
    def __init__(
        self,
        modules: List[SequentialTransductionUnitJagged],
        autocast_dtype: Optional[torch.dtype],
    ) -> None:
        super().__init__()

        self._attention_layers: torch.nn.ModuleList = torch.nn.ModuleList(
            modules=modules
        )
        self._autocast_dtype: Optional[torch.dtype] = autocast_dtype

    def jagged_forward(
        self,
        x: torch.Tensor,
        x_offsets: torch.Tensor,
        all_timestamps: Optional[torch.Tensor],
        invalid_attn_mask: torch.Tensor,
        delta_x_offsets: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
        cache: Optional[List[HSTUCacheState]] = None,
        return_cache_states: bool = False,
    ) -> Tuple[torch.Tensor, List[HSTUCacheState]]:
        """
        Args:
            x: (\sum_i N_i, D) x float
            x_offsets: (B + 1) x int32
            all_timestamps: (B, 1 + N) x int64
            invalid_attn_mask: (B, N, N) x float, each element in {0, 1}
            return_cache_states: bool. True if we should return cache states.

        Returns:
            x' = f(x), (\sum_i N_i, D) x float
        """
        cache_states: List[HSTUCacheState] = []

        with torch.autocast(
            "cuda",
            enabled=self._autocast_dtype is not None,
            dtype=self._autocast_dtype or torch.float16,
        ):
            for i, layer in enumerate(self._attention_layers):
                x, cache_states_i = layer(
                    x=x,
                    x_offsets=x_offsets,
                    all_timestamps=all_timestamps,
                    invalid_attn_mask=invalid_attn_mask,
                    delta_x_offsets=delta_x_offsets,
                    cache=cache[i] if cache is not None else None,
                    return_cache_states=return_cache_states,
                )
                if return_cache_states:
                    cache_states.append(cache_states_i)

        return x, cache_states

    def forward(
        self,
        x: torch.Tensor,
        x_offsets: torch.Tensor,
        all_timestamps: Optional[torch.Tensor],
        invalid_attn_mask: torch.Tensor,
        delta_x_offsets: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
        cache: Optional[List[HSTUCacheState]] = None,
        return_cache_states: bool = False,
    ) -> Tuple[torch.Tensor, List[HSTUCacheState]]:
        """
        Args:
            x: (B, N, D) x float.
            x_offsets: (B + 1) x int32.
            all_timestamps: (B, 1 + N) x int64
            invalid_attn_mask: (B, N, N) x float, each element in {0, 1}.
        Returns:
            x' = f(x), (B, N, D) x float
        """
        if len(x.size()) == 3:
            x = torch.ops.fbgemm.dense_to_jagged(x, [x_offsets])[0]

        jagged_x, cache_states = self.jagged_forward(
            x=x,
            x_offsets=x_offsets,
            all_timestamps=all_timestamps,
            invalid_attn_mask=invalid_attn_mask,
            delta_x_offsets=delta_x_offsets,
            cache=cache,
            return_cache_states=return_cache_states,
        )
        y = torch.ops.fbgemm.jagged_to_padded_dense(
            values=jagged_x,
            offsets=[x_offsets],
            max_lengths=[invalid_attn_mask.size(1)],
            padding_value=0.0,
        )
        return y, cache_states


class HSTU(SequentialEncoderWithLearnedSimilarityModule):
    """
    Implements HSTU (Hierarchical Sequential Transduction Unit) in
    Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations,
    https://arxiv.org/abs/2402.17152.

    Note that this implementation is intended for reproducing experiments in
    the traditional sequential recommender setting (Section 4.1.1), and does
    not yet use optimized kernels discussed in the paper.
    """

    def __init__(
        self,
        max_sequence_len: int,
        max_output_len: int,
        embedding_dim: int,
        num_blocks: int,
        num_heads: int,
        linear_dim: int,
        attention_dim: int,
        normalization: str,
        linear_config: str,
        linear_activation: str,
        linear_dropout_rate: float,
        attn_dropout_rate: float,
        embedding_module: EmbeddingModule,
        similarity_module: SimilarityModule,
        input_features_preproc_module: InputFeaturesPreprocessorModule,
        output_postproc_module: OutputPostprocessorModule,
        enable_relative_attention_bias: bool = True,
        concat_ua: bool = False,
        verbose: bool = True,
    ) -> None:
        super().__init__(ndp_module=similarity_module)

        self._embedding_dim: int = embedding_dim
        self._item_embedding_dim: int = embedding_module.item_embedding_dim
        self._max_sequence_length: int = max_sequence_len
        self._embedding_module: EmbeddingModule = embedding_module
        self._input_features_preproc: InputFeaturesPreprocessorModule = (
            input_features_preproc_module
        )
        self._output_postproc: OutputPostprocessorModule = output_postproc_module
        self._num_blocks: int = num_blocks
        self._num_heads: int = num_heads
        self._dqk: int = attention_dim
        self._dv: int = linear_dim
        self._linear_activation: str = linear_activation
        self._linear_dropout_rate: float = linear_dropout_rate
        self._attn_dropout_rate: float = attn_dropout_rate
        self._enable_relative_attention_bias: bool = enable_relative_attention_bias
        self._hstu = HSTUJagged(
            # Main HSTU transformer module that processes sequences through multiple blocks
            # 
            # 1. Takes input embeddings and processes them through a stack of transformer blocks
            # 2. Each block contains self-attention and feed-forward layers
            # 3. Supports incremental generation via caching mechanism
            # 4. Optionally handles temporal information in attention
            # 5. Returns transformed sequence embeddings
            #
            # Args:
            #   modules: List of SequentialTransductionUnit blocks that make up the transformer
            #   autocast_dtype: Optional dtype for autocast optimization
            #   embedding_dim: Dimension of input embeddings
            #   linear_hidden_dim: Hidden dimension for feed-forward layers
            #   attention_dim: Dimension of attention key/query vectors
            #   normalization: Type of normalization to use ("rel_bias", "hstu_rel_bias", etc)
            #   linear_config: Configuration for linear layers ("uvqk", etc)
            #   linear_activation: Activation function for linear layers ("silu", "none", etc)
            #   num_heads: Number of attention heads
            #   relative_attention_bias_module: Optional module for relative position/time biases
            #   dropout_ratio: Dropout probability for linear layers
            #   attn_dropout_ratio: Dropout probability for attention
            #   concat_ua: Whether to concatenate update and accumulate vectors
            #
            # Returns:
            #   Transformed sequence embeddings with shape (batch_size, seq_len, embedding_dim)
            #
            modules=[
                SequentialTransductionUnitJagged(
                    # SequentialTransductionUnitJagged is a transformer block that processes sequences through multiple blocks
                    #
                    # Args:
                    #   embedding_dim: Dimension of input embeddings
                    #   linear_hidden_dim: Hidden dimension for feed-forward layers
                    #   attention_dim: Dimension of attention key/query vectors
                    #   normalization: Type of normalization to use ("rel_bias", "hstu_rel_bias", etc)
                    #   linear_config: Configuration for linear layers ("uvqk", etc)
                    #   linear_activation: Activation function for linear layers ("silu", "none", etc)
                    #   num_heads: Number of attention heads
                    #   relative_attention_bias_module: Optional module for relative position/time biases
                    #   dropout_ratio: Dropout probability for linear layers
                    #   attn_dropout_ratio: Dropout probability for attention
                    #   concat_ua: Whether to concatenate update and accumulate vectors
                    #
                    # Returns:
                    #   Transformed sequence embeddings with shape (batch_size, seq_len, embedding_dim)
                    embedding_dim=self._embedding_dim,
                    linear_hidden_dim=linear_dim,
                    attention_dim=attention_dim,
                    normalization=normalization,
                    linear_config=linear_config,
                    linear_activation=linear_activation,
                    num_heads=num_heads,
                    # TODO: change to lambda x.
                    relative_attention_bias_module=(
                        RelativeBucketedTimeAndPositionBasedBias(
                            max_seq_len=max_sequence_len
                            + max_output_len,  # accounts for next item.
                            num_buckets=128,
                            bucketization_fn=lambda x: (
                                torch.log(torch.abs(x).clamp(min=1)) / 0.301
                            ).long(),
                        )
                        if enable_relative_attention_bias
                        else None
                    ),
                    dropout_ratio=linear_dropout_rate,
                    attn_dropout_ratio=attn_dropout_rate,
                    concat_ua=concat_ua,
                )
                for _ in range(num_blocks)
            ],
            autocast_dtype=None,
        )
        # causal forward, w/ +1 for padding.
        self.register_buffer(
            "_attn_mask",
            torch.triu(
                torch.ones(
                    (
                        self._max_sequence_length + max_output_len,
                        self._max_sequence_length + max_output_len,
                    ),
                    dtype=torch.bool,
                ),
                diagonal=1,
            ),
        )
        self._verbose: bool = verbose
        self.reset_params()

    def reset_params(self) -> None:
        for name, params in self.named_parameters():
            if ("_hstu" in name) or ("_embedding_module" in name):
                if self._verbose:
                    print(f"Skipping init for {name}")
                continue
            try:
                torch.nn.init.xavier_normal_(params.data)
                if self._verbose:
                    print(
                        f"Initialize {name} as xavier normal: {params.data.size()} params"
                    )
            except:
                if self._verbose:
                    print(f"Failed to initialize {name}: {params.data.size()} params")

    def get_item_embeddings(self, item_ids: torch.Tensor) -> torch.Tensor:
        return self._embedding_module.get_item_embeddings(item_ids)

    def debug_str(self) -> str:
        debug_str = (
            f"HSTU-b{self._num_blocks}-h{self._num_heads}-dqk{self._dqk}-dv{self._dv}"
            + f"-l{self._linear_activation}d{self._linear_dropout_rate}"
            + f"-ad{self._attn_dropout_rate}"
        )
        if not self._enable_relative_attention_bias:
            debug_str += "-norab"
        return debug_str

    def generate_user_embeddings(
        self,
        past_lengths: torch.Tensor,
        past_ids: torch.Tensor,
        past_embeddings: torch.Tensor,
        past_payloads: Dict[str, torch.Tensor],
        delta_x_offsets: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
        cache: Optional[List[HSTUCacheState]] = None,
        return_cache_states: bool = False,
    ) -> Tuple[torch.Tensor, List[HSTUCacheState]]:
        """
        Generates user embeddings from sequence of past interactions.

        This method processes user interaction sequences through the HSTU model to generate
        contextualized user embeddings. The key steps are:

        1. Input Processing:
           - Takes past item IDs, embeddings, lengths and payloads as input
           - Processes through input preprocessor to add positional embeddings
           - Handles variable length sequences via past_lengths

        2. HSTU Processing:
           - Passes preprocessed embeddings through HSTU transformer blocks
           - Uses cumulative sum of lengths to track sequence boundaries
           - Applies attention masking to prevent invalid attention
           - Optionally uses cached states for incremental decoding

        3. Output Processing:
           - Processes HSTU outputs through output postprocessor
           - Returns final user embeddings and optional cache states

        Args:
            past_lengths: [B] Tensor of sequence lengths for each batch
            past_ids: [B,N] Tensor of item IDs in each sequence
            past_embeddings: [B,N,D] Tensor of item embeddings
            past_payloads: Dict of additional sequence features
            delta_x_offsets: Optional tuple of tensors for incremental processing
            cache: Optional list of cached HSTU states
            return_cache_states: Whether to return updated cache states

        Returns:
            Tuple of:
            - [B,N,D] Tensor of processed user embeddings
            - List of updated cache states (if return_cache_states=True)

        The method enables both full sequence processing and incremental generation
        via the caching mechanism.
        """
        device = past_lengths.device
        float_dtype = past_embeddings.dtype
        B, N, _ = past_embeddings.size()

        past_lengths, user_embeddings, _ = self._input_features_preproc(
            # Process input sequence through input preprocessor
            #
            # The input preprocessor:
            # 1. Takes raw sequence inputs (lengths, IDs, embeddings, payloads)
            # 2. Adds learnable positional embeddings to capture sequence order
            # 3. Applies dropout for regularization
            # 4. Returns processed embeddings and sequence metadata
            #
            # Args:
            #   past_lengths: [B] Tensor of sequence lengths
            #   past_ids: [B,N] Tensor of item IDs
            #   past_embeddings: [B,N,D] Tensor of item embeddings
            #   past_payloads: Dict of additional sequence features
            #
            # Returns:
            #   Tuple of:
            #   - [B] Tensor of processed sequence lengths
            #   - [B,N,D] Tensor of processed embeddings with positional info
            #   - Attention mask for valid positions
            past_lengths=past_lengths,
            past_ids=past_ids,
            past_embeddings=past_embeddings,
            past_payloads=past_payloads,
        )

        float_dtype = user_embeddings.dtype
        # Call the HSTU module
        user_embeddings, cached_states = self._hstu(
            # Pass sequence through HSTU transformer blocks
            #
            # Args:
            #   x: [B,N,D] Tensor of input embeddings with positional info
            #   x_offsets: [B] Tensor of cumulative sequence lengths for attention
            #   all_timestamps: Optional [B,N] Tensor of timestamps for temporal attention
            #   invalid_attn_mask: [N,N] Tensor masking invalid attention positions
            #   delta_x_offsets: Optional tuple for incremental processing
            #   cache: Optional list of cached HSTU states
            #   return_cache_states: Whether to return updated cache
            #
            # Returns:
            #   Tuple of:
            #   - [B,N,D] Tensor of transformed sequence embeddings
            #   - List of updated cache states if requested
            #
            # The HSTU transformer:
            # 1. Processes sequences through multiple transformer blocks
            # 2. Each block contains self-attention and feed-forward layers
            # 3. Supports incremental generation via caching mechanism
            # 4. Optionally handles temporal information in attention
            x=user_embeddings,
            x_offsets=torch.ops.fbgemm.asynchronous_complete_cumsum(past_lengths),
            all_timestamps=(
                past_payloads[TIMESTAMPS_KEY]
                if TIMESTAMPS_KEY in past_payloads
                else None
            ),
            invalid_attn_mask=1.0 - self._attn_mask.to(float_dtype),
            delta_x_offsets=delta_x_offsets,
            cache=cache,
            return_cache_states=return_cache_states,
        )
        return self._output_postproc(user_embeddings), cached_states

    def forward(
        self,
        past_lengths: torch.Tensor,
        past_ids: torch.Tensor,
        past_embeddings: torch.Tensor,
        past_payloads: Dict[str, torch.Tensor],
        batch_id: Optional[int] = None,
    ) -> torch.Tensor:
        """
        Runs the main encoder.

        Args:
            past_lengths: (B,) x int64
            past_ids: (B, N,) x int64 where the latest engaged ids come first. In
                particular, past_ids[i, past_lengths[i] - 1] should correspond to
                the latest engaged values.
            past_embeddings: (B, N, D) x float or (\sum_b N_b, D) x float.
            past_payloads: implementation-specific keyed tensors of shape (B, N, ...).

        Returns:
            encoded_embeddings of [B, N, D].
        """
        encoded_embeddings, _ = self.generate_user_embeddings(
            past_lengths=past_lengths,
            past_ids=past_ids,
            past_embeddings=past_embeddings,
            past_payloads=past_payloads,
        )
        return encoded_embeddings

    def _encode(
        self,
        past_lengths: torch.Tensor,
        past_ids: torch.Tensor,
        past_embeddings: torch.Tensor,
        past_payloads: Dict[str, torch.Tensor],
        delta_x_offsets: Optional[Tuple[torch.Tensor, torch.Tensor]],
        cache: Optional[List[HSTUCacheState]],
        return_cache_states: bool,
    ) -> Union[torch.Tensor, Tuple[torch.Tensor, List[HSTUCacheState]]]:
        """
        Args:
            past_lengths: (B,) x int64.
            past_ids: (B, N,) x int64.
            past_embeddings: (B, N, D,) x float.
            past_payloads: implementation-specific keyed tensors of shape (B, N, ...).
            return_cache_states: bool.

        Returns:
            (B, D) x float, representing embeddings for the current state.
        """
        encoded_seq_embeddings, cache_states = self.generate_user_embeddings(
            past_lengths=past_lengths,
            past_ids=past_ids,
            past_embeddings=past_embeddings,
            past_payloads=past_payloads,
            delta_x_offsets=delta_x_offsets,
            cache=cache,
            return_cache_states=return_cache_states,
        )  # [B, N, D]
        current_embeddings = get_current_embeddings(
            lengths=past_lengths, encoded_embeddings=encoded_seq_embeddings
        )
        if return_cache_states:
            return current_embeddings, cache_states
        else:
            return current_embeddings

    def encode(
        self,
        past_lengths: torch.Tensor,
        past_ids: torch.Tensor,
        past_embeddings: torch.Tensor,
        past_payloads: Dict[str, torch.Tensor],
        delta_x_offsets: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
        cache: Optional[List[HSTUCacheState]] = None,
        return_cache_states: bool = False,
    ) -> Union[torch.Tensor, Tuple[torch.Tensor, List[HSTUCacheState]]]:
        """
        Runs encoder to obtain the current hidden states.

        Args:
            past_lengths: (B,) x int.
            past_ids: (B, N,) x int.
            past_embeddings: (B, N, D) x float.
            past_payloads: implementation-specific keyed tensors of shape (B, N, ...).

        Returns:
            (B, D,) x float, representing encoded states at the most recent time step.
        """
        return self._encode(
            past_lengths=past_lengths,
            past_ids=past_ids,
            past_embeddings=past_embeddings,
            past_payloads=past_payloads,
            delta_x_offsets=delta_x_offsets,
            cache=cache,
            return_cache_states=return_cache_states,
        )

</content>

<content full_path="generative_recommenders/modeling/sequential/autoregressive_losses.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# pyre-unsafe

import abc
from collections import OrderedDict
from typing import List, Tuple

import torch
import torch.nn.functional as F

from generative_recommenders.rails.similarities.module import SimilarityModule

from torch.utils.checkpoint import checkpoint


class NegativesSampler(torch.nn.Module):
    def __init__(self, l2_norm: bool, l2_norm_eps: float) -> None:
        super().__init__()

        self._l2_norm: bool = l2_norm
        self._l2_norm_eps: float = l2_norm_eps

    def normalize_embeddings(self, x: torch.Tensor) -> torch.Tensor:
        return self._maybe_l2_norm(x)

    def _maybe_l2_norm(self, x: torch.Tensor) -> torch.Tensor:
        if self._l2_norm:
            x = x / torch.clamp(
                torch.linalg.norm(x, ord=2, dim=-1, keepdim=True),
                min=self._l2_norm_eps,
            )
        return x

    @abc.abstractmethod
    def debug_str(self) -> str:
        pass

    @abc.abstractmethod
    def process_batch(
        self,
        ids: torch.Tensor,
        presences: torch.Tensor,
        embeddings: torch.Tensor,
    ) -> None:
        pass

    @abc.abstractmethod
    def forward(
        self,
        positive_ids: torch.Tensor,
        num_to_sample: int,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Returns:
            A tuple of (sampled_ids, sampled_negative_embeddings).
        """
        pass


class LocalNegativesSampler(NegativesSampler):
    """
    LocalNegativesSampler is a sampler that samples negative items from the full item set (not just the current batch).
    It uses a stored item embedding table to look up embeddings for randomly sampled negative items.

    The key difference from InBatchNegativesSampler is that this sampler:
    1. Samples negative items randomly from the full item set rather than just items in the batch
    2. Uses a provided item embedding table to look up embeddings for sampled negatives
    3. Does not require processing batches of embeddings since it samples globally
    4. May have higher memory usage since it needs the full item embedding table
    
    InBatchNegativesSampler on the other hand:
    1. Only samples negatives from items that appear in the current batch
    2. Uses pre-computed embeddings passed in process_batch() rather than an embedding table
    3. Requires processing each batch to build the pool of candidate negatives
    4. More memory efficient since it only needs batch embeddings
    """
    def __init__(
        self,
        num_items: int,
        item_emb: torch.nn.Embedding,
        all_item_ids: List[int],
        l2_norm: bool,
        l2_norm_eps: float,
    ) -> None:
        super().__init__(l2_norm=l2_norm, l2_norm_eps=l2_norm_eps)

        self._num_items: int = len(all_item_ids)
        self._item_emb: torch.nn.Embedding = item_emb
        self.register_buffer("_all_item_ids", torch.tensor(all_item_ids))

    def debug_str(self) -> str:
        sampling_debug_str = (
            f"local{f'-l2-eps{self._l2_norm_eps}' if self._l2_norm else ''}"
        )
        return sampling_debug_str

    def process_batch(
        self,
        ids: torch.Tensor,
        presences: torch.Tensor,
        embeddings: torch.Tensor,
    ) -> None:
        pass

    def forward(
        self,
        positive_ids: torch.Tensor,
        num_to_sample: int,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Returns:
            A tuple of (sampled_ids, sampled_negative_embeddings).
        """
        # assert torch.max(torch.abs(self._item_emb(positive_ids) - positive_embeddings)) < 1e-4
        output_shape = positive_ids.size() + (num_to_sample,)
        sampled_offsets = torch.randint(
            low=0,
            high=self._num_items,
            size=output_shape,
            dtype=positive_ids.dtype,
            device=positive_ids.device,
        )
        sampled_ids = self._all_item_ids[sampled_offsets.view(-1)].reshape(output_shape)
        return sampled_ids, self.normalize_embeddings(self._item_emb(sampled_ids))


class InBatchNegativesSampler(NegativesSampler):
    def __init__(
        self,
        l2_norm: bool,
        l2_norm_eps: float,
        dedup_embeddings: bool,
    ) -> None:
        """
        Initialize InBatchNegativesSampler for efficient negative sampling.

        This sampler uses other items in the current batch as negative examples,
        which is memory efficient and can work well in practice.

        Args:
            l2_norm: Whether to L2 normalize embeddings
            l2_norm_eps: Epsilon for L2 normalization numerical stability
            dedup_embeddings: Whether to deduplicate embeddings in batch
                            If True, will only use unique items as negatives
                            If False, may sample same item multiple times

        Key features:
        - Uses items from current batch as negatives instead of sampling from full corpus
        - Option to deduplicate embeddings to avoid repeated negatives
        - L2 normalization of embeddings for stable training
        - Memory efficient since no need to store full item embedding matrix
        """
        super().__init__(l2_norm=l2_norm, l2_norm_eps=l2_norm_eps)

        self._dedup_embeddings: bool = dedup_embeddings

    def debug_str(self) -> str:
        sampling_debug_str = (
            f"in-batch{f'-l2-eps{self._l2_norm_eps}' if self._l2_norm else ''}"
        )
        if self._dedup_embeddings:
            sampling_debug_str += "-dedup"
        return sampling_debug_str

    def process_batch(
        self,
        ids: torch.Tensor,
        presences: torch.Tensor,
        embeddings: torch.Tensor,
    ) -> None:
        """
        Args:
           ids: (N') or (B, N) x int64
           presences: (N') or (B, N) x bool
           embeddings: (N', D) or (B, N, D) x float
        """
        assert ids.size() == presences.size()
        assert ids.size() == embeddings.size()[:-1]
        if self._dedup_embeddings:
            valid_ids = ids[presences]
            unique_ids, unique_ids_inverse_indices = torch.unique(
                input=valid_ids, sorted=False, return_inverse=True
            )
            device = unique_ids.device
            unique_embedding_offsets = torch.empty(
                (unique_ids.numel(),),
                dtype=torch.int64,
                device=device,
            )
            unique_embedding_offsets[unique_ids_inverse_indices] = torch.arange(
                valid_ids.numel(), dtype=torch.int64, device=device
            )
            unique_embeddings = embeddings[presences][unique_embedding_offsets, :]
            self._cached_embeddings = self._maybe_l2_norm(unique_embeddings)
            self._cached_ids = unique_ids
        else:
            self._cached_embeddings = self._maybe_l2_norm(embeddings[presences])
            self._cached_ids = ids[presences]

    def get_all_ids_and_embeddings(self) -> Tuple[torch.Tensor, torch.Tensor]:
        return self._cached_ids, self._cached_embeddings

    def forward(
        self,
        positive_ids: torch.Tensor,
        num_to_sample: int,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Returns:
            A tuple of (sampled_ids, sampled_negative_embeddings,).
        """
        X = self._cached_ids.size(0)
        sampled_offsets = torch.randint(
            low=0,
            high=X,
            size=positive_ids.size() + (num_to_sample,),
            dtype=positive_ids.dtype,
            device=positive_ids.device,
        )
        return (
            self._cached_ids[sampled_offsets],
            self._cached_embeddings[sampled_offsets],
        )


class AutoregressiveLoss(torch.nn.Module):
    @abc.abstractmethod
    def jagged_forward(
        self,
        output_embeddings: torch.Tensor,
        supervision_ids: torch.Tensor,
        supervision_embeddings: torch.Tensor,
        supervision_weights: torch.Tensor,
        negatives_sampler: NegativesSampler,
    ) -> torch.Tensor:
        """
        Variant of forward() when the tensors are already in jagged format.

        Args:
            output_embeddings: [N', D] x float, embeddings for the current
                input sequence.
            supervision_ids: [N'] x int64, (positive) supervision ids.
            supervision_embeddings: [N', D] x float.
            supervision_weights: Optional [N'] x float. Optional weights for
                masking out invalid positions, or reweighting supervision labels.
            negatives_sampler: sampler used to obtain negative examples paired with
                positives.

        Returns:
            (1), loss for the current engaged sequence.
        """
        pass

    @abc.abstractmethod
    def forward(
        self,
        lengths: torch.Tensor,
        output_embeddings: torch.Tensor,
        supervision_ids: torch.Tensor,
        supervision_embeddings: torch.Tensor,
        supervision_weights: torch.Tensor,
        negatives_sampler: NegativesSampler,
    ) -> torch.Tensor:
        """
        Args:
            lengths: [B] x int32 representing number of non-zero elements per row.
            output_embeddings: [B, N, D] x float, embeddings for the current
                input sequence.
            supervision_ids: [B, N] x int64, (positive) supervision ids.
            supervision_embeddings: [B, N, D] x float.
            supervision_weights: Optional [B, N] x float. Optional weights for
                masking out invalid positions, or reweighting supervision labels.
            negatives_sampler: sampler used to obtain negative examples paired with
                positives.

        Returns:
            (1), loss for the current engaged sequence.
        """
        pass


class BCELoss(AutoregressiveLoss):
    def __init__(
        self,
        temperature: float,
        model: SimilarityModule,
    ) -> None:
        """
        Initialize BCELoss module.

        Args:
            temperature: Float scaling factor for logits. Higher values make the model more 
                        confident in its predictions.
            model: SimilarityModule that computes interaction scores between input and target 
                  embeddings.

        BCELoss implements binary cross entropy loss for sequential recommendation.
    
        For each position in the sequence, it:
        1. Computes similarity scores between the input embedding and positive target
        2. Samples negative items and computes similarity scores with those
        The negative sampling process helps create contrastive pairs for training:
        - For each positive item, sample 1 negative item the user hasn't interacted with
        - This provides signal about what items are not relevant for the user
        - The model learns to score positive items higher than negative ones
        3. Applies binary cross entropy loss to encourage high scores for positives 
           and low scores for negatives
        
        This creates a contrastive learning signal that helps the model learn to 
        distinguish between items the user would engage with vs not engage with.
        """
        super().__init__()
        self._temperature: float = temperature
        self._model = model

    def jagged_forward(
        self,
        output_embeddings: torch.Tensor,
        supervision_ids: torch.Tensor,
        supervision_embeddings: torch.Tensor,
        supervision_weights: torch.Tensor,
        negatives_sampler: NegativesSampler,
    ) -> torch.Tensor:
        """
        Forward pass for jagged/variable length sequences.

        This method handles sequences where each batch element can have a different length.
        The input tensors are "jagged" - flattened into 1D tensors where sequential elements 
        are concatenated. Jagged tensors are tensors where the dimensions are not uniform,
        often used in scenarios like sparse data or variable-length sequences.
        
        References "jagged":
        - https://pytorch.org/FBGEMM/fbgemm_gpu-overview/jagged-tensor-ops/JaggedTensorOps.html

        Args:
            output_embeddings: Flattened tensor of shape [N', D] containing embeddings for each 
                position in all sequences, where
                N' is the total number of sequence elements across all batches (flattened)
                D is the embedding dimension for each item
            supervision_ids: Flattened tensor [N'] containing target item IDs for each position
            supervision_embeddings: Flattened tensor [N', D] containing target item embeddings
                supervision_embeddings represents the ground truth/target embeddings that the model
                should learn to predict. The term "supervision" is used because these embeddings
                provide the supervisory signal (labels) for training - they represent the correct
                outputs that guide/supervise the model's learning process through the loss function.
                This is standard terminology in machine learning where "supervised learning" refers
                to training with labeled data.
            supervision_weights: Flattened tensor [N'] containing loss weights for each position
            negatives_sampler: Module for sampling negative items

        Returns:
            Scalar loss averaged over all positions with non-zero weights
        """
        # Validate input tensor shapes match expected dimensions
        # output_embeddings and supervision_embeddings should be [N', D]
        # supervision_ids and supervision_weights should be [N']
        # where N' is total sequence elements across batches
        # and D is embedding dimension
        assert output_embeddings.size() == supervision_embeddings.size()
        assert supervision_ids.size() == supervision_embeddings.size()[:-1]
        assert supervision_ids.size() == supervision_weights.size()

        # Sample negative items for contrastive learning
        # Returns:
        # - sampled_ids: [N', 1] tensor of sampled negative item IDs
        # - sampled_negative_embeddings: [N', 1, D] tensor of embeddings for sampled negatives
        # The negative samples are used to create contrastive pairs with the positive examples
        # for training the model to distinguish between relevant and irrelevant items
        sampled_ids, sampled_negative_embeddings = negatives_sampler(
            positive_ids=supervision_ids,
            num_to_sample=1,
        )

        # Compute logits for positive and negative items
        # Logits are raw prediction scores before applying softmax/sigmoid activation
        # They represent how likely each item is to be the next item in the sequence
        # Higher logit = model thinks that item is more likely to be next
        # 
        # positive_logits: [N'] tensor of logits for positive items (true next items)
        # sampled_negatives_logits: [N'] tensor of logits for sampled negative items
        # sampled_negatives_valid_mask: [N'] tensor indicating if negative samples are valid
        #   A negative sample is considered valid if it is different from the positive item
        #   i.e. sampled_id != supervision_id for that position
        #   Invalid negatives (where sampled_id = supervision_id) are masked out
        # 
        # loss_weights: [N'] tensor of loss weights for each position
        #   Determined by:
        #   1. supervision_weights: Initial weights provided for each position
        #   2. sampled_negatives_valid_mask: Only consider positions where negative samples are valid
        #   Final loss_weights = supervision_weights * sampled_negatives_valid_mask
        #
        # weighted_losses: [N'] tensor of weighted losses for each position
        #   Computed as:
        #   The loss formula combines two binary cross entropy terms:
        #   1. BCE between positive_logits and 1's: -log(sigmoid(positive_logits))
        #   2. BCE between negative_logits and 0's: -log(1 - sigmoid(negative_logits))
        #   The two terms are averaged (*0.5) and weighted by loss_weights:
        #   weighted_losses = loss_weights * 0.5 * (-log(sigmoid(positive_logits)) - log(1 - sigmoid(negative_logits)))
        #   
        #   This combines the loss for positive and negative items, scales by loss_weights, and averages
        #   the result to get a single loss value for each position
        positive_logits = (
            self._model.interaction(
                input_embeddings=output_embeddings,  # [B, D] = [N', D]
                target_ids=supervision_ids.unsqueeze(1),  # [N', 1]
                target_embeddings=supervision_embeddings.unsqueeze(
                    1
                ),  # [N', D] -> [N', 1, D]
            )[0].squeeze(1)
            / self._temperature
        )  # [N']

        sampled_negatives_logits = (
            self._model.interaction(
                input_embeddings=output_embeddings,  # [N', D]
                target_ids=sampled_ids,  # [N', 1]
                target_embeddings=sampled_negative_embeddings,  # [N', 1, D]
            )[0].squeeze(1)
            / self._temperature
        )  # [N']
        sampled_negatives_valid_mask = (
            supervision_ids != sampled_ids.squeeze(1)
        ).float()  # [N']
        loss_weights = supervision_weights * sampled_negatives_valid_mask

        # The loss function implements a binary cross entropy (BCE) loss for sequential recommendation
        # 
        # For each position in the sequence:
        # 1. We have a positive item (the actual next item) and a sampled negative item
        # 2. The model produces logits (scores) for both items
        # 3. We want:
        #    - High probability (sigmoid(logit) close to 1) for positive items
        #    - Low probability (sigmoid(logit) close to 0) for negative items
        #
        # The BCE loss is computed in two parts:
        # 1. BCE between positive logits and target=1: -log(sigmoid(positive_logits))
        #    - This encourages high scores for positive items
        # 2. BCE between negative logits and target=0: -log(1-sigmoid(negative_logits)) 
        #    - This encourages low scores for negative items
        #
        # The losses are:
        # - Weighted by loss_weights to mask invalid positions
        # - Averaged (*0.5) between positive and negative terms
        # - Summed across all positions and normalized by sum of weights
        #
        # This trains the model to score actual next items higher than random negative items
        weighted_losses = (
            (
                F.binary_cross_entropy_with_logits(
                    # Compute BCE loss between positive logits and target=1
                    # positive_logits: scores for actual next items [N']
                    # target=1: we want sigmoid(positive_logits) close to 1
                    # reduction="none": return per-element losses without averaging
                    #
                    # Reference: https://pytorch.org/docs/stable/generated/torch.nn.functional.binary_cross_entropy_with_logits.html
                    #
                    # This function is used to compute the binary cross-entropy loss between the target and the input logits.
                    # It is a combination of a sigmoid layer and the binary cross-entropy loss, which is more numerically 
                    # stable than using a plain sigmoid followed by a binary cross-entropy loss.
                    input=positive_logits,
                    target=torch.ones_like(positive_logits),
                    reduction="none",
                )
                + F.binary_cross_entropy_with_logits(
                    input=sampled_negatives_logits,
                    target=torch.zeros_like(sampled_negatives_logits),
                    reduction="none",
                )
            )
            * loss_weights
            * 0.5
        )
        return weighted_losses.sum() / loss_weights.sum()

    def forward(
        self,
        lengths: torch.Tensor,
        output_embeddings: torch.Tensor,
        supervision_ids: torch.Tensor,
        supervision_embeddings: torch.Tensor,
        supervision_weights: torch.Tensor,
        negatives_sampler: NegativesSampler,
    ) -> torch.Tensor:
        """
        Args:
            lengths: Tensor of shape [B] containing sequence lengths for each batch element
                B: Batch size - number of sequences in the batch
                Each value indicates how many valid items are in that sequence
                Used to mask out padding and identify valid positions for loss computation
            output_embeddings: Tensor of shape [B, N, D] containing embeddings for the current
                input sequence.
                B: Batch size - number of sequences in the batch
                N: Sequence length - max number of items in each sequence 
                D: Embedding dimension - size of the embedding vectors
            supervision_ids: Tensor of shape [B, N] containing the ground truth (positive) item IDs.
            supervision_embeddings: Tensor of shape [B, N, D] containing embeddings for the ground
                truth items.
            supervision_weights: Optional tensor of shape [B, N] containing weights for masking
                invalid positions or reweighting supervision labels.

                Tensor of shape [B, N] containing weights for each position
                Used to:
                1. Mask out padding positions (weight=0) vs valid positions (weight=1)
                2. Optionally reweight importance of different positions in the sequence
                3. Gets converted to jagged format and multiplies final per-element losses
            negatives_sampler: Sampler object used to obtain negative examples to pair with the
                positive examples.

        Returns:
            Scalar tensor containing the loss value for the current sequence.
          
        # This method computes the autoregressive loss for sequential recommendation
        # by comparing output embeddings against supervision embeddings.
        #
        # The inputs are:
        # - lengths: Batch of sequence lengths indicating valid elements per row
        # - output_embeddings: Model output embeddings for input sequences 
        # - supervision_ids: Ground truth item IDs for next item prediction
        # - supervision_embeddings: Embeddings of ground truth next items
        # - supervision_weights: Optional weights for masking/reweighting
        # - negatives_sampler: Strategy for sampling negative examples
        #
        # The method:
        # 1. Validates input tensor shapes match
        # 2. Converts dense tensors to jagged format using sequence lengths
        # 3. Calls jagged_forward() with converted tensors to compute loss
        #
        # Returns a scalar loss value averaged over the batch
        """
        # Validate input tensor shapes match expected dimensions
        # output_embeddings and supervision_embeddings should be [B,N,D]
        # supervision_ids should be [B,N] (matches embeddings except for last dim)
        # supervision_weights should be [B,N] (matches supervision_ids shape)
        assert output_embeddings.size() == supervision_embeddings.size()
        assert supervision_ids.size() == supervision_embeddings.size()[:-1]

        # Convert dense tensors to jagged format using sequence lengths
        # jagged_id_offsets: Tensor of shape [B] containing cumulative sum of sequence lengths
        # jagged_supervision_ids: Tensor of shape [B,N'] containing supervision IDs in jagged format
        # jagged_supervision_weights: Tensor of shape [B,N'] containing supervision weights in jagged format
        jagged_id_offsets = torch.ops.fbgemm.asynchronous_complete_cumsum(lengths)
        # jagged_id_offsets contains the cumulative sum of sequence lengths
        # For example, if lengths = [2,3,1], then jagged_id_offsets = [0,2,5,6]
        # This is used to convert dense tensors to jagged format by indicating
        # the start indices for each sequence in the flattened jagged tensor
    
        # Convert dense tensors to jagged format for efficient processing
        # jagged_supervision_ids: Flattened tensor of supervision IDs
        # - Unsqueeze adds singleton dimension for fbgemm compatibility 
        # - Float conversion needed for fbgemm op
        # - Squeeze removes singleton dimension after conversion
        # - Long conversion restores integer dtype
        jagged_supervision_ids = (
            torch.ops.fbgemm.dense_to_jagged(
                # The dense_to_jagged op takes:
                # 1. Dense input tensor
                # 2. List of offsets tensors indicating sequence boundaries
                # Returns a list of jagged tensors, we take first element [0]
                supervision_ids.unsqueeze(-1).float(), [jagged_id_offsets]
            )[0]
            .squeeze(1)
            .long()
        )
        # jagged_supervision_weights: Flattened tensor of supervision weights
        # - Follows same process as supervision IDs but keeps float dtype
        # - Used to mask padding and optionally reweight positions
        jagged_supervision_weights = torch.ops.fbgemm.dense_to_jagged(
            supervision_weights.unsqueeze(-1),
            [jagged_id_offsets],
        )[0].squeeze(1)
        return self.jagged_forward(
            output_embeddings=torch.ops.fbgemm.dense_to_jagged(
                output_embeddings,
                [jagged_id_offsets],
            )[0],
            supervision_ids=jagged_supervision_ids,
            supervision_embeddings=torch.ops.fbgemm.dense_to_jagged(
                supervision_embeddings,
                [jagged_id_offsets],
            )[0],
            supervision_weights=jagged_supervision_weights,
            negatives_sampler=negatives_sampler,
        )


class BCELossWithRatings(AutoregressiveLoss):
    def __init__(
        self,
        temperature: float,
        model: SimilarityModule,
    ) -> None:
        super().__init__()
        self._temperature: float = temperature
        self._model = model

    def jagged_forward(
        self,
        output_embeddings: torch.Tensor,
        supervision_ids: torch.Tensor,
        supervision_embeddings: torch.Tensor,
        supervision_weights: torch.Tensor,
        supervision_ratings: torch.Tensor,
        negatives_sampler: NegativesSampler,
    ) -> torch.Tensor:
        assert output_embeddings.size() == supervision_embeddings.size()
        assert supervision_ids.size() == supervision_embeddings.size()[:-1]
        assert supervision_ids.size() == supervision_weights.size()

        target_logits = (
            self._model.interaction(
                input_embeddings=output_embeddings,  # [B, D] = [N', D]
                target_ids=supervision_ids.unsqueeze(1),  # [N', 1]
                target_embeddings=supervision_embeddings.unsqueeze(
                    1
                ),  # [N', D] -> [N', 1, D]
            )[0].squeeze(1)
            / self._temperature
        )  # [N', 1]

        weighted_losses = (
            F.binary_cross_entropy_with_logits(
                input=target_logits,
                target=supervision_ratings.to(dtype=target_logits.dtype),
                reduction="none",
            )
        ) * supervision_weights
        return weighted_losses.sum() / supervision_weights.sum()

    def forward(
        self,
        lengths: torch.Tensor,
        output_embeddings: torch.Tensor,
        supervision_ids: torch.Tensor,
        supervision_embeddings: torch.Tensor,
        supervision_weights: torch.Tensor,
        supervision_ratings: torch.Tensor,
        negatives_sampler: NegativesSampler,
    ) -> torch.Tensor:
        """
        Args:
          lengths: [B] x int32 representing number of non-zero elements per row.
          output_embeddings: [B, N, D] x float, embeddings for the current
              input sequence.
          supervision_ids: [B, N] x int64, (positive) supervision ids.
          supervision_embeddings: [B, N, D] x float.
          supervision_weights: Optional [B, N] x float. Optional weights for
              masking out invalid positions, or reweighting supervision labels.
          negatives_sampler: sampler used to obtain negative examples paired with
              positives.
        Returns:
          (1), loss for the current engaged sequence.
        """
        assert output_embeddings.size() == supervision_embeddings.size()
        assert supervision_ids.size() == supervision_embeddings.size()[:-1]
        jagged_id_offsets = torch.ops.fbgemm.asynchronous_complete_cumsum(lengths)
        jagged_supervision_ids = (
            torch.ops.fbgemm.dense_to_jagged(
                supervision_ids.unsqueeze(-1).float(), [jagged_id_offsets]
            )[0]
            .squeeze(1)
            .long()
        )
        jagged_supervision_weights = torch.ops.fbgemm.dense_to_jagged(
            supervision_weights.unsqueeze(-1),
            [jagged_id_offsets],
        )[0].squeeze(1)
        return self.jagged_forward(
            output_embeddings=torch.ops.fbgemm.dense_to_jagged(
                output_embeddings,
                [jagged_id_offsets],
            )[0],
            supervision_ids=jagged_supervision_ids,
            supervision_embeddings=torch.ops.fbgemm.dense_to_jagged(
                supervision_embeddings,
                [jagged_id_offsets],
            )[0],
            supervision_weights=jagged_supervision_weights,
            supervision_ratings=torch.ops.fbgemm.dense_to_jagged(
                supervision_ratings.unsqueeze(-1),
                [jagged_id_offsets],
            )[0].squeeze(1),
            negatives_sampler=negatives_sampler,
        )

</content>

<content full_path="generative_recommenders/modeling/sequential/losses/sampled_softmax.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from collections import OrderedDict
from typing import Dict, Optional, Tuple

import torch
import torch.nn.functional as F

from generative_recommenders.modeling.sequential.autoregressive_losses import (
    AutoregressiveLoss,
    NegativesSampler,
)

from torch.utils.checkpoint import checkpoint


class SampledSoftmaxLoss(AutoregressiveLoss):
    """
    Sampled Softmax Loss (SSL) is a loss function used in autoregressive models for 
    next-item prediction. It treats each position in the sequence as a multi-class 
    classification problem over all possible items, allowing the model to learn 
    relative preferences between items.
    """
    def __init__(
        self,
        num_to_sample: int,
        softmax_temperature: float,
        model,
        activation_checkpoint: bool = False,
    ) -> None:
        """
        Args:
            num_to_sample: Number of negative samples to draw for each positive sample.
            softmax_temperature: Temperature parameter for scaling logits before softmax.
                Higher values make the distribution more uniform, lower values make it more peaked.
            model: The model to use for the loss function.
            activation_checkpoint: Whether to use activation checkpointing.
        """
        super().__init__()

        self._num_to_sample: int = num_to_sample
        self._softmax_temperature: float = softmax_temperature
        self._model = model
        self._activation_checkpoint: bool = activation_checkpoint

    def jagged_forward(
        self,
        output_embeddings: torch.Tensor,
        supervision_ids: torch.Tensor,
        supervision_embeddings: torch.Tensor,
        supervision_weights: torch.Tensor,
        negatives_sampler: NegativesSampler,
        **kwargs,
    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        Args:
            output_embeddings: [B, N, D] x float, embeddings for the current
                input sequence.
                # N represents the sequence length dimension in the tensor shape
                # [B, N, D] where:
                # B = batch size
                # N = sequence length
                # D = embedding dimension
            supervision_ids: [B, N] x int64, (positive) supervision ids.
            supervision_embeddings: [B, N, D] x float.
            supervision_weights: Optional [B, N] x float. Optional weights for
                masking out invalid positions, or reweighting supervision labels.
            negatives_sampler: sampler used to obtain negative examples paired with
                positives.
        """
        assert output_embeddings.size() == supervision_embeddings.size()
        assert supervision_ids.size() == supervision_embeddings.size()[:-1]
        assert supervision_ids.size() == supervision_weights.size()

        sampled_ids, sampled_negative_embeddings = negatives_sampler(
            positive_ids=supervision_ids,
            num_to_sample=self._num_to_sample,
        )
        # We don't normalize for BCE Loss because:
        # 1. BCE Loss operates on raw logits and applies sigmoid internally
        # 2. For sampled softmax, we need normalized embeddings to ensure 
        #    consistent similarity scores across different samples
        # 3. The softmax operation is sensitive to the scale of logits,
        #    so normalized embeddings help maintain stable gradients
        sampled_negative_embeddings = negatives_sampler.normalize_embeddings(
            sampled_negative_embeddings
        )
        positive_embeddings = negatives_sampler.normalize_embeddings(
            supervision_embeddings
        )
        positive_logits, aux_losses = self._model.similarity_fn(
        # similarity_fn computes raw similarity scores between query and item embeddings
            # These scores are called logits because they are the raw, unnormalized values
            # that will be passed into softmax later. The term logits comes from logistic
            # regression where raw scores are transformed via softmax into probabilities.
            #
            # Returns:
            # - positive_logits: [B, 1] tensor of raw similarity scores for positive items
            #   that will be normalized via softmax with negative examples
            # - aux_losses: Dict of auxiliary losses from the similarity computation
            #   Examples:
            #   - L2 regularization loss on embeddings
            #   - Dropout regularization loss
            #   - Attention dropout loss if using attention mechanism
            #   - Any other regularization losses from the similarity computation
            query_embeddings=output_embeddings,  # [B, D] = [N', D]
            item_ids=supervision_ids.unsqueeze(1),  # [N', 1]
            item_embeddings=positive_embeddings.unsqueeze(1),  # [N', D] -> [N', 1, D]
            **kwargs,
        )
        # Temperature scaling in softmax serves multiple purposes:
        # 1. Controls the "sharpness" of probability distribution:
        #    - Higher temperature (>1) makes distribution more uniform/smooth
        #    - Lower temperature (<1) makes distribution more peaked/concentrated
        # 2. Helps with gradient flow during training:
        #    - Very large logits can lead to vanishing gradients in softmax
        #    - Temperature scaling prevents this by scaling down large values
        # 3. Can be used for calibration:
        #    - Models often output overconfident predictions
        #    - Temperature scaling helps calibrate confidence levels
        #
        # Example:
        # softmax([1, 2, 3]) = [0.09, 0.24, 0.67]  # Original
        # softmax([1, 2, 3]/2) = [0.18, 0.27, 0.55]  # Higher temp = smoother
        # softmax([1, 2, 3]*2) = [0.02, 0.12, 0.86]  # Lower temp = sharper
        #
        positive_logits = positive_logits / self._softmax_temperature  # [0]
        sampled_negatives_logits, _ = self._model.similarity_fn(
            # We discard aux_losses for negative samples because:
            # 1. They are already included in the positive sample computation
            # 2. Including them again would double-count regularization terms
            # 3. The same embeddings may appear multiple times in negatives
            #    leading to incorrect weighting of regularization
            query_embeddings=output_embeddings,  # [N', D]
            item_ids=sampled_ids,  # [N', R]
            item_embeddings=sampled_negative_embeddings,  # [N', R, D]
            **kwargs,
        )  # [N', R]  # [0]
        sampled_negatives_logits = torch.where(
            # Mask out logits where sampled negatives match the positive item
            #
            # This prevents a positive item from being treated as its own negative
            # We use a large negative value (-5e4) to effectively zero out the 
            # probability after softmax because:
            #
            # 1. We want to completely exclude these cases from contributing to the loss
            # 2. After softmax, e^(-5e4) ≈ 0, so these items get ~0 probability
            # 3. If we didn't zero them out, the model would get conflicting signals:
            #    - Same item treated as both positive and negative example
            #    - This would harm learning since gradients would partially cancel
            #
            # In general, we use a large negative value to effectively zero out the probability 
            # after softmax. Common choices include:
            #
            # - float('-inf'): Most theoretically correct but can cause numerical issues
            #   because:
            #   1. When computing softmax(x) = exp(x)/sum(exp(x)), if any x is -inf,
            #      we get 0/0 which is undefined
            #   2. During backprop, gradients through -inf become NaN
            #   3. NaN values can then propagate through the network
            # 
            # - -1e9: Large enough to zero probability but avoids numerical instability
            # - -1e4 to -1e5: Common range that works well in practice
            #
            # We use -5e4 here as a good balance between:
            # 1. Being large enough to zero out probability (e^(-5e4) ≈ 0)
            # 2. Avoiding potential numerical instability of float('-inf')
            # 3. Following common practice in other implementations
            supervision_ids.unsqueeze(1) == sampled_ids,  # [N', R]
            -5e4,
            sampled_negatives_logits / self._softmax_temperature,
        )

        # We compute softmax over all logits (positive and negative) to get normalized probabilities.
        # By taking the negative log of the positive item's probability, we get a loss that:
        # - Approaches 0 when the positive probability is close to 1 (desired outcome)
        # - Increases as the positive probability decreases (penalizes wrong predictions)
        # This effectively trains the model to assign high probability to true items.
        jagged_loss = -F.log_softmax(
            # Concatenate positive and negative logits for softmax computation
            # positive_logits: [N', 1] - logits for true/positive items 
            # sampled_negatives_logits: [N', R] - logits for sampled negative items
            #   where R = num_to_sample (number of negative samples per positive)
            # Result: [N', R+1] - combined logits tensor for softmax
            #
            # This concatenates the logits for the positive (true) items with the logits
            # for the sampled negative items along dimension 1 (the sequence length dimension).
            # The result is a tensor containing all logits that will be input to softmax.
            #
            # We concatenate the logits to compute softmax over both positive and negative items
            # together. This is necessary because:
            # 1. Softmax needs to normalize over all possible choices to get valid probabilities
            # 2. The denominator in softmax must include both positive and negative logits
            # 3. This gives us proper relative probabilities between positive/negative items
            #
            # For example, if we have:
            # positive_logit = 5.0
            # negative_logits = [3.0, 2.0]
            #
            # softmax([5.0, 3.0, 2.0]) = [0.67, 0.20, 0.13]
            # The 0.67 probability for positive item properly accounts for competition from negatives
            #
            # If we computed softmax separately:
            # softmax([5.0]) = [1.0]  # Incorrect - ignores negatives
            # softmax([3.0, 2.0]) = [0.62, 0.38]  # Incorrect - ignores positive
            # 
            # 
            # Concatenate and apply softmax along dim=1, then select first element
            # Input shapes:
            # - positive_logits: [N', 1] - logits for true items
            # - sampled_negatives_logits: [N', R] - logits for negative samples
            # After cat: [N', R+1] - combined logits
            # After softmax: [N', R+1] - probabilities
            # After [:, 0]: [N'] - probabilities of true items only
            torch.cat([positive_logits, sampled_negatives_logits], dim=1), dim=1
        )[:, 0]

        return (
            # Apply supervision weights to the jagged loss
            # supervision_weights: [N'] - weights for each position
            # jagged_loss: [N'] - raw loss values before weighting
            # 
            # Multiply the loss by weights to:
            # 1. Mask out invalid positions (weight=0)
            # 2. Reweight importance of different positions
            # 3. Normalize by sum of weights to get per-position average
            #
            # For example:
            # jagged_loss = [1.2, 0.8, 1.5]
            # weights = [1.0, 0.0, 0.5]
            # 
            # weighted_loss = [1.2, 0.0, 0.75]  # Element-wise multiply
            # final_loss = sum(weighted_loss) / sum(weights) # Normalize
            jagged_loss * supervision_weights
        ).sum() / supervision_weights.sum(), aux_losses

    def forward(
        self,
        lengths: torch.Tensor,
        output_embeddings: torch.Tensor,
        supervision_ids: torch.Tensor,
        supervision_embeddings: torch.Tensor,
        supervision_weights: torch.Tensor,
        negatives_sampler: NegativesSampler,
        **kwargs,
    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        Args:
            lengths: [B] x int32 representing number of non-zero elements per row.
            output_embeddings: [B, N, D] x float, embeddings for the current
                input sequence.
            supervision_ids: [B, N] x int64, (positive) supervision ids.
            supervision_embeddings: [B, N, D] x float.
            supervision_weights: Optional [B, N] x float. Optional weights for
                masking out invalid positions, or reweighting supervision labels.
            negatives_sampler: sampler used to obtain negative examples paired with
                positives.

        Returns:
            Tuple of (loss for the current engaged sequence, str-keyed aux_losses).
        """
        torch._assert(
            output_embeddings.size() == supervision_embeddings.size(),
            "Invalid supervision embeddings size.",
        )
        torch._assert(
            supervision_ids.size() == supervision_embeddings.size()[:-1],
            "Invalid supervision ids size.",
        )

        jagged_id_offsets = torch.ops.fbgemm.asynchronous_complete_cumsum(lengths)
        jagged_supervision_ids = (
            torch.ops.fbgemm.dense_to_jagged(
                supervision_ids.unsqueeze(-1).float(), [jagged_id_offsets]
            )[0]
            .squeeze(1)
            .long()
        )
        if "user_ids" in kwargs:
            # expand to jagged.
            max_length: int = int(lengths.max())
            kwargs["user_ids"] = torch.ops.fbgemm.dense_to_jagged(
                kwargs["user_ids"]
                .unsqueeze(1)
                .expand(-1, max_length)
                .unsqueeze(2),  # (B, max_length, 1)
                [jagged_id_offsets],
            )[0].squeeze(1)

        args = OrderedDict(
            [
                (
                    "output_embeddings",
                    torch.ops.fbgemm.dense_to_jagged(
                        output_embeddings,
                        [jagged_id_offsets],
                    )[0],
                ),
                ("supervision_ids", jagged_supervision_ids),
                (
                    "supervision_embeddings",
                    torch.ops.fbgemm.dense_to_jagged(
                        supervision_embeddings,
                        [jagged_id_offsets],
                    )[0],
                ),
                (
                    "supervision_weights",
                    torch.ops.fbgemm.dense_to_jagged(
                        supervision_weights.unsqueeze(-1),
                        [jagged_id_offsets],
                    )[0].squeeze(1),
                ),
                ("negatives_sampler", negatives_sampler),
            ]
        )
        args.update(kwargs)
        if self._activation_checkpoint:
            return checkpoint(
                self.jagged_forward,
                *args.values(),
                use_reentrant=False,
            )
        else:
            return self.jagged_forward(
                output_embeddings=torch.ops.fbgemm.dense_to_jagged(
                    output_embeddings,
                    [jagged_id_offsets],
                )[0],
                supervision_ids=jagged_supervision_ids,
                supervision_embeddings=torch.ops.fbgemm.dense_to_jagged(
                    supervision_embeddings,
                    [jagged_id_offsets],
                )[0],
                supervision_weights=torch.ops.fbgemm.dense_to_jagged(
                    supervision_weights.unsqueeze(-1),
                    [jagged_id_offsets],
                )[0].squeeze(1),
                negatives_sampler=negatives_sampler,
                **kwargs,
            )

</content>

<content full_path="generative_recommenders/ops/hstu_attention.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

#!/usr/bin/env python3

# pyre-strict

from typing import Optional

import torch

from generative_recommenders.common import HammerKernel
from generative_recommenders.ops.pytorch.pt_hstu_attention import (
    pytorch_cached_hstu_mha,
    pytorch_hstu_mha,
)
from generative_recommenders.ops.triton.triton_hstu_attention import (
    triton_cached_hstu_mha,
    triton_hstu_mha,
)
from torch.fx._symbolic_trace import is_fx_tracing


def hstu_mha(
    max_seq_len: int,
    alpha: float,
    q: torch.Tensor,
    k: torch.Tensor,
    v: torch.Tensor,
    seq_offsets: torch.Tensor,
    causal: bool = True,
    dropout_pr: float = 0.0,
    training: bool = True,
    num_targets: Optional[torch.Tensor] = None,
    max_attn_len: Optional[int] = None,
    contextual_seq_len: int = 0,
    sort_by_length: bool = False,
    kernel: HammerKernel = HammerKernel.PYTORCH,
) -> torch.Tensor:
    _, H, _ = q.shape
    if not is_fx_tracing():
        torch._assert(max_seq_len > 0, "max_seq_len must be larger than 0")
        torch._assert(q.dim() == 3, "q must be 3-D")
        torch._assert(k.shape == q.shape, "k must be the same shape as q")
        torch._assert(v.dim() == 3, "v must be 3-D")
        torch._assert(v.shape[0] == q.shape[0], "wrong v shape[0]")
        torch._assert(v.shape[1] == H, "wrong v shape[1]")
        if max_attn_len is not None:
            torch._assert(max_attn_len > 0, "max_attn_len must be larger than 0")
        torch._assert(causal, "only support causal attention")

    if kernel in [HammerKernel.TRITON, HammerKernel.TRITON_CC]:
        if not is_fx_tracing() and kernel == HammerKernel.TRITON:
            torch._assert(q.is_cuda, "q must be CUDA tensor")
            torch._assert(k.is_cuda, "k must be CUDA tensor")
            torch._assert(v.is_cuda, "v must be CUDA tensor")
            torch._assert(seq_offsets.is_cuda, "seq_offsets must be CUDA tensor")
            torch._assert(dropout_pr < 1e-6, "dropout for triton path not implemented")
        return triton_hstu_mha(
            N=max_seq_len,
            alpha=alpha,
            q=q,
            k=k,
            v=v,
            seq_offsets=seq_offsets,
            causal=causal,
            num_targets=num_targets,
            max_attn_len=max_attn_len,
            contextual_seq_len=contextual_seq_len,
            sort_by_length=sort_by_length,
            triton_cc=kernel == HammerKernel.TRITON_CC,
        )
    else:
        return pytorch_hstu_mha(
            max_seq_len=max_seq_len,
            alpha=alpha,
            q=q,
            k=k,
            v=v,
            seq_offsets=seq_offsets,
            causal=causal,
            dropout_pr=dropout_pr,
            training=training,
            num_targets=num_targets,
            max_attn_len=max_attn_len,
            contextual_seq_len=contextual_seq_len,
        )


def cached_hstu_mha(
    max_seq_len: int,
    alpha: float,
    delta_q: torch.Tensor,
    k: torch.Tensor,
    v: torch.Tensor,
    delta_x_offsets: torch.Tensor,
    seq_offsets: torch.Tensor,
    num_targets: Optional[torch.Tensor] = None,
    kernel: HammerKernel = HammerKernel.PYTORCH,
) -> torch.Tensor:
    L, H, D = delta_q.shape
    B = seq_offsets.size(0) - 1
    if not is_fx_tracing():
        torch._assert(max_seq_len > 0, "max_seq_len must be larger than 0")
        torch._assert(delta_q.dim() == 3, "delta_q must be 3-D")
        torch._assert(L % B == 0, "delta_q must be padded")
        torch._assert(
            delta_x_offsets.numel() == L,
            "wrong delta_x_offsets shape",
        )
        torch._assert(k.dim() == 3, "k must be 3-D")
        torch._assert(k.shape[1] == H, "wrong k shape[1]")
        torch._assert(k.shape[2] == D, "wrong k shape[2]")
        torch._assert(v.dim() == 3, "v must be 3-D")
        torch._assert(v.shape[1] == H, "wrong v shape[1]")

    if kernel in [HammerKernel.TRITON, HammerKernel.TRITON_CC]:
        if not is_fx_tracing() and kernel == HammerKernel.TRITON:
            torch._assert(delta_q.is_cuda, "q must be CUDA tensor")
            torch._assert(seq_offsets.is_cuda, "seq_offsets must be CUDA tensor")
            torch._assert(
                delta_x_offsets.is_cuda, "delta_x_offsets must be CUDA tensor"
            )
            if num_targets is not None:
                torch._assert(num_targets.is_cuda, "num_targets must be CUDA tensor")
        return triton_cached_hstu_mha(
            N=max_seq_len,
            alpha=alpha,
            delta_q=delta_q,
            k=k,
            v=v,
            delta_x_offsets=delta_x_offsets,
            seq_offsets=seq_offsets,
            num_targets=num_targets,
            triton_cc=(kernel == HammerKernel.TRITON_CC),
        )
    else:
        return pytorch_cached_hstu_mha(
            N=max_seq_len,
            alpha=alpha,
            delta_q=delta_q,
            k=k,
            v=v,
            delta_x_offsets=delta_x_offsets,
            seq_offsets=seq_offsets,
            num_targets=num_targets,
            attn_bias=None,
        )

</content>

<content full_path="generative_recommenders/ops/tests/hstu_attention_test.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

#!/usr/bin/env python3

# pyre-strict

import random
import unittest
from typing import Optional

import torch
from generative_recommenders.common import (
    generate_sparse_seq_len,
    gpu_unavailable,
    HammerKernel,
    set_dev_mode,
)

from hypothesis import given, settings, strategies as st, Verbosity


class HSTUAttentionTest(unittest.TestCase):
    @unittest.skipIf(*gpu_unavailable)
    # pyre-ignore
    @given(
        batch_size=st.integers(4, 8),
        heads=st.integers(1, 4),
        max_uih_len=st.sampled_from([100, 128, 256]),
        max_targets=st.sampled_from([20, 512]),
        attn_dim=st.sampled_from([16, 32, 64, 128]),
        hidden_dim=st.sampled_from([16, 32, 64, 128]),
        causal=st.sampled_from([True]),
        has_multiple_targets=st.sampled_from([True, False]),
        dtype=st.sampled_from(
            [torch.bfloat16, torch.float32]
            if torch.cuda.get_device_capability(torch.device("cuda"))[0] >= 8
            else [torch.float32]
        ),
        has_max_attn_len=st.booleans(),
        contextual_seq_len=st.sampled_from([0, 10]),
    )
    @settings(
        verbosity=Verbosity.verbose,
        max_examples=200,
        deadline=None,
    )
    # pyre-ignore[2]
    def test_attn_triton(self, *args, **kwargs) -> None:
        self._test_attn(
            *args,
            **kwargs,
            test_backward=True,
            ref_kernel=HammerKernel.PYTORCH,
            real_kernel=HammerKernel.TRITON,
        )

    @unittest.skipIf(*gpu_unavailable)
    # pyre-ignore
    @given(
        batch_size=st.just(64),
        heads=st.just(4),
        max_uih_len=st.sampled_from([32768]),
        max_targets=st.sampled_from([32]),
        attn_dim=st.just(128),
        hidden_dim=st.just(128),
        causal=st.sampled_from([True]),
        has_multiple_targets=st.sampled_from([True, False]),
        dtype=st.sampled_from([torch.bfloat16]),
        has_max_attn_len=st.sampled_from([True, False]),
    )
    @settings(
        verbosity=Verbosity.verbose,
        max_examples=5,
        deadline=None,
    )
    # pyre-ignore[2]
    def test_attn_triton_long_seqs(self, *args, **kwargs) -> None:
        self._test_attn(
            *args,
            **kwargs,
            test_backward=True,
            ref_kernel=HammerKernel.TRITON,
            real_kernel=HammerKernel.TRITON,
            skip_comparisons=True,
            sparsity=1.0,
        )

    def _test_attn(
        self,
        batch_size: int,
        heads: int,
        max_uih_len: int,
        max_targets: int,
        attn_dim: int,
        hidden_dim: int,
        causal: bool,
        has_multiple_targets: bool,
        has_max_attn_len: bool,
        dtype: torch.dtype,
        test_backward: bool,
        ref_kernel: HammerKernel,
        real_kernel: HammerKernel,
        skip_comparisons: bool = False,
        sparsity: float = -1.0,
        contextual_seq_len: int = 0,
        atol: Optional[float] = None,
        rtol: Optional[float] = None,
    ) -> None:
        set_dev_mode(True)
        torch.backends.cudnn.allow_tf32 = True
        torch.backends.cuda.matmul.allow_tf32 = True
        from generative_recommenders.ops.hstu_attention import hstu_mha

        alpha = 1.0 / (attn_dim**0.5)
        if sparsity > 0.0:
            lengths = generate_sparse_seq_len(
                size=batch_size,
                max_seq_len=max_uih_len,
                sparsity=sparsity,
                device=torch.device("cuda"),
            )
        else:
            lengths = torch.randint(
                max_uih_len + 1, size=(batch_size,), device=torch.device("cuda")
            )
        num_targets = torch.randint(
            max_targets + 1, size=(batch_size,), device=torch.device("cuda")
        )
        lengths = lengths + num_targets
        max_seq_len = max_uih_len + max_targets
        if has_max_attn_len:
            max_attn_len = random.randint(1, max_uih_len // 5)
        else:
            max_attn_len = None
        seq_offsets = torch.zeros(
            (batch_size + 1,), dtype=torch.int64, device=torch.device("cuda")
        )
        seq_offsets[1:] = torch.cumsum(lengths, dim=0)

        L = int(seq_offsets[-1].item())
        q = (
            torch.empty((L, heads, attn_dim), dtype=dtype, device=torch.device("cuda"))
            .uniform_(-0.1, 0.1)
            .requires_grad_()
        )
        k = (
            torch.empty((L, heads, attn_dim), dtype=dtype, device=torch.device("cuda"))
            .uniform_(-0.1, 0.1)
            .requires_grad_()
        )
        v = (
            torch.empty(
                (L, heads, hidden_dim), dtype=dtype, device=torch.device("cuda")
            )
            .uniform_(-0.1, 0.1)
            .requires_grad_()
        )

        # ref implementation
        ref_out = hstu_mha(
            max_seq_len=max_seq_len,
            alpha=alpha,
            q=q,
            k=k,
            v=v,
            seq_offsets=seq_offsets,
            causal=causal,
            num_targets=num_targets if has_multiple_targets else None,
            dropout_pr=0.0,
            max_attn_len=max_attn_len,
            contextual_seq_len=contextual_seq_len,
            kernel=ref_kernel,
        )
        dout = torch.randn_like(ref_out) * 0.01
        ref_out.backward(dout)

        if skip_comparisons:
            return

        # pyre-ignore
        ref_dv, v.grad = v.grad.clone(), None
        ref_dk, k.grad = k.grad.clone(), None
        ref_dq, q.grad = q.grad.clone(), None

        # triton implementation
        q = q.detach().clone().requires_grad_()
        k = k.detach().clone().requires_grad_()
        v = v.detach().clone().requires_grad_()
        dout = dout.detach().clone()
        real_out = hstu_mha(
            max_seq_len=max_seq_len,
            alpha=alpha,
            q=q,
            k=k,
            v=v,
            seq_offsets=seq_offsets,
            causal=causal,
            num_targets=num_targets if has_multiple_targets else None,
            dropout_pr=0.0,
            max_attn_len=max_attn_len,
            contextual_seq_len=contextual_seq_len,
            kernel=real_kernel,
        )

        torch.testing.assert_close(
            ref_out,
            real_out,
            atol=atol,
            rtol=rtol,
        )
        if test_backward:
            real_out.backward(dout)
            real_dq, real_dk, real_dv = q.grad.clone(), k.grad.clone(), v.grad.clone()
            torch.testing.assert_close(ref_dv, real_dv, atol=atol, rtol=rtol)
            torch.testing.assert_close(ref_dk, real_dk, atol=atol, rtol=rtol)
            torch.testing.assert_close(ref_dq, real_dq, atol=atol, rtol=rtol)

    @unittest.skipIf(*gpu_unavailable)
    # pyre-ignore
    @given(
        batch_size=st.integers(4, 8),
        heads=st.integers(1, 4),
        max_uih_len=st.sampled_from([100, 128, 256]),
        max_targets=st.sampled_from([20, 512]),
        delta_size=st.sampled_from([20, 512]),
        attn_dim=st.sampled_from([16, 32, 64, 128]),
        hidden_dim=st.sampled_from([16, 32, 64, 128]),
        has_multiple_targets=st.sampled_from([True, False]),
        dtype=st.sampled_from(
            [torch.bfloat16]
            if torch.cuda.get_device_capability(torch.device("cuda"))[0] >= 8
            else [torch.float32]
        ),
    )
    @settings(
        verbosity=Verbosity.verbose,
        max_examples=200,
        deadline=None,
    )
    # pyre-ignore[2]
    def test_delta_attn_triton(self, *args, **kwargs) -> None:
        self._test_delta_attn(
            *args,
            **kwargs,
            ref_kernel=HammerKernel.PYTORCH,
            real_kernel=HammerKernel.TRITON,
        )

    def _test_delta_attn(
        self,
        batch_size: int,
        heads: int,
        max_uih_len: int,
        max_targets: int,
        delta_size: int,
        attn_dim: int,
        hidden_dim: int,
        has_multiple_targets: bool,
        dtype: torch.dtype,
        ref_kernel: HammerKernel,
        real_kernel: HammerKernel,
        atol: Optional[float] = None,
        rtol: Optional[float] = None,
    ) -> None:
        set_dev_mode(True)
        torch.backends.cudnn.allow_tf32 = True
        torch.backends.cuda.matmul.allow_tf32 = True
        from generative_recommenders.ops.hstu_attention import cached_hstu_mha

        alpha = 1.0 / (attn_dim**0.5)
        lengths = torch.randint(
            max_uih_len + 1, size=(batch_size,), device=torch.device("cuda")
        )
        lengths = lengths + delta_size
        max_seq_len = max_uih_len + delta_size
        num_targets = torch.randint(
            max_targets + 1, size=(batch_size,), device=torch.device("cuda")
        )
        num_targets = torch.clamp(num_targets, max=lengths)
        seq_offsets = torch.zeros(
            (batch_size + 1,), dtype=torch.int64, device=torch.device("cuda")
        )
        seq_offsets[1:] = torch.cumsum(lengths, dim=0)

        L = int(seq_offsets[-1].item())
        delta_q = torch.empty(
            (batch_size * delta_size, heads, attn_dim),
            dtype=dtype,
            device=torch.device("cuda"),
        ).uniform_(-0.1, 0.1)
        cached_k = torch.empty(
            (L, heads, attn_dim), dtype=dtype, device=torch.device("cuda")
        ).uniform_(-0.1, 0.1)
        cached_v = torch.empty(
            (L, heads, hidden_dim), dtype=dtype, device=torch.device("cuda")
        ).uniform_(-0.1, 0.1)
        delta_x_offsets = torch.arange(0, delta_size, device=torch.device("cuda"))
        delta_x_offsets = (seq_offsets[1:] - delta_size).view(
            batch_size, 1
        ) + delta_x_offsets.view(1, delta_size)
        delta_x_offsets = delta_x_offsets.view(-1)

        # ref implementation
        ref_out = cached_hstu_mha(
            max_seq_len=max_seq_len,
            alpha=alpha,
            delta_q=delta_q,
            k=cached_k,
            v=cached_v,
            delta_x_offsets=delta_x_offsets,
            seq_offsets=seq_offsets,
            num_targets=num_targets,
            kernel=ref_kernel,
        )

        # real implementation
        real_out = cached_hstu_mha(
            max_seq_len=max_seq_len,
            alpha=alpha,
            delta_q=delta_q,
            k=cached_k,
            v=cached_v,
            delta_x_offsets=delta_x_offsets,
            seq_offsets=seq_offsets,
            num_targets=num_targets,
            kernel=real_kernel,
        )
        torch.testing.assert_close(
            ref_out,
            real_out,
            atol=atol,
            rtol=rtol,
        )

</content>

<content full_path="generative_recommenders/ops/triton/triton_hstu_preprocess_and_attention.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

#!/usr/bin/env python3

# pyre-strict

from typing import Optional, Tuple

import torch
from generative_recommenders.ops.triton.triton_hstu_attention import (
    triton_hstu_attention_bwd,
    triton_hstu_attention_fwd,
)
from generative_recommenders.ops.triton.triton_hstu_linear import (
    triton_addmm_bwd,
    triton_addmm_fwd,
)
from generative_recommenders.ops.triton.triton_layer_norm import (
    triton_weighted_layer_norm_bwd,
    triton_weighted_layer_norm_fwd,
)
from torch.nn import functional as F


class _HSTUPreprocessAndAttentionFunction(torch.autograd.Function):
    @staticmethod
    # pyre-ignore [14]
    def forward(
        ctx,  # pyre-ignore [2]
        x: torch.Tensor,
        norm_weight: torch.Tensor,
        norm_bias: torch.Tensor,
        norm_eps: float,
        num_heads: int,
        attn_dim: int,
        hidden_dim: int,
        uvqk_weight: torch.Tensor,
        uvqk_bias: torch.Tensor,
        max_seq_len: int,
        seq_offsets: torch.Tensor,
        alpha: float,
        invalid_attn_mask_type: str,
        num_targets: Optional[torch.Tensor],
        recompute_uvqk_in_backward: bool = False,
        recompute_normed_x_in_backward: bool = False,
        contextual_seq_len: int = 0,
        sort_by_length: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        normed_x, x_mean, x_rstd, BLOCK_D, num_warps = triton_weighted_layer_norm_fwd(
            x=x,
            weight=norm_weight,
            bias=norm_bias,
            eps=norm_eps,
        )
        uvqk = triton_addmm_fwd(x=normed_x, w=uvqk_weight, y=uvqk_bias).contiguous()
        u, v, q, k = uvqk.split(
            [
                hidden_dim * num_heads,
                hidden_dim * num_heads,
                attn_dim * num_heads,
                attn_dim * num_heads,
            ],
            dim=1,
        )
        q = q.view(-1, num_heads, attn_dim)
        k = k.view(-1, num_heads, attn_dim)
        v = v.view(-1, num_heads, hidden_dim)
        silu_u = F.silu(u)
        sort_by_length_indices = None
        if sort_by_length:
            seq_lengths = seq_offsets[1:] - seq_offsets[:-1]
            _, sort_by_length_indices = torch.sort(
                seq_lengths, descending=True, stable=False
            )
        out = triton_hstu_attention_fwd(
            N=max_seq_len,
            alpha=alpha,
            q=q,
            k=k,
            v=v,
            seq_offsets=seq_offsets,
            causal=True,
            num_targets=num_targets,
            max_attn_len=0,
            contextual_seq_len=contextual_seq_len,
            sort_by_length_indices=sort_by_length_indices,
        )
        # update ctx
        saved_tensors = [
            x,
            norm_weight,
            norm_bias,
            x_mean,
            x_rstd,
            uvqk_weight,
            seq_offsets,
        ]
        if num_targets is not None:
            saved_tensors.append(num_targets)
        if not recompute_normed_x_in_backward:
            saved_tensors.append(normed_x)
        if recompute_uvqk_in_backward:
            saved_tensors.append(uvqk_bias)
        else:
            saved_tensors.append(uvqk)
        if sort_by_length:
            saved_tensors.append(sort_by_length_indices)
        ctx.save_for_backward(*saved_tensors)
        ctx.alpha = alpha
        ctx.causal = True
        ctx.has_multiple_targets = num_targets is not None
        ctx.max_seq_len = max_seq_len
        ctx.max_attn_len = 0
        ctx.recompute_normed_x_in_backward = recompute_normed_x_in_backward
        ctx.recompute_uvqk_in_backward = recompute_uvqk_in_backward
        ctx.hidden_dim = hidden_dim
        ctx.attn_dim = attn_dim
        ctx.num_heads = num_heads
        ctx.uvqk_bias_1d = uvqk_bias.dim() == 1
        ctx.norm_eps = norm_eps
        ctx.norm_BLOCK_D = BLOCK_D
        ctx.norm_num_warps = num_warps
        ctx.contextual_seq_len = contextual_seq_len
        ctx.sort_by_length = sort_by_length
        return silu_u, out

    @staticmethod
    # pyre-ignore[14]
    def backward(
        ctx,  # pyre-ignore[2]
        dsilu_u: torch.Tensor,
        dout: torch.Tensor,
    ) -> Tuple[
        torch.Tensor,  # d_x
        torch.Tensor,  # d_norm_weight
        torch.Tensor,  # d_norm_bias
        None,
        None,
        None,
        None,
        torch.Tensor,  # d_uvqk_weight
        torch.Tensor,  # d_uvqk_bias
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
    ]:
        x, norm_weight, norm_bias, x_mean, x_rstd, uvqk_weight, seq_offsets = (
            ctx.saved_tensors[:7]
        )
        idx = 7
        if ctx.has_multiple_targets:
            num_targets = ctx.saved_tensors[idx]
            idx += 1
        else:
            num_targets = None
        if ctx.recompute_normed_x_in_backward:
            normed_x, _, _, _, _ = triton_weighted_layer_norm_fwd(
                x=x,
                weight=norm_weight,
                bias=norm_bias,
                eps=ctx.norm_eps,
                mean=x_mean,
                rstd=x_rstd,
            )
        else:
            normed_x = ctx.saved_tensors[idx]
            idx += 1
        if ctx.recompute_uvqk_in_backward:
            uvqk_bias = ctx.saved_tensors[idx]
            uvqk = triton_addmm_fwd(x=normed_x, w=uvqk_weight, y=uvqk_bias)
            idx += 1
        else:
            uvqk = ctx.saved_tensors[idx]
            idx += 1
        if ctx.sort_by_length:
            sort_by_length_indices = ctx.saved_tensors[idx]
        else:
            sort_by_length_indices = None

        duvqk = torch.empty_like(uvqk)
        du, dv, dq, dk = duvqk.split(
            [
                ctx.hidden_dim * ctx.num_heads,
                ctx.hidden_dim * ctx.num_heads,
                ctx.attn_dim * ctx.num_heads,
                ctx.attn_dim * ctx.num_heads,
            ],
            dim=1,
        )
        u, v, q, k = uvqk.split(
            [
                ctx.hidden_dim * ctx.num_heads,
                ctx.hidden_dim * ctx.num_heads,
                ctx.attn_dim * ctx.num_heads,
                ctx.attn_dim * ctx.num_heads,
            ],
            dim=1,
        )
        q = q.view(-1, ctx.num_heads, ctx.attn_dim)
        k = k.view(-1, ctx.num_heads, ctx.attn_dim)
        v = v.view(-1, ctx.num_heads, ctx.hidden_dim)
        dq = dq.view(-1, ctx.num_heads, ctx.attn_dim)
        dk = dk.view(-1, ctx.num_heads, ctx.attn_dim)
        dv = dv.view(-1, ctx.num_heads, ctx.hidden_dim)
        # Note: the two operations below update duvqk in place
        (
            _dq,
            _dk,
            _dv,
        ) = triton_hstu_attention_bwd(
            dout=dout,
            q=q,
            k=k,
            v=v,
            dq=dq,
            dk=dk,
            dv=dv,
            seq_offsets=seq_offsets,
            num_targets=num_targets,
            N=ctx.max_seq_len,
            max_attn_len=ctx.max_attn_len,
            alpha=ctx.alpha,
            causal=ctx.causal,
            contextual_seq_len=ctx.contextual_seq_len,
            sort_by_length_indices=sort_by_length_indices,
        )
        if dq.data_ptr() != _dq.data_ptr():
            dq.copy_(_dq)
        if dk.data_ptr() != _dk.data_ptr():
            dk.copy_(_dk)
        if dv.data_ptr() != _dv.data_ptr():
            dv.copy_(_dv)
        torch.ops.aten.silu_backward(dsilu_u, u, grad_input=du)
        d_normed_x, d_uvqk_weight, d_uvqk_bias = triton_addmm_bwd(
            x=normed_x,
            w=uvqk_weight,
            dz=duvqk,
            is_y_1d=ctx.uvqk_bias_1d,
        )
        d_x, d_norm_weight, d_norm_bias = triton_weighted_layer_norm_bwd(
            dy=d_normed_x,
            x=x,
            weight=norm_weight,
            bias=norm_bias,
            mean=x_mean,
            rstd=x_rstd,
            learnable=True,
            eps=ctx.norm_eps,
            BLOCK_D=ctx.norm_BLOCK_D,
            num_warps=ctx.norm_num_warps,
        )
        # pyre-ignore[7]
        return (
            d_x,
            d_norm_weight,
            d_norm_bias,
            None,
            None,
            None,
            None,
            d_uvqk_weight,
            d_uvqk_bias,
            None,
            None,
            None,
            None,
            None,
            None,
            None,
            None,
            None,
        )


def triton_hstu_preprocess_and_attention(
    x: torch.Tensor,
    norm_weight: torch.Tensor,
    norm_bias: torch.Tensor,
    norm_eps: float,
    num_heads: int,
    attn_dim: int,
    hidden_dim: int,
    uvqk_weight: torch.Tensor,
    uvqk_bias: torch.Tensor,
    max_seq_len: int,
    seq_offsets: torch.Tensor,
    alpha: float,
    invalid_attn_mask_type: str,
    num_targets: Optional[torch.Tensor],
    recompute_uvqk_in_backward: bool = False,
    recompute_normed_x_in_backward: bool = False,
    contextual_seq_len: int = 0,
    sort_by_length: bool = False,
) -> Tuple[torch.Tensor, torch.Tensor]:
    return _HSTUPreprocessAndAttentionFunction.apply(
        x,
        norm_weight,
        norm_bias,
        norm_eps,
        num_heads,
        attn_dim,
        hidden_dim,
        uvqk_weight,
        uvqk_bias,
        max_seq_len,
        seq_offsets,
        alpha,
        invalid_attn_mask_type,
        num_targets,
        recompute_uvqk_in_backward,
        recompute_normed_x_in_backward,
        contextual_seq_len,
        sort_by_length,
    )

</content>

<content full_path="generative_recommenders/ops/triton/triton_position.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

#!/usr/bin/env python3


from typing import List, Optional, Tuple

import torch

# @manual=//triton:triton
import triton

# @manual=//triton:triton
import triton.language as tl

try:
    torch.ops.load_library("//hammer/ops/cuda:cuda_ops")
except OSError:
    pass

from generative_recommenders.common import (
    autotune_max_seq_len,
    prev_power_of_2,
    switch_to_contiguous_if_needed,
    triton_autotune,
)


def _add_position_embeddings_configs() -> List[triton.Config]:
    configs = []
    for BLOCK_N in [16, 32, 64]:
        for num_stages in [1, 2]:
            for num_warps in [2, 4, 8]:
                configs.append(
                    triton.Config(
                        {
                            "BLOCK_N": BLOCK_N,
                        },
                        num_stages=num_stages,
                        num_warps=num_warps,
                    )
                )
    return configs


@triton_autotune(
    configs=_add_position_embeddings_configs(),
    key=["AUTOTUNE_MAX_SEQ_LEN"],
)
@triton.jit
def _add_position_embeddings_kernel(
    Jagged,
    seq_offsets,
    high_inds,
    Dense,
    Out,
    AUTOTUNE_MAX_SEQ_LEN,
    D,
    scale,
    stride_jn,
    stride_dk,
    stride_on,
    SCALE_JAGGED: tl.constexpr,
    BLOCK_D: tl.constexpr,
    BLOCK_N: tl.constexpr,
):
    """
    Jagged has shape (sum_B(N_i), D),
    Dense has shape (K, D),
    Out has shape (sum_B(N_i), D)
    """

    off_b = tl.program_id(0)
    off_n = tl.program_id(1)
    seq_start = tl.load(seq_offsets + off_b)
    seq_end = tl.load(seq_offsets + off_b + 1)
    max_ind = tl.load(high_inds + off_b)
    seq_len = seq_end - seq_start
    start_n = off_n * BLOCK_N
    if start_n >= seq_len:
        return
    offs_n = start_n + tl.arange(0, BLOCK_N)
    clamped_offs_n = tl.where(offs_n >= max_ind, max_ind, offs_n)
    offs_d = tl.arange(0, BLOCK_D)
    Jagged += seq_start.to(tl.int64) * stride_jn
    jagged_ptr_offsets = offs_n[:, None] * stride_jn + offs_d[None, :]
    Out += seq_start.to(tl.int64) * stride_on
    out_ptrs = Out + offs_n[:, None] * stride_on + offs_d[None, :]
    dense_ptrs = Dense + clamped_offs_n[:, None] * stride_dk + offs_d[None, :]
    for _d in range(0, D, BLOCK_D):
        mask = (offs_n[:, None] < seq_len) and offs_d[None, :] < D
        jg = tl.load(Jagged + jagged_ptr_offsets, mask=mask)
        if SCALE_JAGGED:
            jg = jg * scale
        dn = tl.load(dense_ptrs, mask=mask)
        jg += dn
        tl.store(out_ptrs, jg, mask=mask)
        dense_ptrs += BLOCK_D
        out_ptrs += BLOCK_D
        offs_d += BLOCK_D
        jagged_ptr_offsets += BLOCK_D


@triton.jit
def _add_position_embeddings_bwd_kernel(
    Jagged,
    seq_offsets,
    high_inds,
    DenseOut,
    JaggedOut,
    B,
    D,
    scale,
    stride_jn,
    stride_jon,
    stride_don,
    SCALE_JAGGED: tl.constexpr,
    BLOCK_D: tl.constexpr,
):
    off_k = tl.program_id(0)
    offs_d = tl.arange(0, BLOCK_D)
    accumulator = tl.zeros((BLOCK_D,), dtype=tl.float32)
    for off_b in range(0, B):
        max_ind = tl.load(high_inds + off_b)
        if off_k < max_ind:
            seq_start = tl.load(seq_offsets + off_b)
            jagged_ptr = (
                Jagged
                + seq_start.to(tl.int64) * stride_jn
                + off_k.to(tl.int64) * stride_jn
            )
            jagged_ptrs = jagged_ptr + offs_d
            jg = tl.load(
                jagged_ptrs,
                mask=offs_d < D,
            )
            accumulator += jg
            if SCALE_JAGGED:
                out_jagged_ptr = (
                    JaggedOut
                    + seq_start.to(tl.int64) * stride_jon
                    + off_k.to(tl.int64) * stride_jon
                )
                out_jagged_ptrs = out_jagged_ptr + offs_d
                tl.store(
                    out_jagged_ptrs,
                    jg * scale,
                    mask=offs_d < D,
                )
        elif off_k == max_ind:
            seq_start = tl.load(seq_offsets + off_b).to(tl.int64)
            seq_end = tl.load(seq_offsets + off_b + 1)
            for k in range(seq_start + max_ind, seq_end):
                jagged_ptr = Jagged + k * stride_jn
                jagged_ptrs = jagged_ptr + offs_d
                jg = tl.load(
                    jagged_ptrs,
                    mask=offs_d < D,
                )
                accumulator += jg
                if SCALE_JAGGED:
                    out_jagged_ptr = JaggedOut + k * stride_jon
                    out_jagged_ptrs = out_jagged_ptr + offs_d
                    tl.store(
                        out_jagged_ptrs,
                        jg * scale,
                        mask=offs_d < D,
                    )
    out = accumulator.to(DenseOut.dtype.element_ty)
    out_ptrs = DenseOut + off_k * stride_don + offs_d
    tl.store(
        out_ptrs,
        out,
        mask=offs_d < D,
    )


class _AddPositionEmbeddingsFunction(torch.autograd.Function):
    @staticmethod
    # pyre-ignore[14]
    def forward(
        ctx,
        jagged: torch.Tensor,
        jagged_offsets: torch.Tensor,
        high_inds: torch.Tensor,
        max_seq_len: int,
        dense: torch.Tensor,
        scale: float = 1.0,
    ):
        jagged = switch_to_contiguous_if_needed(jagged)
        dense = switch_to_contiguous_if_needed(dense)
        L, D = jagged.shape
        assert len(dense.shape) == 2
        out = torch.empty_like(jagged)
        B = high_inds.size(0)
        grid = lambda meta: (  # noqa E731
            B,
            triton.cdiv(max_seq_len, meta["BLOCK_N"]),
        )
        BLOCK_D = triton.next_power_of_2(D) if D < 64 else 64
        _add_position_embeddings_kernel[grid](
            Jagged=jagged,
            seq_offsets=jagged_offsets,
            high_inds=high_inds,
            Dense=dense,
            Out=out,
            AUTOTUNE_MAX_SEQ_LEN=autotune_max_seq_len(max_seq_len),
            D=D,
            scale=scale,
            stride_jn=jagged.stride(0),
            stride_dk=dense.stride(0),
            stride_on=out.stride(0),
            SCALE_JAGGED=scale != 1.0,
            BLOCK_D=BLOCK_D,
        )
        ctx.save_for_backward(jagged_offsets, high_inds)
        ctx.B = B
        ctx.D = D
        ctx.scale = scale
        ctx.K = dense.size(0)
        ctx.BLOCK_D = BLOCK_D
        return out

    @staticmethod
    # pyre-ignore[14]
    def backward(
        ctx, d_out: torch.Tensor
    ) -> Tuple[torch.Tensor, None, None, None, torch.Tensor, None]:
        jagged_offsets, high_inds = ctx.saved_tensors
        d_dense = torch.empty((ctx.K, ctx.D), device=d_out.device, dtype=d_out.dtype)
        scale_jagged = ctx.scale != 1.0
        if scale_jagged:
            d_jagged = torch.empty_like(d_out)
        BLOCK_D = triton.next_power_of_2(ctx.D)
        _add_position_embeddings_bwd_kernel[(ctx.K,)](
            Jagged=d_out,
            seq_offsets=jagged_offsets,
            high_inds=high_inds,
            DenseOut=d_dense,
            JaggedOut=d_jagged if scale_jagged else None,  # pyre-ignore[61]
            B=ctx.B,
            D=ctx.D,
            scale=ctx.scale,
            stride_jn=d_out.stride(0),
            stride_jon=d_jagged.stride(0) if scale_jagged else 0,
            stride_don=d_dense.stride(0),
            SCALE_JAGGED=scale_jagged,
            BLOCK_D=BLOCK_D,
        )
        # pyre-ignore[61]
        return d_jagged if scale_jagged else d_out, None, None, None, d_dense, None


@triton_autotune(
    configs=_add_position_embeddings_configs(),
    key=["AUTOTUNE_MAX_SEQ_LEN"],
)
@triton.jit
def _add_timestamp_position_embeddings_kernel(
    SeqEmb,
    Offsets,
    Lengths,
    PosEmb,
    TsEmb,
    Out,
    TS,
    PosInds,
    TsInds,
    NumTargets,
    AUTOTUNE_MAX_SEQ_LEN,
    D,
    num_time_buckets,
    time_bucket_increments,
    time_bucket_scale,
    time_delta,
    max_contextual_seq_len,
    max_pos_ind,
    stride_sn,
    stride_pn,
    stride_tn,
    stride_ts,
    stride_on,
    TRAINING: tl.constexpr,
    HAS_MULTIPLE_TARGETS: tl.constexpr,
    INTERLEAVE_TARGETS: tl.constexpr,
    TIME_BUCKET_FN: tl.constexpr,
    BLOCK_D: tl.constexpr,
    BLOCK_N: tl.constexpr,
):
    """
    SeqEmb has shape (sum_B(N_i), D),
    PosEmb has shape (N_p, D),
    TsEmb has shape (N_t, D),
    Out has shape (sum_B(N_i), D)
    """

    off_b = tl.program_id(0)
    off_n = tl.program_id(1)
    seq_start = tl.load(Offsets + off_b)
    seq_end = tl.load(Offsets + off_b + 1)
    seq_len = seq_end - seq_start
    start_n = off_n * BLOCK_N
    if start_n >= seq_len:
        return
    offs_n = start_n + tl.arange(0, BLOCK_N)
    offs_d = tl.arange(0, BLOCK_D)
    seq_emb_offsets = offs_n[:, None] * stride_sn + offs_d[None, :]
    SeqEmb += seq_start.to(tl.int64) * stride_sn
    mask_n = offs_n < seq_len
    # position encoding
    seq_len = tl.load(Lengths + off_b)
    if HAS_MULTIPLE_TARGETS:
        num_targets = tl.load(NumTargets + off_b)
        if INTERLEAVE_TARGETS:
            high_ind = seq_len - num_targets * 2
        else:
            high_ind = seq_len - num_targets
    else:
        high_ind = seq_len
    pos_inds = tl.where(offs_n < high_ind, offs_n, high_ind)
    pos_inds = high_ind - pos_inds + max_contextual_seq_len
    pos_inds = tl.where(pos_inds < max_pos_ind - 1, pos_inds, max_pos_ind - 1)
    pos_inds = tl.where(offs_n < max_contextual_seq_len, offs_n, pos_inds)
    if TRAINING:
        tl.store(PosInds + seq_start + offs_n, pos_inds, mask=mask_n)
    pos_emb_offsets = pos_inds[:, None] * stride_pn + offs_d[None, :]
    # timestamp encoding
    ts = tl.load(TS + off_b * stride_ts + offs_n, mask=mask_n)
    query_time = tl.load(TS + off_b * stride_ts + seq_len - 1)
    ts = query_time - ts + time_delta
    ts = tl.where(ts > 1e-6, ts, 1e-6) / time_bucket_increments
    if TIME_BUCKET_FN == "log":
        ts = tl.log(ts)
    else:
        ts = tl.sqrt(ts)
    ts = ts * time_bucket_scale
    ts = ts.to(tl.int32)
    ts = tl.where(ts > 0, ts, 0)
    ts = tl.where(ts < num_time_buckets, ts, num_time_buckets)
    if TRAINING:
        tl.store(TsInds + seq_start + offs_n, ts, mask=mask_n)
    ts_emb_offsets = ts[:, None] * stride_tn + offs_d[None, :]
    Out += seq_start.to(tl.int64) * stride_on
    out_offsets = Out + offs_n[:, None] * stride_on + offs_d[None, :]
    for _d in range(0, D, BLOCK_D):
        mask = (offs_n[:, None] < seq_len) and offs_d[None, :] < D
        seq_emb = tl.load(SeqEmb + seq_emb_offsets, mask=mask)
        pos_emb = tl.load(PosEmb + pos_emb_offsets, mask=mask)
        ts_emb = tl.load(TsEmb + ts_emb_offsets, mask=mask)
        tl.store(out_offsets, seq_emb + (pos_emb + ts_emb).to(seq_emb.dtype), mask=mask)
        seq_emb_offsets += BLOCK_D
        pos_emb_offsets += BLOCK_D
        ts_emb_offsets += BLOCK_D
        out_offsets += BLOCK_D
        offs_d += BLOCK_D


def bwd_pre_hook(nargs):
    nargs["Out"].zero_()


def _add_embeddings_bwd_configs() -> List[triton.Config]:
    configs = []
    for BLOCK in [32, 64, 128]:
        for num_stages in [2, 3, 4]:
            for num_warps in [2, 4, 8]:
                configs.append(
                    triton.Config(
                        {
                            "BLOCK": BLOCK,
                        },
                        num_stages=num_stages,
                        num_warps=num_warps,
                        pre_hook=bwd_pre_hook,
                    )
                )
    return configs


@triton_autotune(
    configs=_add_embeddings_bwd_configs(),
    key=["AUTOTUNE_MAX_SEQ_LEN", "AUTOTUNE_B", "D"],
)
@triton.jit
def _add_embeddings_bwd_kernel(
    In,
    KeyInds,
    ValueInds,
    Out,
    AUTOTUNE_MAX_SEQ_LEN,
    B,
    AUTOTUNE_B,
    D,
    jagged_size,
    stride_in,
    stride_on,
    BLOCK_D: tl.constexpr,
    BLOCK: tl.constexpr,
):
    off_block = tl.program_id(0)
    offs_d = tl.arange(0, BLOCK_D)
    mask_d = offs_d < D
    key_ind = -1
    key_ind = key_ind.to(KeyInds.dtype.element_ty)  # pyre-ignore[16]
    accumulator = tl.zeros((BLOCK_D,), dtype=In.dtype.element_ty)
    for off_i in range(0, BLOCK):
        off = off_block * BLOCK + off_i
        if off < jagged_size:
            value_ind = tl.load(ValueInds + off)
            in_offset = In + value_ind.to(tl.int64) * stride_in
            jagged_in = tl.load(in_offset + offs_d, mask=mask_d)
            key_ind_new = tl.load(KeyInds + off)
            if key_ind == key_ind_new:
                accumulator += jagged_in
            else:
                if key_ind >= 0:
                    out_offset = Out + key_ind.to(tl.int64) * stride_on
                    tl.atomic_add(
                        out_offset + offs_d,
                        accumulator.to(Out.dtype.element_ty),
                        mask=mask_d,
                        sem="relaxed",
                    )
                key_ind = key_ind_new
                accumulator = jagged_in
    if key_ind >= 0:
        out_offset = Out + key_ind.to(tl.int64) * stride_on
        tl.atomic_add(
            out_offset + offs_d,
            accumulator.to(Out.dtype.element_ty),
            mask=mask_d,
            sem="relaxed",
        )


class _AddTimestampPositionEmbeddingsFunction(torch.autograd.Function):
    @staticmethod
    # pyre-ignore[14]
    def forward(
        ctx,
        seq_embeddings: torch.Tensor,
        seq_offsets: torch.Tensor,
        pos_embeddings: torch.Tensor,
        ts_embeddings: torch.Tensor,
        timestamps: torch.Tensor,
        max_seq_len: int,
        max_contextual_seq_len: int,
        seq_lengths: torch.Tensor,
        num_targets: Optional[torch.Tensor],
        interleave_targets: bool,
        time_bucket_fn: str,
    ):
        seq_embeddings = switch_to_contiguous_if_needed(seq_embeddings)
        pos_embeddings = switch_to_contiguous_if_needed(pos_embeddings)
        ts_embeddings = switch_to_contiguous_if_needed(ts_embeddings)

        max_pos_ind = pos_embeddings.shape[0]
        B = seq_lengths.shape[0]
        assert timestamps.shape[0] == B, "shape[0] of timestamps much match batch size"
        assert (
            timestamps.shape[1] >= max_seq_len
        ), "shape[1] of timestamps much >= max_seq_len"

        N, D = seq_embeddings.shape
        assert len(pos_embeddings.shape) == 2
        assert len(ts_embeddings.shape) == 2
        assert (
            pos_embeddings.shape[1] == D
        ), "shape[1] of pos_embeddings much match seq_embeddings"
        assert (
            ts_embeddings.shape[1] == D
        ), "shape[1] of ts_embeddings much match seq_embeddings"
        out = torch.empty_like(seq_embeddings)

        timestamps = switch_to_contiguous_if_needed(timestamps)
        ts_inds = torch.empty_like(seq_embeddings[:, 0], dtype=torch.int32)
        pos_inds = torch.empty_like(seq_embeddings[:, 0], dtype=torch.int32)

        grid = lambda meta: (  # noqa E731
            B,
            triton.cdiv(max_seq_len, meta["BLOCK_N"]),
        )
        BLOCK_D = triton.next_power_of_2(D) if D < 64 else 64
        _add_timestamp_position_embeddings_kernel[grid](
            SeqEmb=seq_embeddings,
            Offsets=seq_offsets,
            Lengths=seq_lengths,
            PosEmb=pos_embeddings,
            TsEmb=ts_embeddings,
            Out=out,
            TS=timestamps,
            PosInds=pos_inds,
            TsInds=ts_inds,
            NumTargets=num_targets,
            AUTOTUNE_MAX_SEQ_LEN=autotune_max_seq_len(max_seq_len),
            D=D,
            num_time_buckets=2048,
            time_bucket_increments=60.0,
            time_bucket_scale=1.0,
            time_delta=0,
            max_contextual_seq_len=max_contextual_seq_len,
            max_pos_ind=max_pos_ind,
            stride_sn=seq_embeddings.stride(0),
            stride_pn=pos_embeddings.stride(0),
            stride_tn=ts_embeddings.stride(0),
            stride_ts=timestamps.stride(0),
            stride_on=out.stride(0),
            TRAINING=True,
            HAS_MULTIPLE_TARGETS=num_targets is not None,
            INTERLEAVE_TARGETS=interleave_targets,
            TIME_BUCKET_FN=time_bucket_fn,
            BLOCK_D=BLOCK_D,
        )
        values = torch.arange(0, N, dtype=torch.int32, device=timestamps.device)
        sorted_ts_key_inds, sorted_ts_value_inds = torch.ops.hammer.sort_kv_pairs(
            ts_inds, values
        )
        sorted_pos_key_inds, sorted_pos_value_inds = torch.ops.hammer.sort_kv_pairs(
            pos_inds, values
        )
        ctx.save_for_backward(
            sorted_pos_key_inds,
            sorted_pos_value_inds,
            sorted_ts_key_inds,
            sorted_ts_value_inds,
        )
        ctx.B = B
        ctx.D = D
        ctx.max_seq_len = max_seq_len
        ctx.pos_emb_size = pos_embeddings.shape[0]
        ctx.ts_emb_size = ts_embeddings.shape[0]
        ctx.pos_dtype = pos_embeddings.dtype
        ctx.ts_dtype = ts_embeddings.dtype
        return out

    @staticmethod
    # pyre-ignore[14]
    def backward(
        ctx, d_out: torch.Tensor
    ) -> Tuple[
        torch.Tensor,
        None,
        torch.Tensor,
        torch.Tensor,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
    ]:
        (
            sorted_pos_key_inds,
            sorted_pos_value_inds,
            sorted_ts_key_inds,
            sorted_ts_value_inds,
        ) = ctx.saved_tensors
        d_pos_embeddings = torch.empty(
            (ctx.pos_emb_size, ctx.D), device=d_out.device, dtype=torch.float32
        )
        d_ts_embeddings = torch.empty(
            (ctx.ts_emb_size, ctx.D), device=d_out.device, dtype=torch.float32
        )
        grid = lambda meta: (triton.cdiv(d_out.shape[0], meta["BLOCK"]),)  # noqa E731
        B = ctx.B
        AUTOTUNE_B = prev_power_of_2(B)
        _add_embeddings_bwd_kernel[grid](
            In=d_out,
            KeyInds=sorted_pos_key_inds,
            ValueInds=sorted_pos_value_inds,
            Out=d_pos_embeddings,
            AUTOTUNE_MAX_SEQ_LEN=autotune_max_seq_len(ctx.max_seq_len),
            B=B,
            AUTOTUNE_B=AUTOTUNE_B,
            D=ctx.D,
            jagged_size=d_out.shape[0],
            stride_in=d_out.stride(0),
            stride_on=d_pos_embeddings.stride(0),
            BLOCK_D=triton.next_power_of_2(ctx.D),
        )
        _add_embeddings_bwd_kernel[grid](
            In=d_out,
            KeyInds=sorted_ts_key_inds,
            ValueInds=sorted_ts_value_inds,
            Out=d_ts_embeddings,
            AUTOTUNE_MAX_SEQ_LEN=autotune_max_seq_len(ctx.max_seq_len),
            B=B,
            AUTOTUNE_B=AUTOTUNE_B,
            D=ctx.D,
            jagged_size=d_out.shape[0],
            stride_in=d_out.stride(0),
            stride_on=d_ts_embeddings.stride(0),
            BLOCK_D=triton.next_power_of_2(ctx.D),
        )
        return (
            d_out,
            None,
            d_pos_embeddings.to(ctx.pos_dtype),
            d_ts_embeddings.to(ctx.ts_dtype),
            None,
            None,
            None,
            None,
            None,
            None,
            None,
        )


@torch.fx.wrap
def triton_add_position_embeddings(
    jagged: torch.Tensor,
    jagged_offsets: torch.Tensor,
    high_inds: torch.Tensor,
    max_seq_len: int,
    dense: torch.Tensor,
    scale: float = 1.0,
) -> torch.Tensor:
    return _AddPositionEmbeddingsFunction.apply(
        jagged, jagged_offsets, high_inds, max_seq_len, dense, scale
    )


@torch.fx.wrap
def triton_add_timestamp_positional_embeddings(
    seq_embeddings: torch.Tensor,
    seq_offsets: torch.Tensor,
    pos_embeddings: torch.Tensor,
    ts_embeddings: torch.Tensor,
    timestamps: torch.Tensor,
    max_seq_len: int,
    max_contextual_seq_len: int,
    seq_lengths: torch.Tensor,
    num_targets: Optional[torch.Tensor],
    interleave_targets: bool,
    time_bucket_fn: str,
) -> torch.Tensor:
    return _AddTimestampPositionEmbeddingsFunction.apply(
        seq_embeddings,
        seq_offsets,
        pos_embeddings,
        ts_embeddings,
        timestamps,
        max_seq_len,
        max_contextual_seq_len,
        seq_lengths,
        num_targets,
        interleave_targets,
        time_bucket_fn,
    )

</content>

<content full_path="generative_recommenders/ops/triton/triton_addmm.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

#!/usr/bin/env python3


from typing import List

import torch

# @manual=//triton:triton
import triton

# @manual=//triton:triton
import triton.language as tl
from generative_recommenders.common import triton_autotune

ENABLE_FULL_TURNING_SPACE = False


def get_mm_configs() -> List[triton.Config]:
    if torch.version.hip:
        if ENABLE_FULL_TURNING_SPACE:
            block_m_range = [32, 64, 128, 256]
            block_n_range = [32, 64, 128, 256]
            block_k_range = [32, 64]
            group_m_range = [4, 8]
            matrix_instr_nonkdim_range = [16]
            waves_per_eu_range = [0]
            kpack_range = [1, 2]
            num_warps_range = [4, 8]
            num_stage_range = [2] if triton.__version__ >= "3.2.0" else [0]
        else:
            block_m_range = [256]
            block_n_range = [256]
            block_k_range = [32]
            group_m_range = [8]
            matrix_instr_nonkdim_range = [16]
            waves_per_eu_range = [0]
            kpack_range = [2]
            num_warps_range = [8]
            num_stage_range = [2] if triton.__version__ >= "3.2.0" else [0]

        return [
            triton.Config(
                {
                    "BLOCK_M": block_m,
                    "BLOCK_N": block_n,
                    "BLOCK_K": block_k,
                    "GROUP_M": group_m,
                    "matrix_instr_nonkdim": matrix_instr_nonkdim,
                    "waves_per_eu": waves_per_eu,
                    "kpack": kpack,
                },
                num_stages=num_stages,
                num_warps=num_warps,
            )
            for block_m in block_m_range
            for block_n in block_n_range
            for block_k in block_k_range
            for group_m in group_m_range
            for matrix_instr_nonkdim in matrix_instr_nonkdim_range
            for waves_per_eu in waves_per_eu_range
            for kpack in kpack_range
            for num_stages in num_stage_range
            for num_warps in num_warps_range
        ]
    else:
        return [
            triton.Config(
                {
                    "BLOCK_M": 32,
                    "BLOCK_N": 64,
                    "BLOCK_K": 32,
                    "GROUP_M": 8,
                },
                num_stages=5,
                num_warps=2,
            ),
            triton.Config(
                {
                    "BLOCK_M": 128,
                    "BLOCK_N": 256,
                    "BLOCK_K": 64,
                    "GROUP_M": 8,
                },
                num_stages=3,
                num_warps=8,
            ),
            triton.Config(
                {
                    "BLOCK_M": 64,
                    "BLOCK_N": 256,
                    "BLOCK_K": 32,
                    "GROUP_M": 8,
                },
                num_stages=4,
                num_warps=4,
            ),
            triton.Config(
                {
                    "BLOCK_M": 128,
                    "BLOCK_N": 128,
                    "BLOCK_K": 32,
                    "GROUP_M": 8,
                },
                num_stages=4,
                num_warps=4,
            ),
            triton.Config(
                {
                    "BLOCK_M": 128,
                    "BLOCK_N": 64,
                    "BLOCK_K": 32,
                    "GROUP_M": 8,
                },
                num_stages=4,
                num_warps=4,
            ),
            triton.Config(
                {
                    "BLOCK_M": 64,
                    "BLOCK_N": 128,
                    "BLOCK_K": 32,
                    "GROUP_M": 8,
                },
                num_stages=4,
                num_warps=4,
            ),
            triton.Config(
                {
                    "BLOCK_M": 128,
                    "BLOCK_N": 32,
                    "BLOCK_K": 32,
                    "GROUP_M": 8,
                },
                num_stages=4,
                num_warps=4,
            ),
            triton.Config(
                {
                    "BLOCK_M": 64,
                    "BLOCK_N": 32,
                    "BLOCK_K": 32,
                    "GROUP_M": 8,
                },
                num_stages=5,
                num_warps=2,
            ),
        ]


@triton_autotune(
    configs=get_mm_configs(),
    key=["N", "K"],
)
@triton.jit
def _addmm_fwd(
    x_ptr,
    w_ptr,
    y_ptr,
    z_ptr,
    M,
    N,
    K,
    stride_xm,
    stride_xk,
    stride_wk,
    stride_wn,
    stride_ym,
    stride_yn,
    stride_zm,
    stride_zn,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    BLOCK_K: tl.constexpr,
    GROUP_M: tl.constexpr,
    ALLOW_TF32: tl.constexpr,
    BROADCAST_Y: tl.constexpr,
):
    pid_0, pid_1 = tl.program_id(axis=0), tl.program_id(axis=1)
    pid = pid_0 * tl.num_programs(axis=1) + pid_1
    num_pid_m = tl.cdiv(M, BLOCK_M)
    num_pid_n = tl.cdiv(N, BLOCK_N)
    num_pid_in_group = GROUP_M * num_pid_n
    group_id = pid // num_pid_in_group
    first_pid_m = group_id * GROUP_M
    group_size_m = min(num_pid_m - first_pid_m, GROUP_M)
    pid_m = first_pid_m + (pid % group_size_m)
    pid_n = (pid % num_pid_in_group) // group_size_m

    offs_m = tl.arange(0, BLOCK_M)
    offs_k = tl.arange(0, BLOCK_K)
    offs_n = tl.arange(0, BLOCK_N)
    mask_m = (pid_m * BLOCK_M + offs_m)[:, None] < M
    mask_n = (pid_n * BLOCK_N + offs_n)[None, :] < N
    x_ptr += pid_m.to(tl.int64) * BLOCK_M * stride_xm
    x_ptrs = x_ptr + (offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk)
    w_ptr += pid_n.to(tl.int64) * BLOCK_N * stride_wn
    w_ptrs = w_ptr + (offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn)
    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)
    for k in range(0, tl.cdiv(K, BLOCK_K)):
        mask_k = offs_k[None, :] < K - k * BLOCK_K
        x = tl.load(x_ptrs, mask=mask_k & mask_m, other=0.0)
        mask_k = offs_k[:, None] < K - k * BLOCK_K
        w = tl.load(w_ptrs, mask=mask_k & mask_n, other=0.0)
        accumulator += tl.dot(x, w, allow_tf32=ALLOW_TF32)
        x_ptrs += BLOCK_K * stride_xk
        w_ptrs += BLOCK_K * stride_wk

    z_mask = mask_m & mask_n
    if BROADCAST_Y:
        # y is a vector, broadcast to add to z
        y_ptr += pid_n.to(tl.int64) * BLOCK_N * stride_yn
        y_ptrs = y_ptr + stride_yn * offs_n[None, :]
        y = tl.load(y_ptrs, mask=mask_n)
    else:
        y_ptr += pid_m.to(tl.int64) * BLOCK_M * stride_ym
        y_ptr += pid_n.to(tl.int64) * BLOCK_N * stride_yn
        y_ptrs = y_ptr + stride_ym * offs_m[:, None] + stride_yn * offs_n[None, :]
        y = tl.load(y_ptrs, mask=z_mask)
    z = (accumulator + y.to(tl.float32)).to(z_ptr.dtype.element_ty)
    z_ptr += pid_m.to(tl.int64) * BLOCK_M * stride_zm
    z_ptr += pid_n.to(tl.int64) * BLOCK_N * stride_zn
    z_ptrs = z_ptr + stride_zm * offs_m[:, None] + stride_zn * offs_n[None, :]
    tl.store(z_ptrs, z, mask=z_mask)


class _AddMmFunction(torch.autograd.Function):
    @staticmethod
    # pyre-ignore[14]
    def forward(
        ctx,
        x: torch.Tensor,
        w: torch.Tensor,
        y: torch.Tensor,
    ) -> torch.Tensor:
        M, K = x.shape
        KB, N = w.shape
        assert K == KB, f"incompatible dimensions {K}, {KB}"

        z = torch.empty((M, N), device=x.device, dtype=x.dtype)
        if M == 0 or N == 0:
            return z

        grid = lambda meta: (  # noqa E731
            triton.cdiv(M, meta["BLOCK_M"]),
            triton.cdiv(N, meta["BLOCK_N"]),
        )

        _addmm_fwd[grid](
            x,
            w,
            y,
            z,
            M,
            N,
            K,
            x.stride(0),
            x.stride(1),
            w.stride(0),
            w.stride(1),
            y.stride(0),
            y.stride(1),
            z.stride(0),
            z.stride(1),
            ALLOW_TF32=torch.backends.cuda.matmul.allow_tf32,
        )
        return z

</content>

<content full_path="generative_recommenders/ops/triton/triton_layer_norm.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

#!/usr/bin/env python3


from typing import List, Optional, Tuple

import torch

# @manual=//triton:triton
import triton

# @manual=//triton:triton
import triton.language as tl

from generative_recommenders.common import (
    switch_to_contiguous_if_needed,
    triton_autotune,
)


@triton.jit
def _layer_norm_fwd(
    X,
    Y,
    Mean,
    Rstd,
    D,
    eps,
    stride_x,
    stride_y,
    TRAINING: tl.constexpr,
    BLOCK_D: tl.constexpr,
    COMPUTE_MEAN_AND_RSTD: tl.constexpr,
):
    row = tl.program_id(0)
    X += row.to(tl.int64) * stride_x
    Y += row.to(tl.int64) * stride_y
    cols = tl.arange(0, BLOCK_D)
    x = tl.load(X + cols, mask=cols < D, other=0.0).to(tl.float32)

    if COMPUTE_MEAN_AND_RSTD:
        mean = tl.sum(x, axis=0) / D
    else:
        mean = tl.load(Mean + row)
    x_mean = tl.where(cols < D, x - mean, 0.0)
    if COMPUTE_MEAN_AND_RSTD:
        _var = tl.zeros([BLOCK_D], dtype=tl.float32)
        _var += x_mean * x_mean
        var = tl.sum(_var, axis=0) / D
        rstd = 1 / tl.sqrt(var + eps)
        if TRAINING:
            tl.store(Mean + row, mean)
            tl.store(Rstd + row, rstd)
    else:
        rstd = tl.load(Rstd + row)

    # Normalize and apply linear transformation
    mask = cols < D
    y = x_mean * rstd
    # Write output
    tl.store(Y + cols, y.to(Y.dtype.element_ty), mask=mask)


@triton.jit
def _weighted_layer_norm_fwd(
    X,
    Y,
    W,
    B,
    Mean,
    Rstd,
    D,
    eps,
    stride_x,
    stride_y,
    IS_SWISH: tl.constexpr,
    TRAINING: tl.constexpr,
    BLOCK_D: tl.constexpr,
    COMPUTE_MEAN_AND_RSTD: tl.constexpr,
):
    row = tl.program_id(0)
    X += row.to(tl.int64) * stride_x
    Y += row.to(tl.int64) * stride_y
    cols = tl.arange(0, BLOCK_D)
    x = tl.load(X + cols, mask=cols < D, other=0.0).to(tl.float32)

    if COMPUTE_MEAN_AND_RSTD:
        mean = tl.sum(x, axis=0) / D
    else:
        mean = tl.load(Mean + row)

    x_mean = tl.where(cols < D, x - mean, 0.0)
    if COMPUTE_MEAN_AND_RSTD:
        _var = tl.zeros([BLOCK_D], dtype=tl.float32)
        _var += x_mean * x_mean
        var = tl.sum(_var, axis=0) / D
        rstd = 1 / tl.sqrt(var + eps)
        if TRAINING:
            tl.store(Mean + row, mean)
            tl.store(Rstd + row, rstd)
    else:
        rstd = tl.load(Rstd + row)

    # Normalize and apply linear transformation
    mask = cols < D
    y = x_mean * rstd
    w = tl.load(W + cols, mask=mask).to(tl.float32)
    b = tl.load(B + cols, mask=mask).to(tl.float32)
    y = y * w + b
    if IS_SWISH:
        y = tl.sigmoid(y) * x
    # Write output
    tl.store(Y + cols, y.to(Y.dtype.element_ty), mask=mask)


@triton.jit
def _layer_norm_bwd_dx(
    DX,
    DY,
    X,
    Mean,
    Rstd,
    stride_dx,
    stride_dy,
    stride_x,
    D,
    eps,
    BLOCK_D: tl.constexpr,
):
    row = tl.program_id(0)
    cols = tl.arange(0, BLOCK_D)
    mask = cols < D
    X += row.to(tl.int64) * stride_x
    DY += row.to(tl.int64) * stride_dy
    DX += row.to(tl.int64) * stride_dx

    # Load data to SRAM
    x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)
    dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)
    mean = tl.load(Mean + row)
    rstd = tl.load(Rstd + row)

    # Compute dx
    xhat = (x - mean) * rstd
    xhat = tl.where(mask, xhat, 0.0)
    dy = tl.where(mask, dy, 0.0)
    c1 = tl.sum(xhat * dy, axis=0) / D
    c2 = tl.sum(dy, axis=0) / D
    dx = (dy - (xhat * c1 + c2)) * rstd
    # Write dx
    tl.store(DX + cols, dx, mask=mask)


@triton.jit
def _weighted_layer_norm_bwd_dx(
    DX,
    DY,
    DW,
    DB,
    X,
    W,
    B,
    Mean,
    Rstd,
    stride_dx,
    stride_dy,
    stride_x,
    D,
    eps,
    IS_SWISH: tl.constexpr,
    N,
    BLOCK_D: tl.constexpr,
):
    pid = tl.program_id(0)
    tile_num = tl.num_programs(0)
    rows_per_tile = N // tile_num
    if pid < N % tile_num:
        rows_per_tile += 1

    cols = tl.arange(0, BLOCK_D)
    mask = cols < D

    row = pid

    for idx in range(rows_per_tile):
        x_ptrs = X + row.to(tl.int64) * stride_x
        dy_ptrs = DY + row.to(tl.int64) * stride_dy
        dx_ptrs = DX + row.to(tl.int64) * stride_dx
        dw_ptrs = DW + pid.to(tl.int64) * D
        dw_ptrs += cols
        db_ptrs = DB + pid.to(tl.int64) * D
        db_ptrs += cols

        # Load data to SRAM
        x = tl.load(x_ptrs + cols, mask=mask, other=0).to(tl.float32)
        dy = tl.load(dy_ptrs + cols, mask=mask, other=0).to(tl.float32)
        mean = tl.load(Mean + row)
        rstd = tl.load(Rstd + row)

        # Compute dx
        xhat = (x - mean) * rstd
        w = tl.load(W + cols, mask=mask).to(tl.float32)
        wdy = w * dy
        xhat = tl.where(mask, xhat, 0.0)
        wdy = tl.where(mask, wdy, 0.0)
        sigmoid_layer_norm = None
        if IS_SWISH:
            b = tl.load(B + cols, mask=mask).to(tl.float32)
            sigmoid_layer_norm = tl.sigmoid(xhat * w + b)
            sigmoid_layer_norm = tl.where(mask, sigmoid_layer_norm, 0.0)
            x_ = wdy * x * sigmoid_layer_norm * (1 - sigmoid_layer_norm)
            x_ = tl.where(mask, x_, 0.0)

            c1 = tl.sum(xhat * x_, axis=0) / D
            c2 = tl.sum(x_, axis=0) / D
            dx = (x_ - (xhat * c1 + c2)) * rstd
            dx = dy * sigmoid_layer_norm + dx
        else:
            c1 = tl.sum(xhat * wdy, axis=0) / D
            c2 = tl.sum(wdy, axis=0) / D
            dx = (wdy - (xhat * c1 + c2)) * rstd

        # Write dx
        tl.store(dx_ptrs + cols, dx, mask=mask)

        # Accumulate partial sums for dw/db
        if IS_SWISH:
            partial_dw = dy * x * xhat * sigmoid_layer_norm * (1 - sigmoid_layer_norm)
            partial_db = dy * x * sigmoid_layer_norm * (1 - sigmoid_layer_norm)
        else:
            partial_dw = dy * xhat
            partial_db = dy
        # First store doesn't accumulate
        if idx > 0:
            partial_dw += tl.load(dw_ptrs, mask=mask)
            partial_db += tl.load(db_ptrs, mask=mask)
        tl.store(dw_ptrs, partial_dw, mask=mask)
        tl.store(db_ptrs, partial_db, mask=mask)
        row += tile_num


def _get_bwd_dwdb_configs() -> List[triton.Config]:
    configs = []
    for BLOCK_N in [32, 64, 128, 256]:
        for num_warps in [8, 16] + ([] if torch.ops.hip else [32]):
            configs.append(
                triton.Config(
                    {"BLOCK_N": BLOCK_N},
                    num_warps=num_warps,
                )
            )
    return configs


@triton_autotune(
    configs=_get_bwd_dwdb_configs(),
    key=["D"],
)
@triton.jit
def _layer_norm_bwd_dwdb(
    DW,
    DB,
    FINAL_DW,
    FINAL_DB,
    N,
    D,
    BLOCK_N: tl.constexpr,
    BLOCK_D: tl.constexpr,
):
    pid = tl.program_id(0)
    cols = pid * BLOCK_D + tl.arange(0, BLOCK_D)
    dw = tl.zeros((BLOCK_N, BLOCK_D), dtype=tl.float32)
    db = tl.zeros((BLOCK_N, BLOCK_D), dtype=tl.float32)

    for i in range(0, N, BLOCK_N):
        rows = i + tl.arange(0, BLOCK_N)
        # pyre-fixme[16]: `int` has no attribute `__getitem__`.
        mask = (rows[:, None] < N) & (cols[None, :] < D)
        offs = rows[:, None] * D + cols[None, :]
        dw += tl.load(DW + offs, mask=mask, other=0.0)
        db += tl.load(DB + offs, mask=mask, other=0.0)

    sum_dw = tl.sum(dw, axis=0)
    sum_db = tl.sum(db, axis=0)
    tl.store(FINAL_DW + cols, sum_dw.to(FINAL_DW.dtype.element_ty), mask=cols < D)
    tl.store(FINAL_DB + cols, sum_db.to(FINAL_DB.dtype.element_ty), mask=cols < D)


def triton_weighted_layer_norm_fwd(
    x: torch.Tensor,
    weight: Optional[torch.Tensor],
    bias: Optional[torch.Tensor],
    eps: float,
    mean: Optional[torch.Tensor] = None,
    rstd: Optional[torch.Tensor] = None,
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, int, int]:
    assert x.dim() == 2, f"x.dim() == {x.dim()}, expected 2"
    x = switch_to_contiguous_if_needed(x)
    N, D = x.shape
    learnable = weight is not None
    if learnable:
        assert bias is not None and weight is not None
        assert weight.dim() == 1
        assert bias.dim() == 1
        assert weight.numel() == D
        assert bias.numel() == D

    y = torch.empty_like(x)
    compute_mean_and_rstd = mean is None or rstd is None
    if mean is None:
        mean = torch.empty((N,), dtype=torch.float32, device=x.device)
    if rstd is None:
        rstd = torch.empty((N,), dtype=torch.float32, device=x.device)

    # Less than 64KB per feature: enqueue fused kernel
    MAX_FUSED_SIZE = 65536 // x.element_size()
    BLOCK_D: int = min(MAX_FUSED_SIZE, triton.next_power_of_2(D))
    if D > BLOCK_D:
        raise RuntimeError("This layer norm doesn't support feature dim >= 64KB.")

    num_warps: int = min(max(BLOCK_D // 256, 1), 8)

    if learnable:
        # pyre-ignore[28]
        _weighted_layer_norm_fwd[(N,)](
            x,
            y,
            weight,
            bias,
            mean,
            rstd,
            D,
            eps,
            x.stride(0),
            y.stride(0),
            IS_SWISH=False,
            TRAINING=True,
            BLOCK_D=BLOCK_D,
            COMPUTE_MEAN_AND_RSTD=compute_mean_and_rstd,
            num_warps=num_warps,
        )
    else:
        # pyre-ignore[28]
        _layer_norm_fwd[(N,)](
            x,
            y,
            mean,
            rstd,
            D,
            eps,
            x.stride(0),
            y.stride(0),
            TRAINING=True,
            BLOCK_D=BLOCK_D,
            COMPUTE_MEAN_AND_RSTD=compute_mean_and_rstd,
            num_warps=num_warps,
        )
    return y, mean, rstd, BLOCK_D, num_warps


def triton_weighted_layer_norm_bwd(
    dy: torch.Tensor,
    x: torch.Tensor,
    weight: Optional[torch.Tensor],
    bias: Optional[torch.Tensor],
    mean: torch.Tensor,
    rstd: torch.Tensor,
    learnable: bool,
    eps: float,
    BLOCK_D: int,
    num_warps: int,
) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]:
    if learnable:
        assert weight is not None and bias is not None
        N, D = x.shape
        dx = torch.empty_like(x)
        sms = torch.cuda.get_device_properties(x.device).multi_processor_count
        tile_num = max(1, min(sms * 8, N // 4))
        _dweight = torch.empty((tile_num, D), dtype=torch.float32, device=x.device)
        _dbias = torch.empty((tile_num, D), dtype=torch.float32, device=x.device)
        dweight = torch.empty((D,), dtype=weight.dtype, device=x.device)
        dbias = torch.empty((D,), dtype=weight.dtype, device=x.device)
        # pyre-ignore[28]
        _weighted_layer_norm_bwd_dx[(tile_num,)](
            dx,
            dy,
            _dweight,
            _dbias,
            x,
            weight,
            bias,
            mean,
            rstd,
            dx.stride(0),
            dy.stride(0),
            x.stride(0),
            D,
            eps,
            IS_SWISH=False,
            N=N,
            BLOCK_D=BLOCK_D,
            num_warps=num_warps,
        )

        def grid(META):
            return (triton.cdiv(D, META["BLOCK_D"]),)

        blocks = triton.next_power_of_2(sms * 4)
        BLOCK_D = triton.next_power_of_2(triton.cdiv(D, blocks))
        BLOCK_D = min(max(BLOCK_D, 4), 128)
        _layer_norm_bwd_dwdb[grid](
            _dweight,
            _dbias,
            dweight,
            dbias,
            tile_num,
            D,
            BLOCK_D=BLOCK_D,
        )

        return dx, dweight, dbias
    else:
        N, D = x.shape
        dx = torch.empty_like(x)
        # pyre-ignore[28]
        _layer_norm_bwd_dx[(N,)](
            dx,
            dy,
            x,
            mean,
            rstd,
            dx.stride(0),
            dy.stride(0),
            x.stride(0),
            D,
            eps,
            BLOCK_D=BLOCK_D,
            num_warps=num_warps,
        )
        return dx, None, None


class LayerNormFunction(torch.autograd.Function):
    @staticmethod
    # pyre-ignore[14]
    def forward(
        ctx,
        x: torch.Tensor,
        weight: Optional[torch.Tensor],
        bias: Optional[torch.Tensor],
        eps: float,
    ) -> torch.Tensor:
        y, mean, rstd, BLOCK_D, num_warps = triton_weighted_layer_norm_fwd(
            x=x,
            weight=weight,
            bias=bias,
            eps=eps,
        )
        learnable = weight is not None
        if learnable:
            ctx.save_for_backward(x, weight, bias, mean, rstd)
        else:
            ctx.save_for_backward(x, mean, rstd)
        ctx.BLOCK_D = BLOCK_D
        ctx.num_warps = num_warps
        ctx.eps = eps
        ctx.learnable = learnable
        return y

    @staticmethod
    # pyre-ignore[14]
    def backward(
        ctx, dy: torch.Tensor
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor], None]:
        if ctx.learnable:
            x, weight, bias, mean, rstd = ctx.saved_tensors
        else:
            x, mean, rstd = ctx.saved_tensors
            weight, bias = None, None
        dx, dweight, dbias = triton_weighted_layer_norm_bwd(
            dy=dy,
            x=x,
            weight=weight,
            bias=bias,
            mean=mean,
            rstd=rstd,
            learnable=ctx.learnable,
            eps=ctx.eps,
            BLOCK_D=ctx.BLOCK_D,
            num_warps=ctx.num_warps,
        )
        return dx, dweight, dbias, None


@triton.jit
def _weighted_rms_norm_fwd(
    X,
    Y,
    W,
    Rstd,
    D,
    eps,
    stride_x,
    stride_y,
    BLOCK_D: tl.constexpr,
):
    row = tl.program_id(0)
    X += row.to(tl.int64) * stride_x
    Y += row.to(tl.int64) * stride_y
    cols = tl.arange(0, BLOCK_D)
    x = tl.load(X + cols, mask=cols < D, other=0.0).to(tl.float32)

    # Compute variance
    _var = tl.zeros([BLOCK_D], dtype=tl.float32)
    x_mean = tl.where(cols < D, x, 0.0)
    _var += x_mean * x_mean
    var = tl.sum(_var, axis=0) / D
    rstd = 1 / tl.sqrt(var + eps)
    tl.store(Rstd + row, rstd)

    # Normalize and apply linear transformation
    mask = cols < D
    y = x_mean * rstd
    w = tl.load(W + cols, mask=mask).to(tl.float32)
    y = y * w
    # Write output
    tl.store(Y + cols, y.to(Y.dtype.element_ty), mask=mask)


@triton.jit
def _weighted_rms_norm_bwd_dx(
    DX,
    DY,
    DW,
    X,
    W,
    Rstd,
    Lock,
    stride_dx,
    stride_dy,
    stride_x,
    D,
    eps,
    GROUP_N,
    BLOCK_D: tl.constexpr,
):
    row = tl.program_id(0)
    cols = tl.arange(0, BLOCK_D)
    mask = cols < D
    X += row.to(tl.int64) * stride_x
    DY += row.to(tl.int64) * stride_dy
    DX += row.to(tl.int64) * stride_dx

    # Load data to SRAM
    x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)
    dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)
    rstd = tl.load(Rstd + row)

    # Compute dx
    xhat = x * rstd
    w = tl.load(W + cols, mask=mask).to(tl.float32)
    wdy = w * dy

    xhat = tl.where(mask, xhat, 0.0)
    wdy = tl.where(mask, wdy, 0.0)
    c1 = tl.sum(xhat * wdy, axis=0) / D
    dx = (wdy - (xhat * c1)) * rstd
    # Write dx
    tl.store(DX + cols, dx, mask=mask)

    # Offset locks and weights/biases gradient pointer for parallel reduction
    lock_id = row % GROUP_N
    Lock += lock_id
    Count = Lock + GROUP_N
    DW = DW + lock_id * D + cols
    # Accumulate partial sums for dw/db
    partial_dw = dy * xhat
    while tl.atomic_cas(Lock, 0, 1) == 1:
        pass
    count = tl.load(Count)
    # First store doesn't accumulate
    if count == 0:
        tl.atomic_xchg(Count, 1)
    else:
        partial_dw += tl.load(DW, mask=mask)
    tl.store(DW, partial_dw, mask=mask)
    # Release the lock
    tl.atomic_xchg(Lock, 0)


@triton_autotune(
    configs=_get_bwd_dwdb_configs(),
    key=["D"],
)
@triton.jit
def _rms_norm_bwd_dwdb(
    DW,
    FINAL_DW,
    N,
    D,
    BLOCK_N: tl.constexpr,
    BLOCK_D: tl.constexpr,
):
    pid = tl.program_id(0)
    cols = pid * BLOCK_D + tl.arange(0, BLOCK_D)
    dw = tl.zeros((BLOCK_N, BLOCK_D), dtype=tl.float32)

    for i in range(0, N, BLOCK_N):
        rows = i + tl.arange(0, BLOCK_N)
        # pyre-fixme[16]: `int` has no attribute `__getitem__`.
        mask = (rows[:, None] < N) & (cols[None, :] < D)
        offs = rows[:, None] * D + cols[None, :]
        dw += tl.load(DW + offs, mask=mask, other=0.0)

    sum_dw = tl.sum(dw, axis=0)
    tl.store(FINAL_DW + cols, sum_dw.to(FINAL_DW.dtype.element_ty), mask=cols < D)


class RMSNormFunction(torch.autograd.Function):
    @staticmethod
    # pyre-ignore[14]
    def forward(
        ctx,
        x: torch.Tensor,
        weight: torch.Tensor,
        eps: float,
    ) -> torch.Tensor:
        assert x.dim() == 2
        x = switch_to_contiguous_if_needed(x)
        N, D = x.shape
        assert weight.dim() == 1
        assert weight.numel() == D

        y = torch.empty_like(x)
        rstd = torch.empty((N,), dtype=torch.float32, device=x.device)

        # Less than 64KB per feature: enqueue fused kernel
        MAX_FUSED_SIZE = 65536 // x.element_size()
        BLOCK_D = min(MAX_FUSED_SIZE, triton.next_power_of_2(D))
        if D > BLOCK_D:
            raise RuntimeError("This layer norm doesn't support feature dim >= 64KB.")

        num_warps = min(max(BLOCK_D // 256, 1), 8)

        # pyre-ignore[28]
        _weighted_rms_norm_fwd[(N,)](
            x,
            y,
            weight,
            rstd,
            D,
            eps,
            x.stride(0),
            y.stride(0),
            BLOCK_D=BLOCK_D,
            num_warps=num_warps,
        )
        ctx.save_for_backward(x, weight, rstd)

        ctx.BLOCK_D = BLOCK_D
        ctx.num_warps = num_warps
        ctx.eps = eps
        return y

    @staticmethod
    # pyre-ignore[14]
    def backward(
        ctx, dy: torch.Tensor
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], None]:
        x, weight, rstd = ctx.saved_tensors
        N, D = x.shape
        dx = torch.empty_like(x)
        if D <= 1024:
            GROUP_N = 256 * 8
        elif D <= 4096:
            GROUP_N = 128 * 8
        elif D <= 8192:
            GROUP_N = 96 * 8
        else:
            GROUP_N = 64 * 8
        GROUP_N = N if GROUP_N > N else GROUP_N
        locks = torch.zeros(2 * GROUP_N, dtype=torch.int32, device=x.device)
        _dweight = torch.empty((GROUP_N, D), dtype=torch.float32, device=x.device)
        dweight = torch.empty((D,), dtype=weight.dtype, device=x.device)
        # pyre-ignore[28]
        _weighted_rms_norm_bwd_dx[(N,)](
            dx,
            dy,
            _dweight,
            x,
            weight,
            rstd,
            locks,
            dx.stride(0),
            dy.stride(0),
            x.stride(0),
            D,
            ctx.eps,
            GROUP_N=GROUP_N,
            BLOCK_D=ctx.BLOCK_D,
            num_warps=ctx.num_warps,
        )

        def grid(META):
            return (triton.cdiv(D, META["BLOCK_D"]),)

        sms = torch.cuda.get_device_properties(x.device).multi_processor_count
        blocks = triton.next_power_of_2(sms * 4)
        BLOCK_D = triton.next_power_of_2(triton.cdiv(D, blocks))
        BLOCK_D = min(max(BLOCK_D, 4), 128)
        _rms_norm_bwd_dwdb[grid](
            _dweight,
            dweight,
            GROUP_N,
            D,
            BLOCK_D=BLOCK_D,
        )

        return dx, dweight, None


class SwishLayerNormFunction(torch.autograd.Function):
    @staticmethod
    # pyre-ignore[14]
    def forward(
        ctx,
        x: torch.Tensor,
        weight: torch.Tensor,
        bias: torch.Tensor,
        eps: float,
    ) -> torch.Tensor:
        assert x.dim() == 2, f"x.dim() == {x.dim()}, expected 2"
        x = switch_to_contiguous_if_needed(x)
        N, D = x.shape

        assert bias is not None and weight is not None
        assert weight.dim() == 1
        assert bias.dim() == 1
        assert weight.numel() == D
        assert bias.numel() == D

        y = torch.empty_like(x)
        mean = torch.empty((N,), dtype=torch.float32, device=x.device)
        rstd = torch.empty((N,), dtype=torch.float32, device=x.device)

        BLOCK_D = triton.next_power_of_2(D)
        num_warps = min(max(BLOCK_D // 256, 1), 8)

        # pyre-ignore[28]
        _weighted_layer_norm_fwd[(N,)](
            x,
            y,
            weight,
            bias,
            mean,
            rstd,
            D,
            eps,
            x.stride(0),
            y.stride(0),
            IS_SWISH=True,
            TRAINING=True,
            BLOCK_D=BLOCK_D,
            COMPUTE_MEAN_AND_RSTD=True,
            num_warps=num_warps,
        )

        ctx.save_for_backward(x, weight, bias, mean, rstd)
        ctx.BLOCK_D = BLOCK_D
        ctx.num_warps = num_warps
        ctx.eps = eps

        return y

    @staticmethod
    # pyre-ignore[14]
    def backward(
        ctx, dy: torch.Tensor
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor], None]:
        x, weight, bias, mean, rstd = ctx.saved_tensors
        N, D = x.shape
        dx = torch.empty_like(x)
        sms = torch.cuda.get_device_properties(x.device).multi_processor_count
        tile_num = min(sms * 8, N // 4)
        _dweight = torch.empty((tile_num, D), dtype=torch.float32, device=x.device)
        _dbias = torch.empty((tile_num, D), dtype=torch.float32, device=x.device)
        dweight = torch.empty((D,), dtype=weight.dtype, device=x.device)
        dbias = torch.empty((D,), dtype=weight.dtype, device=x.device)
        # pyre-ignore[28]
        _weighted_layer_norm_bwd_dx[(tile_num,)](
            dx,
            dy,
            _dweight,
            _dbias,
            x,
            weight,
            bias,
            mean,
            rstd,
            dx.stride(0),
            dy.stride(0),
            x.stride(0),
            D,
            ctx.eps,
            IS_SWISH=True,
            N=N,
            BLOCK_D=ctx.BLOCK_D,
            num_warps=ctx.num_warps,
        )

        def grid(META):
            return (triton.cdiv(D, META["BLOCK_D"]),)

        blocks = triton.next_power_of_2(sms * 4)
        BLOCK_D = triton.next_power_of_2(triton.cdiv(D, blocks))
        BLOCK_D = min(max(BLOCK_D, 4), 128)
        _layer_norm_bwd_dwdb[grid](
            _dweight,
            _dbias,
            dweight,
            dbias,
            tile_num,
            D,
            BLOCK_D=BLOCK_D,
        )

        return dx, dweight, dbias, None


@torch.fx.wrap
def triton_layer_norm(
    x: torch.Tensor,
    weight: Optional[torch.Tensor],
    bias: Optional[torch.Tensor],
    eps: float,
) -> torch.Tensor:
    return LayerNormFunction.apply(x, weight, bias, eps)


def triton_rms_norm(
    x: torch.Tensor,
    weight: Optional[torch.Tensor],
    eps: float,
) -> torch.Tensor:
    return RMSNormFunction.apply(x, weight, eps)


@torch.fx.wrap
def triton_swish_layer_norm(
    x: torch.Tensor,
    normalized_shape: List[int],
    weight: torch.Tensor,
    bias: torch.Tensor,
    eps: float,
) -> torch.Tensor:
    return SwishLayerNormFunction.apply(x, weight, bias, eps)

</content>

<content full_path="generative_recommenders/ops/triton/triton_hstu_attention.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

#!/usr/bin/env python3

from typing import List, Optional, Tuple

import torch

# @manual=//triton:triton
import triton

# @manual=//triton:triton
import triton.language as tl

from generative_recommenders.common import (
    autotune_max_seq_len,
    NamedSpecType,
    prev_power_of_2,
    register_tritoncc_specs,
    switch_to_contiguous_if_needed,
    triton_autotune,
    VersionedSpec,
)

try:
    # @manual=//triton:triton
    from triton.language.extra.libdevice import fast_dividef
except ImportError:
    try:
        # @manual=//triton:triton
        from triton.language.extra.cuda.libdevice import fast_dividef
    except ImportError:
        # pyre-ignore: Undefined import [21]
        # @manual=//triton:triton
        from triton.language.math import fast_dividef


def _get_fw_configs() -> List[triton.Config]:  # noqa: C901
    configs = []
    if torch.version.hip:
        for BLOCK_M in [32, 64, 128]:
            for BLOCK_N in [32, 64]:
                for num_stages in [1, 2]:
                    for num_warps in [4, 8]:
                        for matrix_instr_nonkdim in [16, 32]:
                            configs.append(
                                triton.Config(
                                    {
                                        "BLOCK_M": BLOCK_M,
                                        "BLOCK_N": BLOCK_N,
                                        "matrix_instr_nonkdim": matrix_instr_nonkdim,
                                        "waves_per_eu": 0,
                                        "kpack": 2,
                                    },
                                    num_stages=num_stages,
                                    num_warps=num_warps,
                                )
                            )
    else:
        configs = [
            triton.Config(
                {"BLOCK_M": 16, "BLOCK_N": 32},
                num_stages=2,
                num_warps=2,
            ),
            triton.Config(
                {"BLOCK_M": 32, "BLOCK_N": 32},
                num_stages=2,
                num_warps=2,
            ),
            triton.Config(
                {"BLOCK_M": 32, "BLOCK_N": 32},
                num_stages=4,
                num_warps=2,
            ),
            triton.Config(
                {"BLOCK_M": 32, "BLOCK_N": 32},
                num_stages=2,
                num_warps=4,
            ),
            triton.Config(
                {"BLOCK_M": 32, "BLOCK_N": 32},
                num_stages=4,
                num_warps=4,
            ),
            triton.Config(
                {"BLOCK_M": 32, "BLOCK_N": 64},
                num_stages=2,
                num_warps=4,
            ),
            triton.Config(
                {"BLOCK_M": 32, "BLOCK_N": 64},
                num_stages=4,
                num_warps=4,
            ),
            triton.Config(
                {"BLOCK_M": 32, "BLOCK_N": 64},
                num_stages=4,
                num_warps=8,
            ),
            triton.Config(
                {"BLOCK_M": 32, "BLOCK_N": 128},
                num_stages=2,
                num_warps=4,
            ),
            triton.Config(
                {"BLOCK_M": 32, "BLOCK_N": 128},
                num_stages=2,
                num_warps=8,
            ),
            triton.Config(
                {"BLOCK_M": 64, "BLOCK_N": 32},
                num_stages=4,
                num_warps=2,
            ),
            triton.Config(
                {"BLOCK_M": 64, "BLOCK_N": 32},
                num_stages=2,
                num_warps=4,
            ),
            triton.Config(
                {"BLOCK_M": 64, "BLOCK_N": 32},
                num_stages=4,
                num_warps=4,
            ),
            triton.Config(
                {"BLOCK_M": 64, "BLOCK_N": 32},
                num_stages=2,
                num_warps=8,
            ),
            triton.Config(
                {"BLOCK_M": 64, "BLOCK_N": 64},
                num_stages=2,
                num_warps=2,
            ),
            triton.Config(
                {"BLOCK_M": 64, "BLOCK_N": 64},
                num_stages=2,
                num_warps=4,
            ),
            triton.Config(
                {"BLOCK_M": 64, "BLOCK_N": 64},
                num_stages=4,
                num_warps=4,
            ),
            triton.Config(
                {"BLOCK_M": 64, "BLOCK_N": 64},
                num_stages=4,
                num_warps=8,
            ),
            triton.Config(
                {"BLOCK_M": 128, "BLOCK_N": 32},
                num_stages=2,
                num_warps=2,
            ),
            triton.Config(
                {"BLOCK_M": 128, "BLOCK_N": 32},
                num_stages=4,
                num_warps=2,
            ),
            triton.Config(
                {"BLOCK_M": 128, "BLOCK_N": 32},
                num_stages=2,
                num_warps=4,
            ),
            triton.Config(
                {"BLOCK_M": 128, "BLOCK_N": 32},
                num_stages=4,
                num_warps=4,
            ),
            triton.Config(
                {"BLOCK_M": 128, "BLOCK_N": 32},
                num_stages=2,
                num_warps=8,
            ),
            triton.Config(
                {"BLOCK_M": 128, "BLOCK_N": 32},
                num_stages=4,
                num_warps=8,
            ),
            triton.Config(
                {"BLOCK_M": 128, "BLOCK_N": 64},
                num_stages=2,
                num_warps=4,
            ),
            triton.Config(
                {"BLOCK_M": 128, "BLOCK_N": 64},
                num_stages=2,
                num_warps=8,
            ),
            triton.Config(
                {"BLOCK_M": 128, "BLOCK_N": 64},
                num_stages=4,
                num_warps=8,
            ),
            triton.Config(
                {"BLOCK_M": 128, "BLOCK_N": 128},
                num_stages=4,
                num_warps=4,
            ),
            triton.Config(
                {"BLOCK_M": 128, "BLOCK_N": 128},
                num_stages=2,
                num_warps=8,
            ),
        ]
    return configs


@triton.jit
def _hstu_attn_fwd_one_block(  # noqa: C901
    start_n,
    seq_len,
    offs_m,
    offs_n,
    mask_m,
    mask_n,
    q,
    K_block_ptr,
    V_block_ptr,
    n_targets,
    alpha,
    MAX_SEQ_LEN,
    contextual_seq_len,
    MAX_ATTN_LEN: tl.constexpr,
    CAUSAL: tl.constexpr,
    HAS_MULTIPLE_TARGETS: tl.constexpr,
    HAS_CONTEXTUAL_SEQ_LEN: tl.constexpr,
    IS_DELTA_Q: tl.constexpr,
    ALLOW_TF32: tl.constexpr,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
):
    start_n = tl.multiple_of(start_n, BLOCK_N)
    # -- compute qk ----
    k = tl.load(K_block_ptr, boundary_check=(1,), padding_option="zero")
    qk = tl.dot(q, k, allow_tf32=ALLOW_TF32) * alpha
    invalid_mask = offs_m[:, None] == offs_n[None, :]
    max_ids = seq_len
    if HAS_CONTEXTUAL_SEQ_LEN:
        offs_m = offs_m - contextual_seq_len + 1
        offs_m = tl.where(
            offs_m > 0,
            offs_m,
            0,
        )
        offs_n = offs_n - contextual_seq_len + 1
        offs_n = tl.where(
            offs_n > 0,
            offs_n,
            0,
        )
        max_ids = max_ids - contextual_seq_len + 1
    if HAS_MULTIPLE_TARGETS:
        max_ids = max_ids - n_targets
        offs_m = tl.where(
            offs_m < max_ids,
            offs_m,
            max_ids,
        )
        offs_n = tl.where(
            offs_n < max_ids,
            offs_n,
            max_ids,
        )
    offs_n_minus_m = offs_n[None, :] - offs_m[:, None]
    if MAX_ATTN_LEN > 0:
        if CAUSAL:
            invalid_mask = invalid_mask or (
                offs_n_minus_m < 0 and offs_n_minus_m >= -MAX_ATTN_LEN
            )
    else:
        if CAUSAL:
            invalid_mask = invalid_mask or offs_n_minus_m < 0
    if HAS_CONTEXTUAL_SEQ_LEN:
        invalid_mask = invalid_mask or (
            offs_m[:, None] == 0 and offs_n[None, :] < max_ids
        )
    # pyre-fixme[16]: Module `math` has no attribute `fast_dividef`.
    silu = fast_dividef(qk, 1.0 + tl.exp(-qk)) * (1.0 / MAX_SEQ_LEN)
    silu = tl.where(invalid_mask, silu, 0)
    v = tl.load(V_block_ptr, boundary_check=(0,), padding_option="zero")
    silu = silu.to(v.dtype)
    return tl.dot(silu, v, allow_tf32=ALLOW_TF32)


@triton.jit
def _hstu_attn_fwd_compute(  # noqa C901
    Q,
    K,
    V,
    seq_offsets,
    delta_x_offsets,
    num_targets,
    Out,
    stride_qm,
    stride_qh,
    stride_kn,
    stride_kh,
    stride_vn,
    stride_vh,
    stride_om,
    stride_oh,
    alpha,
    Z,
    H,
    MAX_SEQ_LEN,
    DimQ,
    DimV,
    DeltaSize,
    contextual_seq_len,
    off_z,
    off_h,
    pid,
    CAUSAL: tl.constexpr,
    HAS_MULTIPLE_TARGETS: tl.constexpr,
    IS_DELTA_Q: tl.constexpr,
    ALLOW_TF32: tl.constexpr,
    BLOCK_D_Q: tl.constexpr,
    BLOCK_D_V: tl.constexpr,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    MAX_ATTN_LEN: tl.constexpr,
    HAS_CONTEXTUAL_SEQ_LEN: tl.constexpr,
):
    seq_start = tl.load(seq_offsets + off_z).to(tl.int64)
    off_h = off_h.to(tl.int64)
    off_z = off_z.to(tl.int64)
    seq_end = tl.load(seq_offsets + off_z + 1)
    seq_len = (seq_end - seq_start).to(tl.int32)
    if IS_DELTA_Q:
        start_m_delta = pid * BLOCK_M
        delta_start = tl.load(delta_x_offsets + off_z * DeltaSize)
        start_m = (start_m_delta + delta_start - seq_start).to(tl.int32)
    else:
        start_m_delta = 0
        start_m = pid * BLOCK_M
    if start_m < seq_len:
        if HAS_MULTIPLE_TARGETS:
            n_targets = tl.load(num_targets + off_z).to(tl.int32)
        else:
            n_targets = None

        # initialize offsets
        offs_m = start_m + tl.arange(0, BLOCK_M)
        offs_n = tl.arange(0, BLOCK_N)
        if IS_DELTA_Q:
            Q_block_ptr = tl.make_block_ptr(
                base=Q + off_h * stride_qh + off_z * DeltaSize * stride_qm,
                shape=(DeltaSize, BLOCK_D_Q),
                strides=(stride_qm, 1),
                offsets=(start_m_delta, 0),
                block_shape=(BLOCK_M, BLOCK_D_Q),
                order=(1, 0),
            )
        else:
            Q_block_ptr = tl.make_block_ptr(
                base=Q + off_h * stride_qh + seq_start * stride_qm,
                shape=(seq_len, BLOCK_D_Q),
                strides=(stride_qm, 1),
                offsets=(start_m, 0),
                block_shape=(BLOCK_M, BLOCK_D_Q),
                order=(1, 0),
            )
        K_block_ptr = tl.make_block_ptr(
            base=K + off_h * stride_kh + seq_start * stride_kn,
            shape=(BLOCK_D_Q, seq_len),
            strides=(1, stride_kn),
            offsets=(0, 0),
            block_shape=(BLOCK_D_Q, BLOCK_N),
            order=(0, 1),
        )
        V_block_ptr = tl.make_block_ptr(
            base=V + off_h * stride_vh + seq_start * stride_vn,
            shape=(seq_len, BLOCK_D_V),
            strides=(stride_vn, 1),
            offsets=(0, 0),
            block_shape=(BLOCK_N, BLOCK_D_V),
            order=(1, 0),
        )
        mask_m = offs_m < seq_len

        q = tl.load(Q_block_ptr, boundary_check=(0,), padding_option="zero")
        acc = tl.zeros([BLOCK_M, BLOCK_D_V], dtype=tl.float32)
        if CAUSAL:
            if HAS_MULTIPLE_TARGETS:
                if MAX_ATTN_LEN > 0:
                    if start_m > seq_len - n_targets:
                        low = seq_len - n_targets - MAX_ATTN_LEN
                    else:
                        low = start_m - MAX_ATTN_LEN
                    low = low if low > 0 else 0
                else:
                    low = 0
                uih_end = (seq_len - n_targets + BLOCK_N - 1) // BLOCK_N * BLOCK_N
                if uih_end < start_m:
                    high = seq_len - n_targets
                else:
                    high = start_m + BLOCK_M
                if HAS_CONTEXTUAL_SEQ_LEN:
                    if start_m < contextual_seq_len:
                        low = 0
                        high = seq_len - n_targets
            else:
                if MAX_ATTN_LEN > 0:
                    low = start_m - MAX_ATTN_LEN
                    low = low if low > 0 else 0
                else:
                    low = 0
                high = start_m + BLOCK_M
                if HAS_CONTEXTUAL_SEQ_LEN:
                    if start_m < contextual_seq_len:
                        low = 0
                        high = seq_len
        else:
            low = start_m
            high = seq_len
        # pyre-ignore[61]
        if low > 0:
            # pyre-ignore[61]
            K_block_ptr = tl.advance(K_block_ptr, (0, low))
            # pyre-ignore[61]
            V_block_ptr = tl.advance(V_block_ptr, (low, 0))
        # pyre-ignore[61]
        for start_n in range(low, high, BLOCK_N):
            cur_offs_n = offs_n + start_n
            mask_n = cur_offs_n < seq_len
            acc += _hstu_attn_fwd_one_block(
                start_n=start_n,
                seq_len=seq_len,
                offs_m=offs_m,
                offs_n=cur_offs_n,
                mask_m=mask_m,
                mask_n=mask_n,
                q=q,
                K_block_ptr=K_block_ptr,
                V_block_ptr=V_block_ptr,
                n_targets=n_targets if HAS_MULTIPLE_TARGETS else None,
                alpha=alpha,
                MAX_SEQ_LEN=MAX_SEQ_LEN,
                MAX_ATTN_LEN=MAX_ATTN_LEN,
                contextual_seq_len=contextual_seq_len,
                CAUSAL=CAUSAL,
                HAS_MULTIPLE_TARGETS=HAS_MULTIPLE_TARGETS,
                HAS_CONTEXTUAL_SEQ_LEN=HAS_CONTEXTUAL_SEQ_LEN,
                IS_DELTA_Q=IS_DELTA_Q,
                ALLOW_TF32=ALLOW_TF32,
                BLOCK_M=BLOCK_M,
                BLOCK_N=BLOCK_N,
            )
            K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))
            V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))

        if HAS_MULTIPLE_TARGETS and CAUSAL:
            # pyre-ignore[61]
            if uih_end < start_m:
                low_delta = start_m
                high_delta = start_m + BLOCK_M
                offset = (low_delta - uih_end).to(tl.int32)  # pyre-ignore [61]
                K_block_ptr = tl.advance(K_block_ptr, (0, offset))
                V_block_ptr = tl.advance(V_block_ptr, (offset, 0))
                for start_delta in tl.range(
                    low_delta, high_delta, BLOCK_N, num_stages=0
                ):
                    cur_offs_n = offs_n + start_delta
                    mask_n = cur_offs_n < seq_len
                    acc += _hstu_attn_fwd_one_block(
                        start_n=start_delta,
                        seq_len=seq_len,
                        offs_m=offs_m,
                        offs_n=cur_offs_n,
                        mask_m=mask_m,
                        mask_n=mask_n,
                        q=q,
                        K_block_ptr=K_block_ptr,
                        V_block_ptr=V_block_ptr,
                        n_targets=n_targets if HAS_MULTIPLE_TARGETS else None,
                        alpha=alpha,
                        MAX_SEQ_LEN=MAX_SEQ_LEN,
                        MAX_ATTN_LEN=MAX_ATTN_LEN,
                        contextual_seq_len=contextual_seq_len,
                        CAUSAL=CAUSAL,
                        HAS_MULTIPLE_TARGETS=HAS_MULTIPLE_TARGETS,
                        HAS_CONTEXTUAL_SEQ_LEN=HAS_CONTEXTUAL_SEQ_LEN,
                        IS_DELTA_Q=IS_DELTA_Q,
                        ALLOW_TF32=ALLOW_TF32,
                        BLOCK_M=BLOCK_M,
                        BLOCK_N=BLOCK_N,
                    )
                    K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))
                    V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))

        if IS_DELTA_Q:
            start_m_delta = pid * BLOCK_M
            offs_m_delta = start_m_delta + tl.arange(0, BLOCK_M)
            offs_v_d = tl.arange(0, BLOCK_D_V)
            off_o = Out + off_z * DeltaSize * stride_om + off_h * stride_oh
            out_ptrs = off_o + offs_m_delta[:, None] * stride_om + offs_v_d[None, :]
            tl.store(out_ptrs, acc, mask=(offs_m_delta < DeltaSize)[:, None])
        else:
            # rematerialize offsets to save registers
            start_m = pid * BLOCK_M
            offs_m = start_m + tl.arange(0, BLOCK_M)
            offs_v_d = tl.arange(0, BLOCK_D_V)
            off_o = Out + seq_start * stride_om + off_h * stride_oh
            out_ptrs = off_o + offs_m[:, None] * stride_om + offs_v_d[None, :]
            tl.store(out_ptrs, acc, mask=(offs_m < seq_len)[:, None])


@triton.autotune(
    configs=_get_fw_configs(),
    key=[
        "AUTOTUNE_Z",
        "H",
        "AUTOTUNE_MAX_SEQ_LEN",
        "DimQ",
        "DimV",
        "DeltaSize",
        "IS_DELTA_Q",
    ],
)
@triton.jit
def _hstu_attn_fwd(  # noqa C901
    Q,
    K,
    V,
    sort_by_length_indices,
    seq_offsets,
    delta_x_offsets,
    num_targets,
    Out,
    stride_qm,
    stride_qh,
    stride_kn,
    stride_kh,
    stride_vn,
    stride_vh,
    stride_om,
    stride_oh,
    alpha,
    Z,
    AUTOTUNE_Z,
    H,
    MAX_SEQ_LEN,
    AUTOTUNE_MAX_SEQ_LEN,  # Quantized MAX_SEQ_LEN used as an autotuning key
    DimQ,
    DimV,
    DeltaSize,
    contextual_seq_len,
    CAUSAL: tl.constexpr,
    HAS_MULTIPLE_TARGETS: tl.constexpr,
    IS_DELTA_Q: tl.constexpr,
    ALLOW_TF32: tl.constexpr,
    BLOCK_D_Q: tl.constexpr,
    BLOCK_D_V: tl.constexpr,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    MAX_ATTN_LEN: tl.constexpr,
    HAS_CONTEXTUAL_SEQ_LEN: tl.constexpr,
    HAS_SORT_BY_LENGTH_INDICES: tl.constexpr,
):
    off_hz = tl.program_id(1)
    off_z = off_hz // H
    if HAS_SORT_BY_LENGTH_INDICES:
        off_z = tl.load(sort_by_length_indices + off_z)
    off_h = off_hz % H
    pid = tl.program_id(0)
    _hstu_attn_fwd_compute(
        Q=Q,
        K=K,
        V=V,
        seq_offsets=seq_offsets,
        delta_x_offsets=delta_x_offsets,
        num_targets=num_targets,
        Out=Out,
        stride_qm=stride_qm,
        stride_qh=stride_qh,
        stride_kn=stride_kn,
        stride_kh=stride_kh,
        stride_vn=stride_vn,
        stride_vh=stride_vh,
        stride_om=stride_om,
        stride_oh=stride_oh,
        alpha=alpha,
        Z=Z,
        H=H,
        MAX_SEQ_LEN=MAX_SEQ_LEN,
        DimQ=DimQ,
        DimV=DimV,
        DeltaSize=DeltaSize,
        contextual_seq_len=contextual_seq_len,
        off_z=off_z,
        off_h=off_h,
        pid=pid,
        CAUSAL=CAUSAL,
        HAS_MULTIPLE_TARGETS=HAS_MULTIPLE_TARGETS,
        IS_DELTA_Q=IS_DELTA_Q,
        ALLOW_TF32=ALLOW_TF32,
        BLOCK_D_Q=BLOCK_D_Q,
        BLOCK_D_V=BLOCK_D_V,
        MAX_ATTN_LEN=MAX_ATTN_LEN,
        HAS_CONTEXTUAL_SEQ_LEN=HAS_CONTEXTUAL_SEQ_LEN,
        BLOCK_M=BLOCK_M,
        BLOCK_N=BLOCK_N,
    )


@triton.autotune(
    configs=_get_fw_configs(),
    key=[
        "AUTOTUNE_Z",
        "H",
        "AUTOTUNE_MAX_SEQ_LEN",
        "DimQ",
        "DimV",
        "DeltaSize",
        "IS_DELTA_Q",
    ],
)
@triton.jit
def _hstu_attn_fwd_persistent(  # noqa C901
    Q,
    K,
    V,
    sort_by_length_indices,
    seq_offsets,
    delta_x_offsets,
    num_targets,
    Out,
    stride_qm,
    stride_qh,
    stride_kn,
    stride_kh,
    stride_vn,
    stride_vh,
    stride_om,
    stride_oh,
    alpha,
    Z,
    AUTOTUNE_Z,
    H,
    MAX_SEQ_LEN,
    AUTOTUNE_MAX_SEQ_LEN,  # Quantized MAX_SEQ_LEN used as an autotuning key
    DimQ,
    DimV,
    DeltaSize,
    contextual_seq_len,
    CAUSAL: tl.constexpr,
    HAS_MULTIPLE_TARGETS: tl.constexpr,
    IS_DELTA_Q: tl.constexpr,
    ALLOW_TF32: tl.constexpr,
    BLOCK_D_Q: tl.constexpr,
    BLOCK_D_V: tl.constexpr,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    MAX_ATTN_LEN: tl.constexpr,
    HAS_CONTEXTUAL_SEQ_LEN: tl.constexpr,
    HAS_SORT_BY_LENGTH_INDICES: tl.constexpr,
):
    n_tile_num = tl.cdiv(MAX_SEQ_LEN, BLOCK_M)
    prog_id = tl.program_id(0)
    num_progs = tl.num_programs(0)

    total_tiles = n_tile_num * Z * H

    tiles_per_sm = total_tiles // num_progs
    if prog_id < total_tiles % num_progs:
        tiles_per_sm += 1

    tile_idx = prog_id
    for _ in range(0, tiles_per_sm):
        pid = (total_tiles - tile_idx - 1) // (Z * H)
        off_hz = (total_tiles - tile_idx - 1) % (Z * H)
        off_z = off_hz // H
        off_h = off_hz % H
        _hstu_attn_fwd_compute(
            Q=Q,
            K=K,
            V=V,
            seq_offsets=seq_offsets,
            delta_x_offsets=delta_x_offsets,
            num_targets=num_targets,
            Out=Out,
            stride_qm=stride_qm,
            stride_qh=stride_qh,
            stride_kn=stride_kn,
            stride_kh=stride_kh,
            stride_vn=stride_vn,
            stride_vh=stride_vh,
            stride_om=stride_om,
            stride_oh=stride_oh,
            alpha=alpha,
            Z=Z,
            H=H,
            MAX_SEQ_LEN=MAX_SEQ_LEN,
            DimQ=DimQ,
            DimV=DimV,
            DeltaSize=DeltaSize,
            contextual_seq_len=contextual_seq_len,
            off_z=off_z,
            off_h=off_h,
            pid=pid,
            CAUSAL=CAUSAL,
            HAS_MULTIPLE_TARGETS=HAS_MULTIPLE_TARGETS,
            IS_DELTA_Q=IS_DELTA_Q,
            ALLOW_TF32=ALLOW_TF32,
            BLOCK_D_Q=BLOCK_D_Q,
            BLOCK_D_V=BLOCK_D_V,
            MAX_ATTN_LEN=MAX_ATTN_LEN,
            HAS_CONTEXTUAL_SEQ_LEN=HAS_CONTEXTUAL_SEQ_LEN,
            BLOCK_M=BLOCK_M,
            BLOCK_N=BLOCK_N,
        )
        tile_idx += num_progs


def _get_named_specs() -> List[VersionedSpec]:
    s: int = 16
    CAUSAL: bool = True

    def _common_specs(dtype: str = "*bf16") -> NamedSpecType:
        return {
            "Q": (dtype, s),
            "K": (dtype, s),
            "V": (dtype, s),
            "seq_offsets": ("*i64", s),
            "Out": (dtype, s),
            "stride_qm": ("i32", s),
            "stride_qh": ("i32", s),
            "stride_kn": ("i32", s),
            "stride_kh": ("i32", s),
            "stride_vn": ("i32", s),
            "stride_vh": ("i32", s),
            "stride_om": ("i32", s),
            "stride_oh": ("i32", s),
            "alpha": "fp32",
            "contextual_seq_len": "i32",
            "Z": "i32",
            "AUTOTUNE_Z": "i32",
            "H": "i32",
            "MAX_SEQ_LEN": "i32",
            "AUTOTUNE_MAX_SEQ_LEN": "i32",
            "DimQ": "i32",
            "DimV": "i32",
            "DeltaSize": "i32",
            "sort_by_length_indices": ("*i64", s, False),
            "CAUSAL": CAUSAL,
            "BLOCK_M": -1,  # autotuned
            "BLOCK_N": -1,  # autotuned
            "MAX_ATTN_LEN": 0,
            "HAS_SORT_BY_LENGTH_INDICES": False,
        }

    default_values = {
        "MAX_ATTN_LEN": 0,
        "HAS_CONTEXTUAL_SEQ_LEN": 0,
        "HAS_SORT_BY_LENGTH_INDICES": 0,
    }

    return (
        [
            VersionedSpec(
                spec={
                    "delta_x_offsets": ("*i64", s, False),
                    "num_targets": ("*i64", s, False),
                    "HAS_MULTIPLE_TARGETS": False,
                    "IS_DELTA_Q": False,
                    "BLOCK_D_Q": block_dq,
                    "BLOCK_D_V": block_dv,
                    "ALLOW_TF32": True,
                    "HAS_CONTEXTUAL_SEQ_LEN": has_contextual_seq_len,
                    **_common_specs(dtype=dtype),
                },
                default_values=default_values,
            )
            for dtype in ["*bf16", "*fp16"]
            for block_dq, block_dv in [(128, 128), (32, 64)]
            for has_contextual_seq_len in [True, False]
        ]
        + [
            VersionedSpec(
                spec={
                    "delta_x_offsets": ("*i64", s, is_delta_q),
                    "num_targets": ("*i64", s, True),
                    "HAS_MULTIPLE_TARGETS": True,
                    "IS_DELTA_Q": is_delta_q,
                    "BLOCK_D_Q": block,
                    "BLOCK_D_V": block,
                    "ALLOW_TF32": True,
                    "HAS_CONTEXTUAL_SEQ_LEN": False,
                    **_common_specs(dtype=dtype),
                },
                default_values=default_values,
            )
            for dtype in ["*bf16", "*fp16"]
            for block in [64, 128]
            for is_delta_q in [True, False]
        ]
        + [
            VersionedSpec(
                spec={
                    "delta_x_offsets": ("*i64", s, is_delta_q),
                    "num_targets": ("*i64", s, True),
                    "HAS_MULTIPLE_TARGETS": True,
                    "IS_DELTA_Q": is_delta_q,
                    "BLOCK_D_Q": block,
                    "BLOCK_D_V": block,
                    "ALLOW_TF32": True,
                    "HAS_CONTEXTUAL_SEQ_LEN": False,
                    **_common_specs(dtype=dtype),
                },
                default_values=default_values,
            )
            for dtype in ["*bf16", "*fp16"]
            for block in [64, 128]
            for is_delta_q in [True, False]
        ]
    )


_hstu_attn_fwd = register_tritoncc_specs(
    func=_hstu_attn_fwd, versioned_specs=_get_named_specs()
)
_hstu_attn_fwd = triton_autotune(
    configs=_get_fw_configs(),
    key=[
        "AUTOTUNE_Z",
        "H",
        "AUTOTUNE_MAX_SEQ_LEN",
        "DimQ",
        "DimV",
        "DeltaSize",
        "IS_DELTA_Q",
    ],
)(_hstu_attn_fwd.fn)

_hstu_attn_fwd_persistent = register_tritoncc_specs(
    func=_hstu_attn_fwd_persistent, versioned_specs=_get_named_specs()
)
_hstu_attn_fwd_persistent = triton_autotune(
    configs=_get_fw_configs(),
    key=[
        "AUTOTUNE_Z",
        "H",
        "AUTOTUNE_MAX_SEQ_LEN",
        "DimQ",
        "DimV",
        "DeltaSize",
        "IS_DELTA_Q",
    ],
)(_hstu_attn_fwd_persistent.fn)


@triton.jit
def _hstu_attn_bwd_one_block(  # noqa C901
    start_m,
    offs_n,
    offs_m,
    q_ptrs_trans,
    dq_ptrs_trans,
    mask_n,
    do_ptrs,
    dk,
    dv,
    k,
    v,
    pos_offs_n,
    seq_len,
    n_targets,
    max_ids,
    contextual_seq_len,
    LOCK,
    stride_qm,
    stride_dom,
    stride_dqm,
    alpha,
    MAX_SEQ_LEN,
    MAX_ATTN_LEN: tl.constexpr,
    CAUSAL: tl.constexpr,
    HAS_MULTIPLE_TARGETS: tl.constexpr,
    HAS_CONTEXTUAL_SEQ_LEN: tl.constexpr,
    ALLOW_TF32: tl.constexpr,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    ATOMIC_ADD: tl.constexpr,
):
    pos_offs_m = offs_m + start_m
    mask_m = pos_offs_m < seq_len
    invalid_mask_trans = pos_offs_m[None, :] == offs_n[:, None]
    # recompute qk and silu
    if HAS_CONTEXTUAL_SEQ_LEN:
        pos_offs_m = pos_offs_m - contextual_seq_len + 1
        pos_offs_m = tl.where(
            pos_offs_m > 0,
            pos_offs_m,
            0,
        )
    if HAS_MULTIPLE_TARGETS:
        pos_offs_m = tl.where(
            pos_offs_m < max_ids,
            pos_offs_m,
            max_ids,
        )
    q_trans = tl.load(
        q_ptrs_trans + start_m * stride_qm,
        mask=mask_m[None, :],
        other=0.0,
    )
    qk_trans = tl.dot(k, q_trans, allow_tf32=ALLOW_TF32) * alpha
    # pyre-fixme[16]: Module `math` has no attribute `fast_dividef`.
    sig_trans = fast_dividef(1.0, 1.0 + tl.exp(-qk_trans))
    silu_trans = qk_trans * sig_trans * (1.0 / MAX_SEQ_LEN)
    if MAX_ATTN_LEN > 0:
        if CAUSAL:
            invalid_mask_trans = invalid_mask_trans or (
                pos_offs_m[None, :] > pos_offs_n[:, None]
                and pos_offs_n[:, None] - pos_offs_m[None, :] >= -MAX_ATTN_LEN
            )
    else:
        if CAUSAL:
            invalid_mask_trans = (
                invalid_mask_trans or pos_offs_m[None, :] > pos_offs_n[:, None]
            )
    if HAS_CONTEXTUAL_SEQ_LEN:
        invalid_mask_trans = invalid_mask_trans or (
            pos_offs_m[None, :] == 0 and pos_offs_n[:, None] < max_ids
        )
    silu_trans = tl.where(invalid_mask_trans, silu_trans, 0)
    silu_trans = silu_trans.to(k.dtype)
    # compute dv
    do = tl.load(
        do_ptrs + start_m * stride_dom,
        mask=mask_m[:, None],
        other=0.0,
    )
    dv += tl.dot(silu_trans, do, allow_tf32=ALLOW_TF32)

    # compute dk and dq
    dqk_trans = tl.dot(v, tl.trans(do), allow_tf32=ALLOW_TF32)
    dqk_trans = (
        dqk_trans * sig_trans * (1 + qk_trans * (1 - sig_trans)) * (1.0 / MAX_SEQ_LEN)
    )
    dqk_trans = tl.where(invalid_mask_trans, dqk_trans, 0)
    dqk_trans = dqk_trans.to(k.dtype)

    # Note: the factor `alpha` is delayed until the end of the function to reduce the cost
    dk += tl.dot(dqk_trans, tl.trans(q_trans), allow_tf32=ALLOW_TF32)
    if ATOMIC_ADD:
        lock_id = start_m // BLOCK_M
        stride_lock = tl.cdiv(MAX_SEQ_LEN, BLOCK_M)
        lock = LOCK + tl.program_id(0) * stride_lock + lock_id
        tl.debug_barrier()  # add a barrier to force sync
        while tl.atomic_cas(lock, 0, 1) == 1:
            pass
    dq_trans = tl.load(
        dq_ptrs_trans + start_m * stride_dqm,
        mask=mask_m[None, :],
        other=0.0,
        eviction_policy="evict_last",
    )
    dq_trans += tl.dot(tl.trans(k), dqk_trans, allow_tf32=ALLOW_TF32) * alpha
    dq_trans = dq_trans.to(k.dtype)
    tl.store(
        dq_ptrs_trans + start_m * stride_dqm,
        dq_trans,
        mask=mask_m[None, :],
        eviction_policy="evict_last",
    )
    if ATOMIC_ADD:
        tl.atomic_xchg(lock, 0)  # pyre-ignore [61]
    return dk, dv


@triton.jit
def _hstu_attn_bwd_one_col_block(  # noqa C901
    start_n,
    seq_len,
    n_targets,
    contextual_seq_len,
    Q,
    K,
    V,
    DOut,
    DQ,
    DK,
    DV,
    LOCK,
    stride_qm,
    stride_kn,
    stride_vn,
    stride_dom,
    stride_dqm,
    stride_dkn,
    stride_dvn,
    alpha,
    MAX_SEQ_LEN,
    MAX_ATTN_LEN: tl.constexpr,
    CAUSAL: tl.constexpr,
    HAS_MULTIPLE_TARGETS: tl.constexpr,
    HAS_CONTEXTUAL_SEQ_LEN: tl.constexpr,
    ALLOW_TF32: tl.constexpr,
    BLOCK_D_Q: tl.constexpr,
    BLOCK_D_V: tl.constexpr,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    UNROLL: tl.constexpr,
    ATOMIC_ADD: tl.constexpr,
):
    # Work on the subsequence dv[start_n, start_n + BLOCK_N, :]
    if CAUSAL:
        if HAS_MULTIPLE_TARGETS:
            low = start_n
            if MAX_ATTN_LEN > 0:
                high = start_n + MAX_ATTN_LEN + BLOCK_N
                high = high if high + n_targets < seq_len else seq_len
            else:
                high = seq_len
        else:
            low = start_n
            if MAX_ATTN_LEN > 0:
                high = start_n + MAX_ATTN_LEN + BLOCK_N
                high = high if high < seq_len else seq_len
            else:
                high = seq_len
        if HAS_CONTEXTUAL_SEQ_LEN:
            contextual_block_end = tl.cdiv(contextual_seq_len, BLOCK_M) * BLOCK_M
            if low < contextual_block_end:
                low = contextual_block_end
    else:
        low = 0
        high = start_n + BLOCK_N

    # initialize row/col offsets
    offs_m = tl.arange(0, BLOCK_M)
    offs_qk_d = tl.arange(0, BLOCK_D_Q)
    offs_v_d = tl.arange(0, BLOCK_D_V)
    offs_n = start_n + tl.arange(0, BLOCK_N)

    # initialize pointers to value-like data
    q_ptrs_trans = Q + (offs_m[None, :] * stride_qm + offs_qk_d[:, None])
    dq_ptrs_trans = DQ + (offs_m[None, :] * stride_dqm + offs_qk_d[:, None])
    k_ptrs = K + (offs_n[:, None] * stride_kn + offs_qk_d[None, :])
    v_ptrs = V + (offs_n[:, None] * stride_vn + offs_v_d[None, :])
    mask_n = offs_n < seq_len

    do_ptrs = DOut + (offs_m[:, None] * stride_dom + offs_v_d[None, :])
    # initialize dv and dk
    dv = tl.zeros([BLOCK_N, BLOCK_D_V], dtype=tl.float32)
    dk = tl.zeros([BLOCK_N, BLOCK_D_Q], dtype=tl.float32)
    # k and v stay in SRAM throughout
    k = tl.load(k_ptrs, mask=mask_n[:, None], other=0.0)
    v = tl.load(v_ptrs, mask=mask_n[:, None], other=0.0)
    max_ids = seq_len
    if HAS_CONTEXTUAL_SEQ_LEN:
        pos_offs_n = offs_n - contextual_seq_len + 1
        pos_offs_n = tl.where(
            pos_offs_n > 0,
            pos_offs_n,
            0,
        )
        max_ids = max_ids - contextual_seq_len + 1
    else:
        pos_offs_n = offs_n
    if HAS_MULTIPLE_TARGETS:
        max_ids = max_ids - n_targets
        pos_offs_n = tl.where(
            offs_n < max_ids,
            offs_n,
            max_ids,
        )
    # loop over rows
    if HAS_CONTEXTUAL_SEQ_LEN and CAUSAL:
        for start_m in range(0, contextual_seq_len, BLOCK_M):
            start_m = tl.multiple_of(start_m, BLOCK_M)
            dk, dv = _hstu_attn_bwd_one_block(
                start_m=start_m,
                offs_n=offs_n,
                offs_m=offs_m,
                q_ptrs_trans=q_ptrs_trans,
                dq_ptrs_trans=dq_ptrs_trans,
                mask_n=mask_n,
                do_ptrs=do_ptrs,
                dk=dk,
                dv=dv,
                k=k,
                v=v,
                pos_offs_n=pos_offs_n,
                seq_len=seq_len,
                n_targets=n_targets,
                max_ids=max_ids,
                contextual_seq_len=contextual_seq_len,
                LOCK=LOCK,
                stride_qm=stride_qm,
                stride_dom=stride_dom,
                stride_dqm=stride_dqm,
                alpha=alpha,
                MAX_SEQ_LEN=MAX_SEQ_LEN,
                MAX_ATTN_LEN=MAX_ATTN_LEN,
                CAUSAL=CAUSAL,
                HAS_MULTIPLE_TARGETS=HAS_MULTIPLE_TARGETS,
                HAS_CONTEXTUAL_SEQ_LEN=HAS_CONTEXTUAL_SEQ_LEN,
                ALLOW_TF32=ALLOW_TF32,
                BLOCK_M=BLOCK_M,
                BLOCK_N=BLOCK_N,
                ATOMIC_ADD=ATOMIC_ADD,
            )
    for start_m in tl.range(low, high, BLOCK_M, loop_unroll_factor=UNROLL):
        start_m = tl.multiple_of(start_m, BLOCK_M)
        dk, dv = _hstu_attn_bwd_one_block(
            start_m=start_m,
            offs_n=offs_n,
            offs_m=offs_m,
            q_ptrs_trans=q_ptrs_trans,
            dq_ptrs_trans=dq_ptrs_trans,
            mask_n=mask_n,
            do_ptrs=do_ptrs,
            dk=dk,
            dv=dv,
            k=k,
            v=v,
            pos_offs_n=pos_offs_n,
            seq_len=seq_len,
            n_targets=n_targets,
            max_ids=max_ids,
            contextual_seq_len=contextual_seq_len,
            LOCK=LOCK,
            stride_qm=stride_qm,
            stride_dom=stride_dom,
            stride_dqm=stride_dqm,
            alpha=alpha,
            MAX_SEQ_LEN=MAX_SEQ_LEN,
            MAX_ATTN_LEN=MAX_ATTN_LEN,
            CAUSAL=CAUSAL,
            HAS_MULTIPLE_TARGETS=HAS_MULTIPLE_TARGETS,
            HAS_CONTEXTUAL_SEQ_LEN=HAS_CONTEXTUAL_SEQ_LEN,
            ALLOW_TF32=ALLOW_TF32,
            BLOCK_M=BLOCK_M,
            BLOCK_N=BLOCK_N,
            ATOMIC_ADD=ATOMIC_ADD,
        )
    # write-back
    dv_ptrs = DV + (offs_n[:, None] * stride_dvn + offs_v_d[None, :])
    dk_ptrs = DK + (offs_n[:, None] * stride_dkn + offs_qk_d[None, :])
    dk = dk * alpha
    tl.store(dv_ptrs, dv.to(k.dtype), mask=mask_n[:, None])
    tl.store(dk_ptrs, dk.to(k.dtype), mask=mask_n[:, None])


def _bwd_pre_hook(nargs):
    nargs["DQ"].zero_()
    if nargs["SEQUENCE_PARALLEL"] is True:
        nargs["LOCK"].zero_()


def _get_bw_configs() -> List[triton.Config]:
    if torch.version.hip:
        configs = []
        for BLOCK_M in [32, 64]:
            for BLOCK_N in [32, 64]:
                for num_stages in [1, 2]:
                    for num_warps in [4, 8]:
                        for matrix_instr_nonkdim in [16, 32]:
                            for waves_per_eu in [0, 2, 4]:
                                for sp in [True, False]:
                                    configs.append(
                                        triton.Config(
                                            {
                                                "BLOCK_M": BLOCK_M,
                                                "BLOCK_N": BLOCK_N,
                                                "matrix_instr_nonkdim": matrix_instr_nonkdim,
                                                "waves_per_eu": waves_per_eu,
                                                "SEQUENCE_PARALLEL": sp,
                                                "UNROLL": 1,
                                            },
                                            num_stages=num_stages,
                                            num_warps=num_warps,
                                            pre_hook=_bwd_pre_hook,
                                        )
                                    )
        return configs

    configs = [
        triton.Config(
            {"BLOCK_M": 16, "BLOCK_N": 32, "SEQUENCE_PARALLEL": False, "UNROLL": 1},
            num_stages=2,
            num_warps=2,
            pre_hook=_bwd_pre_hook,
        ),
        triton.Config(
            {"BLOCK_M": 16, "BLOCK_N": 16, "SEQUENCE_PARALLEL": False, "UNROLL": 1},
            num_stages=2,
            num_warps=2,
            pre_hook=_bwd_pre_hook,
        ),
        triton.Config(
            {"BLOCK_M": 16, "BLOCK_N": 32, "SEQUENCE_PARALLEL": False, "UNROLL": 1},
            num_stages=2,
            num_warps=4,
            pre_hook=_bwd_pre_hook,
        ),
        triton.Config(
            {"BLOCK_M": 16, "BLOCK_N": 32, "SEQUENCE_PARALLEL": False, "UNROLL": 1},
            num_stages=1,
            num_warps=8,
            pre_hook=_bwd_pre_hook,
        ),
        triton.Config(
            {"BLOCK_M": 16, "BLOCK_N": 64, "SEQUENCE_PARALLEL": False, "UNROLL": 1},
            num_stages=1,
            num_warps=4,
            pre_hook=_bwd_pre_hook,
        ),
        triton.Config(
            {"BLOCK_M": 32, "BLOCK_N": 32, "SEQUENCE_PARALLEL": False, "UNROLL": 1},
            num_stages=1,
            num_warps=4,
            pre_hook=_bwd_pre_hook,
        ),
        triton.Config(
            {"BLOCK_M": 32, "BLOCK_N": 32, "SEQUENCE_PARALLEL": False, "UNROLL": 1},
            num_stages=2,
            num_warps=4,
            pre_hook=_bwd_pre_hook,
        ),
        triton.Config(
            {"BLOCK_M": 32, "BLOCK_N": 64, "SEQUENCE_PARALLEL": False, "UNROLL": 1},
            num_stages=1,
            num_warps=4,
            pre_hook=_bwd_pre_hook,
        ),
        triton.Config(
            {"BLOCK_M": 32, "BLOCK_N": 64, "SEQUENCE_PARALLEL": False, "UNROLL": 1},
            num_stages=2,
            num_warps=4,
            pre_hook=_bwd_pre_hook,
        ),
        triton.Config(
            {"BLOCK_M": 32, "BLOCK_N": 64, "SEQUENCE_PARALLEL": False, "UNROLL": 1},
            num_stages=1,
            num_warps=8,
            pre_hook=_bwd_pre_hook,
        ),
        triton.Config(
            {"BLOCK_M": 32, "BLOCK_N": 64, "SEQUENCE_PARALLEL": False, "UNROLL": 1},
            num_stages=2,
            num_warps=8,
            pre_hook=_bwd_pre_hook,
        ),
        triton.Config(
            {"BLOCK_M": 64, "BLOCK_N": 64, "SEQUENCE_PARALLEL": False, "UNROLL": 1},
            num_stages=1,
            num_warps=4,
            pre_hook=_bwd_pre_hook,
        ),
        triton.Config(
            {"BLOCK_M": 64, "BLOCK_N": 64, "SEQUENCE_PARALLEL": False, "UNROLL": 1},
            num_stages=2,
            num_warps=4,
            pre_hook=_bwd_pre_hook,
        ),
        triton.Config(
            {"BLOCK_M": 64, "BLOCK_N": 64, "SEQUENCE_PARALLEL": False, "UNROLL": 1},
            num_stages=1,
            num_warps=8,
            pre_hook=_bwd_pre_hook,
        ),
        triton.Config(
            {"BLOCK_M": 64, "BLOCK_N": 64, "SEQUENCE_PARALLEL": False, "UNROLL": 1},
            num_stages=2,
            num_warps=8,
            pre_hook=_bwd_pre_hook,
        ),
        triton.Config(
            {"BLOCK_M": 32, "BLOCK_N": 128, "SEQUENCE_PARALLEL": False, "UNROLL": 1},
            num_stages=2,
            num_warps=8,
            pre_hook=_bwd_pre_hook,
        ),
        triton.Config(
            {"BLOCK_M": 32, "BLOCK_N": 128, "SEQUENCE_PARALLEL": False, "UNROLL": 1},
            num_stages=3,
            num_warps=8,
            pre_hook=_bwd_pre_hook,
        ),
        triton.Config(
            {"BLOCK_M": 32, "BLOCK_N": 128, "SEQUENCE_PARALLEL": False, "UNROLL": 2},
            num_stages=2,
            num_warps=8,
            pre_hook=_bwd_pre_hook,
        ),
        triton.Config(
            {"BLOCK_M": 32, "BLOCK_N": 128, "SEQUENCE_PARALLEL": False, "UNROLL": 4},
            num_stages=2,
            num_warps=8,
            pre_hook=_bwd_pre_hook,
        ),
        triton.Config(
            {"BLOCK_M": 16, "BLOCK_N": 32, "SEQUENCE_PARALLEL": True, "UNROLL": 1},
            num_stages=2,
            num_warps=2,
            pre_hook=_bwd_pre_hook,
        ),
        triton.Config(
            {"BLOCK_M": 32, "BLOCK_N": 32, "SEQUENCE_PARALLEL": True, "UNROLL": 1},
            num_stages=1,
            num_warps=4,
            pre_hook=_bwd_pre_hook,
        ),
        triton.Config(
            {"BLOCK_M": 32, "BLOCK_N": 32, "SEQUENCE_PARALLEL": True, "UNROLL": 1},
            num_stages=2,
            num_warps=4,
            pre_hook=_bwd_pre_hook,
        ),
        triton.Config(
            {"BLOCK_M": 32, "BLOCK_N": 64, "SEQUENCE_PARALLEL": True, "UNROLL": 1},
            num_stages=1,
            num_warps=4,
            pre_hook=_bwd_pre_hook,
        ),
        triton.Config(
            {"BLOCK_M": 32, "BLOCK_N": 64, "SEQUENCE_PARALLEL": True, "UNROLL": 1},
            num_stages=2,
            num_warps=4,
            pre_hook=_bwd_pre_hook,
        ),
        triton.Config(
            {"BLOCK_M": 32, "BLOCK_N": 64, "SEQUENCE_PARALLEL": True, "UNROLL": 1},
            num_stages=1,
            num_warps=8,
            pre_hook=_bwd_pre_hook,
        ),
        triton.Config(
            {"BLOCK_M": 64, "BLOCK_N": 64, "SEQUENCE_PARALLEL": True, "UNROLL": 1},
            num_stages=1,
            num_warps=4,
            pre_hook=_bwd_pre_hook,
        ),
        triton.Config(
            {"BLOCK_M": 64, "BLOCK_N": 64, "SEQUENCE_PARALLEL": True, "UNROLL": 1},
            num_stages=2,
            num_warps=4,
            pre_hook=_bwd_pre_hook,
        ),
        triton.Config(
            {"BLOCK_M": 32, "BLOCK_N": 128, "SEQUENCE_PARALLEL": True, "UNROLL": 1},
            num_stages=3,
            num_warps=8,
            pre_hook=_bwd_pre_hook,
        ),
    ]
    return configs


@triton_autotune(
    configs=_get_bw_configs(),
    key=[
        "AUTOTUNE_Z",
        "H",
        "AUTOTUNE_MAX_SEQ_LEN",
        "DimQ",
        "DimV",
    ],
)
@triton.jit
def _hstu_attn_bwd(  # noqa C901
    Q,
    K,
    V,
    sort_by_length_indices,
    seq_offsets,
    num_targets,
    DOut,
    DQ,
    DK,
    DV,
    LOCK,
    stride_qm,
    stride_qh,
    stride_kn,
    stride_kh,
    stride_vn,
    stride_vh,
    stride_dom,
    stride_doh,
    stride_dqm,
    stride_dqh,
    stride_dkn,
    stride_dkh,
    stride_dvn,
    stride_dvh,
    alpha,
    contextual_seq_len,
    Z,
    AUTOTUNE_Z,
    H,
    MAX_SEQ_LEN,
    AUTOTUNE_MAX_SEQ_LEN,  # Quantized MAX_SEQ_LEN used as an autotuning key
    DimQ,
    DimV,
    MAX_ATTN_LEN: tl.constexpr,
    CAUSAL: tl.constexpr,
    HAS_MULTIPLE_TARGETS: tl.constexpr,
    HAS_CONTEXTUAL_SEQ_LEN: tl.constexpr,
    ALLOW_TF32: tl.constexpr,
    BLOCK_D_Q: tl.constexpr,
    BLOCK_D_V: tl.constexpr,
    SEQUENCE_PARALLEL: tl.constexpr,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    UNROLL: tl.constexpr,
    HAS_SORT_BY_LENGTH_INDICES: tl.constexpr,
):
    off_hz = tl.program_id(0)
    off_z = off_hz // H
    if HAS_SORT_BY_LENGTH_INDICES:
        off_z = tl.load(sort_by_length_indices + off_z)
    off_h = off_hz % H
    off_h = off_h.to(tl.int64)
    seq_start = tl.load(seq_offsets + off_z).to(tl.int64)
    seq_end = tl.load(seq_offsets + off_z + 1)
    seq_len = (seq_end - seq_start).to(tl.int32)
    if HAS_MULTIPLE_TARGETS:
        n_targets = tl.load(num_targets + off_z).to(tl.int32)
    else:
        n_targets = None
    # offset pointers for batch/head
    Q = Q + seq_start * stride_qm + off_h * stride_qh
    K = K + seq_start * stride_kn + off_h * stride_kh
    V = V + seq_start * stride_vn + off_h * stride_vh
    DOut = DOut + seq_start * stride_dom + off_h * stride_doh
    DQ = DQ + seq_start * stride_dqm + off_h * stride_dqh
    DK = DK + seq_start * stride_dkn + off_h * stride_dkh
    DV = DV + seq_start * stride_dvn + off_h * stride_dvh
    if SEQUENCE_PARALLEL:
        start_n = tl.program_id(1) * BLOCK_N
        if start_n >= seq_len:
            return
        _hstu_attn_bwd_one_col_block(
            start_n=start_n,
            seq_len=seq_len,
            n_targets=n_targets,
            contextual_seq_len=contextual_seq_len,
            Q=Q,
            K=K,
            V=V,
            DOut=DOut,
            DQ=DQ,
            DK=DK,
            DV=DV,
            LOCK=LOCK,
            stride_qm=stride_qm,
            stride_kn=stride_kn,
            stride_vn=stride_vn,
            stride_dom=stride_dom,
            stride_dqm=stride_dqm,
            stride_dkn=stride_dkn,
            stride_dvn=stride_dvn,
            alpha=alpha,
            MAX_SEQ_LEN=MAX_SEQ_LEN,
            MAX_ATTN_LEN=MAX_ATTN_LEN,
            CAUSAL=CAUSAL,
            HAS_MULTIPLE_TARGETS=HAS_MULTIPLE_TARGETS,
            HAS_CONTEXTUAL_SEQ_LEN=HAS_CONTEXTUAL_SEQ_LEN,
            ALLOW_TF32=ALLOW_TF32,
            BLOCK_D_Q=BLOCK_D_Q,
            BLOCK_D_V=BLOCK_D_V,
            BLOCK_M=BLOCK_M,
            BLOCK_N=BLOCK_N,
            UNROLL=UNROLL,
            ATOMIC_ADD=True,
        )
    else:
        for start_n in range(0, seq_len, BLOCK_N):
            _hstu_attn_bwd_one_col_block(
                start_n=start_n,
                seq_len=seq_len,
                n_targets=n_targets,
                contextual_seq_len=contextual_seq_len,
                Q=Q,
                K=K,
                V=V,
                DOut=DOut,
                DQ=DQ,
                DK=DK,
                DV=DV,
                LOCK=LOCK,
                stride_qm=stride_qm,
                stride_kn=stride_kn,
                stride_vn=stride_vn,
                stride_dom=stride_dom,
                stride_dqm=stride_dqm,
                stride_dkn=stride_dkn,
                stride_dvn=stride_dvn,
                alpha=alpha,
                MAX_SEQ_LEN=MAX_SEQ_LEN,
                MAX_ATTN_LEN=MAX_ATTN_LEN,
                CAUSAL=CAUSAL,
                HAS_MULTIPLE_TARGETS=HAS_MULTIPLE_TARGETS,
                HAS_CONTEXTUAL_SEQ_LEN=HAS_CONTEXTUAL_SEQ_LEN,
                ALLOW_TF32=ALLOW_TF32,
                BLOCK_D_Q=BLOCK_D_Q,
                BLOCK_D_V=BLOCK_D_V,
                BLOCK_M=BLOCK_M,
                BLOCK_N=BLOCK_N,
                UNROLL=UNROLL,
                ATOMIC_ADD=False,
            )


def triton_hstu_attention_fwd(
    N: int,
    alpha: float,
    q: torch.Tensor,
    k: torch.Tensor,
    v: torch.Tensor,
    seq_offsets: torch.Tensor,
    causal: bool,
    num_targets: Optional[torch.Tensor],
    max_attn_len: Optional[int],
    contextual_seq_len: int,
    sort_by_length_indices: Optional[torch.Tensor],
) -> torch.Tensor:
    Z = seq_offsets.numel() - 1
    AUTOTUNE_Z = prev_power_of_2(Z)
    L, H, DimQ = q.shape
    _, _, DimV = v.shape
    out = torch.empty_like(v)
    max_attn_len = max_attn_len or 0
    has_multiple_targets = num_targets is not None
    has_contextual_seq_len = contextual_seq_len > 0
    has_sort_by_length_indices = sort_by_length_indices is not None
    if L == 0:
        return out

    grid = lambda meta: (  # noqa E731
        triton.cdiv(N, meta["BLOCK_M"]),
        Z * H,
    )

    _hstu_attn_fwd[grid](
        Q=q,
        K=k,
        V=v,
        sort_by_length_indices=sort_by_length_indices,
        seq_offsets=seq_offsets,
        delta_x_offsets=None,
        num_targets=num_targets,
        Out=out,
        stride_qm=q.stride(0),
        stride_qh=q.stride(1),
        stride_kn=k.stride(0),
        stride_kh=k.stride(1),
        stride_vn=v.stride(0),
        stride_vh=v.stride(1),
        stride_om=out.stride(0),
        stride_oh=out.stride(1),
        alpha=alpha,
        Z=Z,
        AUTOTUNE_Z=AUTOTUNE_Z,
        H=H,
        MAX_SEQ_LEN=N,
        AUTOTUNE_MAX_SEQ_LEN=autotune_max_seq_len(N),
        DimQ=DimQ,
        DimV=DimV,
        DeltaSize=0,
        contextual_seq_len=contextual_seq_len,
        CAUSAL=causal,
        HAS_MULTIPLE_TARGETS=has_multiple_targets,
        IS_DELTA_Q=False,
        ALLOW_TF32=torch.backends.cuda.matmul.allow_tf32,
        BLOCK_D_Q=DimQ,
        BLOCK_D_V=DimV,
        MAX_ATTN_LEN=max_attn_len,
        HAS_CONTEXTUAL_SEQ_LEN=has_contextual_seq_len,
        HAS_SORT_BY_LENGTH_INDICES=has_sort_by_length_indices,
    )
    return out


def triton_hstu_attention_bwd(
    dout: torch.Tensor,
    q: torch.Tensor,
    k: torch.Tensor,
    v: torch.Tensor,
    dq: torch.Tensor,
    dk: torch.Tensor,
    dv: torch.Tensor,
    seq_offsets: torch.Tensor,
    num_targets: Optional[torch.Tensor],
    N: int,
    alpha: float,
    max_attn_len: int,
    causal: float,
    contextual_seq_len: int,
    sort_by_length_indices: Optional[torch.Tensor],
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    dout = switch_to_contiguous_if_needed(dout)
    dq = switch_to_contiguous_if_needed(dq)
    dk = switch_to_contiguous_if_needed(dk)
    dv = switch_to_contiguous_if_needed(dv)
    if dout.shape[0] == 0:
        return torch.zeros_like(q), torch.zeros_like(k), torch.zeros_like(v)
    Z = seq_offsets.numel() - 1
    _, H, DimQ = q.shape
    _, _, DimV = v.shape
    grid = lambda meta: (  # noqa E731
        Z * H,
        (triton.cdiv(N, meta["BLOCK_N"]) if meta["SEQUENCE_PARALLEL"] else 1),
    )
    # The minimum size of BLOCK_M used in `_get_bw_configs`.
    # TODO (linjianma): avoid hardcoding the value.
    MIN_BLOCK_M = 16
    lock = torch.empty(
        (Z * H, triton.cdiv(N, MIN_BLOCK_M)),
        dtype=torch.int32,
        device=q.device,
    )
    AUTOTUNE_Z = prev_power_of_2(Z)
    _hstu_attn_bwd[grid](
        Q=q,
        K=k,
        V=v,
        sort_by_length_indices=sort_by_length_indices,
        seq_offsets=seq_offsets,
        num_targets=num_targets,
        DOut=dout,
        DQ=dq,
        DK=dk,
        DV=dv,
        LOCK=lock,
        stride_qm=q.stride(0),
        stride_qh=q.stride(1),
        stride_kn=k.stride(0),
        stride_kh=k.stride(1),
        stride_vn=v.stride(0),
        stride_vh=v.stride(1),
        stride_dom=dout.stride(0),
        stride_doh=dout.stride(1),
        stride_dqm=dq.stride(0),
        stride_dqh=dq.stride(1),
        stride_dkn=dk.stride(0),
        stride_dkh=dk.stride(1),
        stride_dvn=dv.stride(0),
        stride_dvh=dv.stride(1),
        alpha=alpha,
        contextual_seq_len=contextual_seq_len,
        Z=Z,
        AUTOTUNE_Z=AUTOTUNE_Z,
        H=H,
        MAX_SEQ_LEN=N,
        AUTOTUNE_MAX_SEQ_LEN=autotune_max_seq_len(N),
        DimQ=DimQ,
        DimV=DimV,
        MAX_ATTN_LEN=max_attn_len,
        CAUSAL=causal,
        HAS_MULTIPLE_TARGETS=num_targets is not None,
        HAS_CONTEXTUAL_SEQ_LEN=contextual_seq_len > 0,
        ALLOW_TF32=torch.backends.cuda.matmul.allow_tf32,
        BLOCK_D_Q=DimQ,
        BLOCK_D_V=DimV,
        HAS_SORT_BY_LENGTH_INDICES=sort_by_length_indices is not None,
    )

    return dq, dk, dv


class _AttentionFunction(torch.autograd.Function):
    @staticmethod
    # pyre-ignore[14]
    def forward(
        ctx,
        N: int,
        alpha: float,
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        seq_offsets: torch.Tensor,
        causal: bool,
        num_targets: Optional[torch.Tensor],
        max_attn_len: Optional[int],
        contextual_seq_len: int,
        sort_by_length: bool,
    ) -> torch.Tensor:
        sort_by_length_indices = None
        if sort_by_length:
            seq_lengths = seq_offsets[1:] - seq_offsets[:-1]
            _, sort_by_length_indices = torch.sort(
                seq_lengths, descending=True, stable=False
            )
        saved_tensors = [q, k, v, seq_offsets]
        if num_targets is not None:
            saved_tensors.append(num_targets)
        max_attn_len = max_attn_len or 0
        if sort_by_length_indices is not None:
            saved_tensors.append(sort_by_length_indices)
        ctx.save_for_backward(*saved_tensors)
        ctx.alpha = alpha
        ctx.causal = causal
        ctx.has_multiple_targets = num_targets is not None
        ctx.max_attn_len = max_attn_len
        ctx.N = N
        ctx.contextual_seq_len = contextual_seq_len
        ctx.sort_by_length = sort_by_length
        return triton_hstu_attention_fwd(
            N=N,
            alpha=alpha,
            q=q,
            k=k,
            v=v,
            seq_offsets=seq_offsets,
            causal=causal,
            num_targets=num_targets,
            max_attn_len=max_attn_len,
            contextual_seq_len=contextual_seq_len,
            sort_by_length_indices=sort_by_length_indices,
        )

    @staticmethod
    # pyre-ignore[14]
    def backward(
        ctx, dout: torch.Tensor
    ) -> Tuple[
        None,
        None,
        torch.Tensor,
        torch.Tensor,
        torch.Tensor,
        None,
        None,
        None,
        None,
        None,
        None,
    ]:
        with torch.inference_mode():
            q, k, v, seq_offsets = ctx.saved_tensors[:4]
            idx = 4
            if ctx.has_multiple_targets:
                num_targets = ctx.saved_tensors[idx]
                idx += 1
            else:
                num_targets = None
            if ctx.sort_by_length:
                sort_by_length_indices = ctx.saved_tensors[idx]
            else:
                sort_by_length_indices = None

            dq = torch.empty_like(q)
            dk = torch.empty_like(k)
            dv = torch.empty_like(v)
            dq, dk, dv = triton_hstu_attention_bwd(
                dout=dout,
                q=q,
                k=k,
                v=v,
                dq=dq,
                dk=dk,
                dv=dv,
                seq_offsets=seq_offsets,
                num_targets=num_targets,
                N=ctx.N,
                alpha=ctx.alpha,
                max_attn_len=ctx.max_attn_len,
                causal=ctx.causal,
                contextual_seq_len=ctx.contextual_seq_len,
                sort_by_length_indices=sort_by_length_indices,
            )
            return (
                None,
                None,
                dq,
                dk,
                dv,
                None,
                None,
                None,
                None,
                None,
                None,
            )


@torch.fx.wrap
def native_triton_hstu_mha(
    N: int,
    alpha: float,
    q: torch.Tensor,
    k: torch.Tensor,
    v: torch.Tensor,
    seq_offsets: torch.Tensor,
    causal: bool,
    num_targets: Optional[torch.Tensor] = None,
    max_attn_len: Optional[int] = None,
    contextual_seq_len: int = 0,
    sort_by_length: bool = False,
) -> torch.Tensor:
    return _AttentionFunction.apply(
        N,
        alpha,
        q,
        k,
        v,
        seq_offsets,
        causal,
        num_targets,
        max_attn_len,
        contextual_seq_len,
        sort_by_length,
    )


def triton_hstu_mha(
    N: int,
    alpha: float,
    q: torch.Tensor,
    k: torch.Tensor,
    v: torch.Tensor,
    seq_offsets: torch.Tensor,
    causal: bool,
    num_targets: Optional[torch.Tensor] = None,
    max_attn_len: Optional[int] = None,
    contextual_seq_len: int = 0,
    sort_by_length: bool = False,
    triton_cc: bool = False,
) -> torch.Tensor:
    q = switch_to_contiguous_if_needed(q)
    k = switch_to_contiguous_if_needed(k)
    v = switch_to_contiguous_if_needed(v)
    seq_offsets = seq_offsets.contiguous()
    if triton_cc:
        return native_triton_hstu_mha(
            N,
            alpha,
            q,
            k,
            v,
            seq_offsets,
            causal,
            num_targets,
            max_attn_len,
            contextual_seq_len,
            sort_by_length,
        )
    else:
        return native_triton_hstu_mha(
            N,
            alpha,
            q,
            k,
            v,
            seq_offsets,
            causal,
            num_targets,
            max_attn_len,
            contextual_seq_len,
            sort_by_length,
        )


@torch.fx.wrap
def native_triton_cached_hstu_mha(
    N: int,
    alpha: float,
    delta_q: torch.Tensor,
    k: torch.Tensor,
    v: torch.Tensor,
    delta_x_offsets: torch.Tensor,
    seq_offsets: torch.Tensor,
    num_targets: Optional[torch.Tensor] = None,
    max_attn_len: Optional[int] = None,
) -> torch.Tensor:
    Z = seq_offsets.size(0) - 1
    AUTOTUNE_Z = prev_power_of_2(Z)
    L, H, DimQ = delta_q.shape
    DeltaSize = L // Z
    _, _, DimV = v.shape
    out = torch.empty((L, H, DimV), dtype=delta_q.dtype, device=delta_q.device)
    grid = lambda meta: (  # noqa E731
        triton.cdiv(DeltaSize, meta["BLOCK_M"]),
        Z * H,
    )
    _hstu_attn_fwd[grid](
        Q=delta_q,
        K=k,
        V=v,
        sort_by_length_indices=None,
        seq_offsets=seq_offsets,
        delta_x_offsets=delta_x_offsets,
        num_targets=num_targets,
        Out=out,
        stride_qm=delta_q.stride(0),
        stride_qh=delta_q.stride(1),
        stride_kn=k.stride(0),
        stride_kh=k.stride(1),
        stride_vn=v.stride(0),
        stride_vh=v.stride(1),
        stride_om=out.stride(0),
        stride_oh=out.stride(1),
        alpha=alpha,
        contextual_seq_len=0,
        Z=Z,
        AUTOTUNE_Z=AUTOTUNE_Z,
        H=H,
        MAX_SEQ_LEN=N,
        AUTOTUNE_MAX_SEQ_LEN=autotune_max_seq_len(N),
        DimQ=DimQ,
        DimV=DimV,
        DeltaSize=DeltaSize,
        MAX_ATTN_LEN=max_attn_len or 0,
        CAUSAL=True,
        HAS_MULTIPLE_TARGETS=num_targets is not None,
        IS_DELTA_Q=True,
        ALLOW_TF32=torch.backends.cuda.matmul.allow_tf32,
        BLOCK_D_Q=DimQ,
        BLOCK_D_V=DimV,
        HAS_CONTEXTUAL_SEQ_LEN=False,
        HAS_SORT_BY_LENGTH_INDICES=False,
    )
    return out


def triton_cached_hstu_mha(
    N: int,
    alpha: float,
    delta_q: torch.Tensor,
    k: torch.Tensor,
    v: torch.Tensor,
    delta_x_offsets: torch.Tensor,
    seq_offsets: torch.Tensor,
    num_targets: Optional[torch.Tensor] = None,
    max_attn_len: Optional[int] = None,
    triton_cc: bool = False,
) -> torch.Tensor:
    seq_offsets = seq_offsets.contiguous()
    delta_x_offsets = delta_x_offsets.contiguous()
    delta_q = switch_to_contiguous_if_needed(delta_q)
    k = switch_to_contiguous_if_needed(k)
    v = switch_to_contiguous_if_needed(v)

    if triton_cc:
        return native_triton_cached_hstu_mha(
            N=N,
            alpha=alpha,
            delta_q=delta_q,
            k=k,
            v=v,
            delta_x_offsets=delta_x_offsets,
            seq_offsets=seq_offsets,
            num_targets=num_targets,
            max_attn_len=max_attn_len,
        )
    else:
        return native_triton_cached_hstu_mha(
            N=N,
            alpha=alpha,
            delta_q=delta_q,
            k=k,
            v=v,
            delta_x_offsets=delta_x_offsets,
            seq_offsets=seq_offsets,
            num_targets=num_targets,
            max_attn_len=max_attn_len,
        )

</content>

<content full_path="generative_recommenders/ops/triton/triton_hstu_linear.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

#!/usr/bin/env python3


from typing import List, Optional, Tuple

import torch

# @manual=//triton:triton
import triton

# @manual=//triton:triton
import triton.language as tl

from generative_recommenders.common import (
    switch_to_contiguous_if_needed,
    triton_autotune,
)

from generative_recommenders.ops.triton.triton_addmm import _addmm_fwd


@triton.jit
def _ln_mul_dropout_fwd(
    X,
    U,
    Y,
    W,
    B,
    Mean,
    Rstd,
    D,
    eps,
    seed,
    dropout_ratio,
    stride_x,
    stride_u,
    stride_y,
    BLOCK_D: tl.constexpr,
    TRAINING: tl.constexpr,
    CONCAT_UX: tl.constexpr,
):
    row = tl.program_id(0)
    X += row.to(tl.int64) * stride_x
    U += row.to(tl.int64) * stride_u
    Y += row.to(tl.int64) * stride_y
    cols = tl.arange(0, BLOCK_D)

    # Compute mean
    mean = 0.0
    x = tl.load(X + cols, mask=cols < D, other=0.0).to(tl.float32)
    mean = tl.sum(x, axis=0) / D

    # Compute variance
    _var = tl.zeros([BLOCK_D], dtype=tl.float32)
    x_mean = tl.where(cols < D, x - mean, 0.0)
    _var += x_mean * x_mean
    var = tl.sum(_var, axis=0) / D
    rstd = 1 / tl.sqrt(var + eps)
    tl.store(Mean + row, mean)
    tl.store(Rstd + row, rstd)

    # Normalize and apply linear transformation
    mask = cols < D
    y = x_mean * rstd
    w = tl.load(W + cols, mask=mask).to(tl.float32)
    b = tl.load(B + cols, mask=mask).to(tl.float32)
    y = y * w + b
    u = tl.load(U + cols, mask=cols < D, other=0.0).to(tl.float32)
    y = y * u

    if TRAINING:
        random_offsets = row * BLOCK_D + cols
        if CONCAT_UX:
            # apply dropout on u
            random_u = tl.rand(seed, random_offsets)
            u_keep = random_u > dropout_ratio
            u = tl.where(u_keep, u / (1.0 - dropout_ratio), 0.0)
            # apply dropout on x
            random_x = tl.rand(seed, random_offsets + D)
            x_keep = random_x > dropout_ratio
            x = tl.where(x_keep, x / (1.0 - dropout_ratio), 0.0)
            # apply dropout on y
            random_y = tl.rand(seed, random_offsets + 2 * D)
            y_keep = random_y > dropout_ratio
            y = tl.where(y_keep, y / (1.0 - dropout_ratio), 0.0)
        else:
            random = tl.rand(seed, random_offsets)
            y_keep = random > dropout_ratio
            # write-back
            y = tl.where(y_keep, y / (1.0 - dropout_ratio), 0.0)

    # Write output
    if CONCAT_UX:
        tl.store(Y + cols, u.to(Y.dtype.element_ty), mask=mask)
        tl.store(Y + D + cols, x.to(Y.dtype.element_ty), mask=mask)
        tl.store(Y + 2 * D + cols, y.to(Y.dtype.element_ty), mask=mask)
    else:
        tl.store(Y + cols, y.to(Y.dtype.element_ty), mask=mask)


@triton.jit
def _ln_mul_dropout_bwd_dx_du(
    DX,
    DU,
    DY,
    DW,
    DB,
    X,
    U,
    Y,
    W,
    B,
    Mean,
    Rstd,
    stride_dx,
    stride_du,
    stride_dy,
    stride_x,
    stride_u,
    stride_y,
    D,
    eps,
    seed,
    dropout_ratio,
    N,
    BLOCK_D: tl.constexpr,
    TRAINING: tl.constexpr,
    CONCAT_UX: tl.constexpr,
    COMPUTE_Y: tl.constexpr,
):
    pid = tl.program_id(0)
    tile_num = tl.num_programs(0)
    rows_per_tile = N // tile_num
    if pid < N % tile_num:
        rows_per_tile += 1

    if rows_per_tile == 0:
        return

    cols = tl.arange(0, BLOCK_D)
    mask = cols < D

    row = pid
    X += row.to(tl.int64) * stride_x
    U += row.to(tl.int64) * stride_u
    if COMPUTE_Y:
        Y += row.to(tl.int64) * stride_y
    DY += row.to(tl.int64) * stride_dy
    DX += row.to(tl.int64) * stride_dx
    DU += row.to(tl.int64) * stride_du
    DW = DW + pid * D + cols
    DB = DB + pid * D + cols

    for idx in range(0, rows_per_tile):
        # Load data to SRAM
        x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)
        if CONCAT_UX:
            du = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)
            dx = tl.load(DY + D + cols, mask=mask, other=0).to(tl.float32)
            dy = tl.load(DY + 2 * D + cols, mask=mask, other=0).to(tl.float32)
        else:
            du = tl.zeros([BLOCK_D], dtype=tl.float32)
            dx = tl.zeros([BLOCK_D], dtype=tl.float32)
            dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)
        if TRAINING:
            random_offsets = row * BLOCK_D + cols
            if CONCAT_UX:
                # apply dropout on du
                random_du = tl.rand(seed, random_offsets)
                du_keep = random_du > dropout_ratio
                du = tl.where(du_keep, du / (1.0 - dropout_ratio), 0.0)
                # apply dropout on dx
                random_dx = tl.rand(seed, random_offsets + D)
                dx_keep = random_dx > dropout_ratio
                dx = tl.where(dx_keep, dx / (1.0 - dropout_ratio), 0.0)
                # apply dropout on dy
                random_dy = tl.rand(seed, random_offsets + 2 * D)
                dy_keep = random_dy > dropout_ratio
                dy = tl.where(dy_keep, dy / (1.0 - dropout_ratio), 0.0)
            else:
                random = tl.rand(seed, random_offsets)
                dy_keep = random > dropout_ratio
                # write-back
                dy = tl.where(dy_keep, dy / (1.0 - dropout_ratio), 0.0)

        mean = tl.load(Mean + row)
        rstd = tl.load(Rstd + row)

        # Compute dx
        xhat = (x - mean) * rstd
        w = tl.load(W + cols, mask=mask).to(tl.float32)
        b = tl.load(B + cols, mask=mask).to(tl.float32)
        ln = xhat * w + b
        du += dy * ln
        tl.store(DU + cols, du.to(DU.dtype.element_ty), mask=mask)
        u = tl.load(U + cols, mask=mask, other=0).to(tl.float32)
        dy = dy * u
        wdy = w * dy
        if COMPUTE_Y:
            y = ln * u
            if TRAINING:
                if CONCAT_UX:
                    u = tl.where(
                        du_keep,  # pyre-ignore [61]
                        u / (1.0 - dropout_ratio),
                        0.0,
                    )
                    x = tl.where(
                        dx_keep,  # pyre-ignore [61]
                        x / (1.0 - dropout_ratio),
                        0.0,
                    )
                    y = tl.where(
                        dy_keep,  # pyre-ignore [61]
                        y / (1.0 - dropout_ratio),
                        0.0,
                    )
                else:
                    y = tl.where(
                        dy_keep,  # pyre-ignore [61]
                        y / (1.0 - dropout_ratio),
                        0.0,
                    )
            if CONCAT_UX:
                tl.store(Y + cols, u.to(Y.dtype.element_ty), mask=mask)
                tl.store(Y + D + cols, x.to(Y.dtype.element_ty), mask=mask)
                tl.store(Y + 2 * D + cols, y.to(Y.dtype.element_ty), mask=mask)
            else:
                tl.store(Y + cols, y.to(Y.dtype.element_ty), mask=mask)
            Y += tile_num.to(tl.int64) * stride_y

        xhat = tl.where(mask, xhat, 0.0)
        wdy = tl.where(mask, wdy, 0.0)
        c1 = tl.sum(xhat * wdy, axis=0) / D
        c2 = tl.sum(wdy, axis=0) / D
        dx += (wdy - (xhat * c1 + c2)) * rstd
        # Write dx
        tl.store(DX + cols, dx, mask=mask)

        # Accumulate partial sums for dw/db
        partial_dw = dy * xhat
        partial_db = dy
        # First store doesn't accumulate
        if idx > 0:
            partial_dw += tl.load(DW, mask=mask)
            partial_db += tl.load(DB, mask=mask)
        tl.store(DW, partial_dw, mask=mask)
        tl.store(DB, partial_db, mask=mask)
        X += tile_num.to(tl.int64) * stride_x
        U += tile_num.to(tl.int64) * stride_u
        DY += tile_num.to(tl.int64) * stride_dy
        DX += tile_num.to(tl.int64) * stride_dx
        DU += tile_num.to(tl.int64) * stride_du
        row += tile_num


def _get_bwd_dwdb_configs() -> List[triton.Config]:
    configs = []
    for BLOCK_N in [32, 64, 128, 256]:
        for num_warps in [8, 16] + ([] if torch.ops.hip else [32]):
            configs.append(
                triton.Config(
                    {"BLOCK_N": BLOCK_N},
                    num_warps=num_warps,
                )
            )
    return configs


@triton_autotune(
    configs=_get_bwd_dwdb_configs(),
    key=["D"],
)
@triton.jit
def _ln_mul_dropout_bwd_dwdb(
    DW,
    DB,
    FINAL_DW,
    FINAL_DB,
    N,
    D,
    BLOCK_N: tl.constexpr,
    BLOCK_D: tl.constexpr,
):
    pid = tl.program_id(0)
    cols = pid * BLOCK_D + tl.arange(0, BLOCK_D)
    dw = tl.zeros((BLOCK_N, BLOCK_D), dtype=tl.float32)
    db = tl.zeros((BLOCK_N, BLOCK_D), dtype=tl.float32)

    for i in range(0, N, BLOCK_N):
        rows = i + tl.arange(0, BLOCK_N)
        # pyre-fixme[16]: `int` has no attribute `__getitem__`.
        mask = (rows[:, None] < N) & (cols[None, :] < D)
        offs = rows[:, None] * D + cols[None, :]
        dw += tl.load(DW + offs, mask=mask, other=0.0)
        db += tl.load(DB + offs, mask=mask, other=0.0)

    sum_dw = tl.sum(dw, axis=0)
    sum_db = tl.sum(db, axis=0)
    tl.store(FINAL_DW + cols, sum_dw.to(FINAL_DW.dtype.element_ty), mask=cols < D)
    tl.store(FINAL_DB + cols, sum_db.to(FINAL_DB.dtype.element_ty), mask=cols < D)


def triton_layer_norm_mul_dropout_fwd(
    x: torch.Tensor,
    u: torch.Tensor,
    weight: torch.Tensor,
    bias: torch.Tensor,
    eps: float,
    dropout_ratio: float,
    training: bool,
    concat_ux: bool = False,
    seed: Optional[int] = None,
) -> Tuple[
    torch.Tensor, torch.Tensor, torch.Tensor, int, int, int
]:  # y, mean, rstd, BLOCK_D, num_warps, seed
    assert x.dim() == 2
    x = switch_to_contiguous_if_needed(x)
    N, D = x.shape
    assert weight.dim() == 1
    assert bias.dim() == 1
    assert weight.numel() == D
    assert bias.numel() == D

    if concat_ux:
        y = torch.empty((N, 3 * D), dtype=x.dtype, device=x.device)
    else:
        y = torch.empty_like(x)
    mean = torch.empty((N,), dtype=torch.float32, device=x.device)
    rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
    if N == 0:
        return y, mean, rstd, 0, 0, 0
    # Less than 64KB per feature: enqueue fused kernel
    MAX_FUSED_SIZE = 65536 // x.element_size()
    BLOCK_D: int = min(MAX_FUSED_SIZE, triton.next_power_of_2(D))
    if D > BLOCK_D:
        raise RuntimeError("This layer norm doesn't support feature dim >= 64KB.")

    if seed is None:
        seed = torch.randint(low=0, high=2**62, size=(1,), dtype=torch.int64).item()
    num_warps: int = min(max(BLOCK_D // 256, 1), 8)
    # pyre-ignore[28]
    _ln_mul_dropout_fwd[(N,)](
        x,
        u,
        y,
        weight,
        bias,
        mean,
        rstd,
        D,
        eps,
        seed,
        dropout_ratio,
        x.stride(0),
        u.stride(0),
        y.stride(0),
        BLOCK_D=BLOCK_D,
        TRAINING=training,
        CONCAT_UX=concat_ux,
        num_warps=num_warps,
    )
    return y, mean, rstd, BLOCK_D, num_warps, seed  # pyre-ignore [7]


def triton_layer_norm_mul_dropout_bwd(
    dy: torch.Tensor,
    x: torch.Tensor,
    u: torch.Tensor,
    weight: torch.Tensor,
    bias: torch.Tensor,
    mean: torch.Tensor,
    rstd: torch.Tensor,
    BLOCK_D: int,
    num_warps: int,
    eps: float,
    training: bool,
    dropout_ratio: float,
    seed: Optional[int] = None,
    concat_ux: bool = False,
    compute_y: bool = False,
) -> Tuple[
    torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, Optional[torch.Tensor]
]:
    y = None
    N, D = x.shape
    if compute_y:
        if concat_ux:
            y = torch.empty((N, 3 * D), dtype=x.dtype, device=x.device)
        else:
            y = torch.empty_like(x)
    if N == 0:
        return (
            torch.zeros_like(x),
            torch.zeros_like(u),
            torch.zeros((D,), dtype=weight.dtype, device=x.device),
            torch.zeros((D,), dtype=weight.dtype, device=x.device),
            y,
        )
    dx = torch.empty_like(x)
    du = torch.empty_like(u)
    sms = torch.cuda.get_device_properties(x.device).multi_processor_count
    tile_num = max(1, min(sms * 64, N // 4))
    _dweight = torch.empty((tile_num, D), dtype=torch.float32, device=x.device)
    _dbias = torch.empty((tile_num, D), dtype=torch.float32, device=x.device)
    dweight = torch.empty((D,), dtype=weight.dtype, device=x.device)
    dbias = torch.empty((D,), dtype=weight.dtype, device=x.device)
    # pyre-ignore[28]
    _ln_mul_dropout_bwd_dx_du[(tile_num,)](
        dx,
        du,
        dy,
        _dweight,
        _dbias,
        x,
        u,
        y,
        weight,
        bias,
        mean,
        rstd,
        dx.stride(0),
        du.stride(0),
        dy.stride(0),
        x.stride(0),
        u.stride(0),
        y.stride(0) if compute_y else 0,  # pyre-ignore [16]
        D,
        eps,
        seed,
        dropout_ratio,
        N=N,
        BLOCK_D=BLOCK_D,
        TRAINING=training,
        CONCAT_UX=concat_ux,
        COMPUTE_Y=compute_y,
        num_warps=num_warps,
    )

    def grid(META):
        return (triton.cdiv(D, META["BLOCK_D"]),)

    blocks = triton.next_power_of_2(sms * 4)
    BLOCK_D = triton.next_power_of_2(triton.cdiv(D, blocks))
    BLOCK_D = min(max(BLOCK_D, 4), 128)
    _ln_mul_dropout_bwd_dwdb[grid](
        _dweight,
        _dbias,
        dweight,
        dbias,
        tile_num,
        D,
        BLOCK_D=BLOCK_D,
    )
    return dx, du, dweight, dbias, y


class LayerNormMulDropoutFunction(torch.autograd.Function):
    @staticmethod
    # pyre-ignore[14]
    def forward(
        ctx,
        x: torch.Tensor,
        u: torch.Tensor,
        weight: torch.Tensor,
        bias: torch.Tensor,
        eps: float,
        dropout_ratio: float,
        training: bool,
        concat_ux: bool = False,
        seed: Optional[int] = None,
    ) -> torch.Tensor:
        y, mean, rstd, BLOCK_D, num_warps, seed = triton_layer_norm_mul_dropout_fwd(
            x=x,
            u=u,
            weight=weight,
            bias=bias,
            eps=eps,
            dropout_ratio=dropout_ratio,
            training=training,
            concat_ux=concat_ux,
            seed=seed,
        )
        ctx.save_for_backward(x, u, weight, bias, mean, rstd)
        ctx.BLOCK_D = BLOCK_D
        ctx.num_warps = num_warps
        ctx.eps = eps
        ctx.seed = seed
        ctx.training = training
        ctx.concat_ux = concat_ux
        ctx.dropout_ratio = dropout_ratio
        return y

    @staticmethod
    # pyre-ignore[14]
    def backward(
        ctx, dy: torch.Tensor
    ) -> Tuple[
        torch.Tensor,
        torch.Tensor,
        torch.Tensor,
        torch.Tensor,
        None,
        None,
        None,
        None,
        None,
    ]:
        x, u, weight, bias, mean, rstd = ctx.saved_tensors
        dx, du, dweight, dbias, _ = triton_layer_norm_mul_dropout_bwd(
            dy=dy,
            x=x,
            u=u,
            weight=weight,
            bias=bias,
            mean=mean,
            rstd=rstd,
            BLOCK_D=ctx.BLOCK_D,
            num_warps=ctx.num_warps,
            eps=ctx.eps,
            training=ctx.training,
            dropout_ratio=ctx.dropout_ratio,
            seed=ctx.seed,
            concat_ux=ctx.concat_ux,
            compute_y=False,
        )
        return dx, du, dweight, dbias, None, None, None, None, None


@triton.jit
def _group_norm_mul_dropout_fwd(
    X,
    U,
    Y,
    W,
    B,
    Mean,
    Rstd,
    D,
    Heads,
    eps,
    seed,
    dropout_ratio,
    stride_x,
    stride_u,
    stride_y,
    BLOCK_D: tl.constexpr,
    BLOCK_H: tl.constexpr,
    TRAINING: tl.constexpr,
    CONCAT_UX: tl.constexpr,
):
    row = tl.program_id(0)
    X += row.to(tl.int64) * stride_x
    U += row.to(tl.int64) * stride_u
    Y += row.to(tl.int64) * stride_y
    cols = tl.arange(0, BLOCK_D)
    heads = tl.arange(0, BLOCK_H)
    offsets = heads[:, None] * D + cols[None, :]
    mask_h = heads < Heads
    mask_c = cols < D
    mask = mask_c[None, :] & mask_h[:, None]

    # Compute mean
    mean = 0.0
    x = tl.load(X + offsets, mask=mask, other=0.0).to(tl.float32)
    mean = tl.sum(x, axis=1) / D
    mean = tl.ravel(mean)

    # Compute variance
    _var = tl.zeros([BLOCK_H, BLOCK_D], dtype=tl.float32)
    x_mean = tl.where(mask, x - mean[:, None], 0.0)
    _var += x_mean * x_mean
    var = tl.sum(_var, axis=1) / D
    var = tl.ravel(var)
    rstd = 1 / tl.sqrt(var + eps)
    tl.store(Mean + row * Heads + heads, mean, mask=mask_h)
    tl.store(Rstd + row * Heads + heads, rstd, mask=mask_h)

    # Normalize and apply linear transformation
    y = x_mean * rstd[:, None]  # pyre-ignore [16]
    w = tl.load(W + heads, mask=mask_h).to(tl.float32)
    b = tl.load(B + heads, mask=mask_h).to(tl.float32)
    y = y * w[:, None] + b[:, None]
    u = tl.load(U + offsets, mask=mask, other=0.0).to(tl.float32)
    y = y * u

    if TRAINING:
        if CONCAT_UX:
            random_offsets = row * 3 * D * Heads + offsets
            # apply dropout on u
            random_u = tl.rand(seed, random_offsets)
            u_keep = random_u > dropout_ratio
            u = tl.where(u_keep, u / (1.0 - dropout_ratio), 0.0)
            # apply dropout on x
            random_x = tl.rand(seed, random_offsets + Heads * D)
            x_keep = random_x > dropout_ratio
            x = tl.where(x_keep, x / (1.0 - dropout_ratio), 0.0)
            # apply dropout on y
            random_y = tl.rand(seed, random_offsets + 2 * Heads * D)
            y_keep = random_y > dropout_ratio
            y = tl.where(y_keep, y / (1.0 - dropout_ratio), 0.0)
        else:
            random_offsets = row * D * Heads + offsets
            random = tl.rand(seed, random_offsets)
            y_keep = random > dropout_ratio
            # write-back
            y = tl.where(y_keep, y / (1.0 - dropout_ratio), 0.0)

    # Write output
    if CONCAT_UX:
        tl.store(Y + offsets, u.to(Y.dtype.element_ty), mask=mask)
        tl.store(Y + Heads * D + offsets, x.to(Y.dtype.element_ty), mask=mask)
        tl.store(Y + 2 * Heads * D + offsets, y.to(Y.dtype.element_ty), mask=mask)
    else:
        tl.store(Y + offsets, y.to(Y.dtype.element_ty), mask=mask)


@triton.jit
def _group_norm_mul_dropout_bwd_dx_du(
    DX,
    DU,
    DY,
    DW,
    DB,
    X,
    U,
    Y,
    W,
    B,
    Mean,
    Rstd,
    stride_dx,
    stride_du,
    stride_dy,
    stride_x,
    stride_u,
    stride_y,
    D,
    Heads,
    eps,
    seed,
    dropout_ratio,
    GROUP_N: tl.constexpr,
    BLOCK_D: tl.constexpr,
    BLOCK_H: tl.constexpr,
    TRAINING: tl.constexpr,
    CONCAT_UX: tl.constexpr,
    COMPUTE_Y: tl.constexpr,
):
    row = tl.program_id(0)
    cols = tl.arange(0, BLOCK_D)
    off_heads = tl.arange(0, BLOCK_H)
    mask_c = cols < D
    mask_h = off_heads < Heads
    mask = mask_c[None, :] & mask_h[:, None]
    X += row.to(tl.int64) * stride_x
    U += row.to(tl.int64) * stride_u
    DY += row.to(tl.int64) * stride_dy
    DX += row.to(tl.int64) * stride_dx
    DU += row.to(tl.int64) * stride_du
    offsets = off_heads[:, None] * D + cols[None, :]

    # Load data to SRAM
    x = tl.load(X + offsets, mask=mask, other=0).to(tl.float32)
    if CONCAT_UX:
        du = tl.load(DY + offsets, mask=mask, other=0).to(tl.float32)
        dx = tl.load(DY + Heads * D + offsets, mask=mask, other=0).to(tl.float32)
        dy = tl.load(DY + 2 * Heads * D + offsets, mask=mask, other=0).to(tl.float32)
    else:
        du = tl.zeros([BLOCK_H, BLOCK_D], dtype=tl.float32)
        dx = tl.zeros([BLOCK_H, BLOCK_D], dtype=tl.float32)
        dy = tl.load(DY + offsets, mask=mask, other=0).to(tl.float32)
    if TRAINING:
        if CONCAT_UX:
            random_offsets = row * 3 * D * Heads + offsets
            # apply dropout on du
            random_du = tl.rand(seed, random_offsets)
            du_keep = random_du > dropout_ratio
            du = tl.where(du_keep, du / (1.0 - dropout_ratio), 0.0)
            # apply dropout on dx
            random_dx = tl.rand(seed, random_offsets + Heads * D)
            dx_keep = random_dx > dropout_ratio
            dx = tl.where(dx_keep, dx / (1.0 - dropout_ratio), 0.0)
            # apply dropout on dy
            random_dy = tl.rand(seed, random_offsets + 2 * Heads * D)
            dy_keep = random_dy > dropout_ratio
            dy = tl.where(dy_keep, dy / (1.0 - dropout_ratio), 0.0)
        else:
            random_offsets = row * D * Heads + offsets
            random = tl.rand(seed, random_offsets)
            dy_keep = random > dropout_ratio
            # write-back
            dy = tl.where(dy_keep, dy / (1.0 - dropout_ratio), 0.0)

    mean = tl.load(Mean + row * Heads + off_heads)
    rstd = tl.load(Rstd + row * Heads + off_heads)

    # Compute dx
    xhat = (x - mean[:, None]) * rstd[:, None]
    w = tl.load(W + off_heads, mask=mask_h).to(tl.float32)
    b = tl.load(B + off_heads, mask=mask_h).to(tl.float32)
    ln = xhat * w[:, None] + b[:, None]
    du += dy * ln
    tl.store(DU + offsets, du.to(DU.dtype.element_ty), mask=mask)
    u = tl.load(U + offsets, mask=mask, other=0).to(tl.float32)
    dy = dy * u
    wdy = w[:, None] * dy
    if COMPUTE_Y:
        Y += row.to(tl.int64) * stride_y
        y = ln * u
        if TRAINING:
            if CONCAT_UX:
                u = tl.where(
                    du_keep,  # pyre-ignore [61]
                    u / (1.0 - dropout_ratio),
                    0.0,
                )
                x = tl.where(
                    dx_keep,  # pyre-ignore [61]
                    x / (1.0 - dropout_ratio),
                    0.0,
                )
                y = tl.where(
                    dy_keep,  # pyre-ignore [61]
                    y / (1.0 - dropout_ratio),
                    0.0,
                )
            else:
                y = tl.where(
                    dy_keep,  # pyre-ignore [61]
                    y / (1.0 - dropout_ratio),
                    0.0,
                )
        if CONCAT_UX:
            tl.store(Y + offsets, u.to(Y.dtype.element_ty), mask=mask)
            tl.store(Y + Heads * D + offsets, x.to(Y.dtype.element_ty), mask=mask)
            tl.store(Y + 2 * Heads * D + offsets, y.to(Y.dtype.element_ty), mask=mask)
        else:
            tl.store(Y + offsets, y.to(Y.dtype.element_ty), mask=mask)

    xhat = tl.where(mask, xhat, 0.0)
    wdy = tl.where(mask, wdy, 0.0)
    c1 = tl.sum(xhat * wdy, axis=1) / D
    c2 = tl.sum(wdy, axis=1) / D
    dx += (wdy - (xhat * c1[:, None] + c2[:, None])) * rstd[:, None]
    # Write dx
    tl.store(DX + offsets, dx, mask=mask)

    # Offset locks and weights/biases gradient pointer for parallel reduction
    lock_id = row % GROUP_N
    DW = DW + lock_id * Heads + off_heads
    DB = DB + lock_id * Heads + off_heads
    # Accumulate partial sums for dw/db
    partial_dw = tl.sum(dy * xhat, axis=1)
    partial_dw = tl.ravel(partial_dw)
    partial_db = tl.sum(dy, axis=1)
    partial_db = tl.ravel(partial_db)
    tl.atomic_add(
        DW,
        partial_dw,
        mask=mask_h,
        sem="relaxed",
    )
    tl.atomic_add(
        DB,
        partial_db,
        mask=mask_h,
        sem="relaxed",
    )


def triton_group_norm_mul_dropout_fwd(
    x: torch.Tensor,
    u: torch.Tensor,
    weight: torch.Tensor,
    bias: torch.Tensor,
    eps: float,
    dropout_ratio: float,
    training: bool,
    concat_ux: bool = False,
    num_heads: int = 1,
    linear_dim: int = -1,
    seed: Optional[int] = None,
) -> Tuple[
    torch.Tensor, torch.Tensor, torch.Tensor, int, int, int, int
]:  # y, mean, rstd, BLOCK_D, BLOCK_H, num_warps, seed
    assert x.dim() == 2
    assert x.shape == u.shape
    assert x.shape[1] == num_heads * linear_dim
    x = switch_to_contiguous_if_needed(x)
    u = switch_to_contiguous_if_needed(u)
    N, _ = x.shape
    assert weight.dim() == 1
    assert bias.dim() == 1
    assert weight.numel() == num_heads
    assert bias.numel() == num_heads

    if concat_ux:
        y = torch.empty((N, 3 * num_heads * linear_dim), dtype=x.dtype, device=x.device)
    else:
        y = torch.empty((N, num_heads * linear_dim), dtype=x.dtype, device=x.device)
    mean = torch.empty((N * num_heads,), dtype=torch.float32, device=x.device)
    rstd = torch.empty((N * num_heads,), dtype=torch.float32, device=x.device)
    if N == 0:
        return y, mean, rstd, 0, 0, 0, 0
    # Less than 64KB per feature: enqueue fused kernel
    MAX_FUSED_SIZE = 65536 // x.element_size()
    BLOCK_D: int = triton.next_power_of_2(linear_dim)
    BLOCK_H: int = triton.next_power_of_2(num_heads)
    if BLOCK_D * BLOCK_H > MAX_FUSED_SIZE:
        raise RuntimeError(
            "This group norm doesn't support num_heads * linear_dim >= 64KB."
        )

    if seed is None:
        seed = torch.randint(low=0, high=2**62, size=(1,), dtype=torch.int64).item()
    num_warps: int = min(max(BLOCK_D * BLOCK_H // 256, 1), 8)
    # pyre-ignore[28]
    _group_norm_mul_dropout_fwd[(N,)](
        x,
        u,
        y,
        weight,
        bias,
        mean,
        rstd,
        linear_dim,
        num_heads,
        eps,
        seed,
        dropout_ratio,
        x.stride(0),
        u.stride(0),
        y.stride(0),
        BLOCK_D=BLOCK_D,
        BLOCK_H=BLOCK_H,
        TRAINING=training,
        CONCAT_UX=concat_ux,
        num_warps=num_warps,
    )
    return y, mean, rstd, BLOCK_D, BLOCK_H, num_warps, seed  # pyre-ignore [7]


def triton_group_norm_mul_dropout_bwd(
    dy: torch.Tensor,
    x: torch.Tensor,
    u: torch.Tensor,
    weight: torch.Tensor,
    bias: torch.Tensor,
    mean: torch.Tensor,
    rstd: torch.Tensor,
    BLOCK_D: int,
    BLOCK_H: int,
    num_warps: int,
    eps: float,
    training: bool,
    dropout_ratio: float,
    seed: Optional[int] = None,
    concat_ux: bool = False,
    num_heads: int = 1,
    linear_dim: int = -1,
    compute_y: bool = False,
) -> Tuple[
    torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, Optional[torch.Tensor]
]:
    y = None
    N, dim = x.shape
    if compute_y:
        if concat_ux:
            y = torch.empty(
                (N, 3 * num_heads * linear_dim), dtype=x.dtype, device=x.device
            )
        else:
            y = torch.empty((N, num_heads * linear_dim), dtype=x.dtype, device=x.device)
    if N == 0:
        return (
            torch.zeros_like(x),
            torch.zeros_like(u),
            torch.zeros_like(weight),
            torch.zeros_like(bias),
            y,
        )
    dx = torch.empty_like(x)
    du = torch.empty_like(u)
    if dim <= 1024:
        GROUP_N = 256 * 8
    elif dim <= 4096:
        GROUP_N = 128 * 8
    elif dim <= 8192:
        GROUP_N = 96 * 8
    else:
        GROUP_N = 64 * 8
    GROUP_N = N if GROUP_N > N else GROUP_N
    _dweight = torch.zeros((GROUP_N, num_heads), dtype=torch.float32, device=x.device)
    _dbias = torch.zeros((GROUP_N, num_heads), dtype=torch.float32, device=x.device)
    dweight = torch.empty((num_heads,), dtype=weight.dtype, device=x.device)
    dbias = torch.empty((num_heads,), dtype=weight.dtype, device=x.device)
    # pyre-ignore[28]
    _group_norm_mul_dropout_bwd_dx_du[(N,)](
        dx,
        du,
        dy,
        _dweight,
        _dbias,
        x,
        u,
        y,
        weight,
        bias,
        mean,
        rstd,
        dx.stride(0),
        du.stride(0),
        dy.stride(0),
        x.stride(0),
        u.stride(0),
        y.stride(0) if compute_y else 0,  # pyre-ignore [16]
        linear_dim,
        num_heads,
        eps,
        seed,
        dropout_ratio,
        GROUP_N=GROUP_N,
        BLOCK_D=BLOCK_D,
        BLOCK_H=BLOCK_H,
        TRAINING=training,
        CONCAT_UX=concat_ux,
        COMPUTE_Y=compute_y,
        num_warps=num_warps,
    )
    _group_norm_bwd_dwdb[(num_heads,)](
        _dweight,
        _dbias,
        dweight,
        dbias,
        GROUP_N,
    )
    return dx, du, dweight, dbias, y


def _get_bwd_dwdb_configs() -> List[triton.Config]:
    configs = []
    for BLOCK_N in [32, 64, 128, 256]:
        for num_warps in [8, 16] + ([] if torch.ops.hip else [32]):
            configs.append(
                triton.Config(
                    {"BLOCK_N": BLOCK_N},
                    num_warps=num_warps,
                )
            )
    return configs


@triton_autotune(
    configs=_get_bwd_dwdb_configs(),
    key=[],
)
@triton.jit
def _group_norm_bwd_dwdb(
    DW,
    DB,
    FINAL_DW,
    FINAL_DB,
    N,
    BLOCK_N: tl.constexpr,
):
    col = tl.program_id(0)
    num_heads = tl.num_programs(0)
    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)
    db = tl.zeros((BLOCK_N,), dtype=tl.float32)

    for i in range(0, N, BLOCK_N):
        rows = i + tl.arange(0, BLOCK_N)
        mask = rows < N
        offs = rows * num_heads + col
        dw += tl.load(DW + offs, mask=mask, other=0.0)
        db += tl.load(DB + offs, mask=mask, other=0.0)

    sum_dw = tl.sum(dw, axis=0)
    sum_db = tl.sum(db, axis=0)
    tl.store(FINAL_DW + col, sum_dw.to(FINAL_DW.dtype.element_ty))
    tl.store(FINAL_DB + col, sum_db.to(FINAL_DB.dtype.element_ty))


class GroupNormMulDropoutFunction(torch.autograd.Function):
    @staticmethod
    # pyre-ignore[14]
    def forward(
        ctx,
        x: torch.Tensor,
        u: torch.Tensor,
        weight: torch.Tensor,
        bias: torch.Tensor,
        eps: float,
        dropout_ratio: float,
        training: bool,
        concat_ux: bool = False,
        num_heads: int = 1,
        linear_dim: int = -1,
        seed: Optional[int] = None,
    ) -> torch.Tensor:
        y, mean, rstd, BLOCK_D, BLOCK_H, num_warps, seed = (
            triton_group_norm_mul_dropout_fwd(
                x=x,
                u=u,
                weight=weight,
                bias=bias,
                eps=eps,
                dropout_ratio=dropout_ratio,
                training=training,
                concat_ux=concat_ux,
                num_heads=num_heads,
                linear_dim=linear_dim,
                seed=seed,
            )
        )
        ctx.save_for_backward(x, u, weight, bias, mean, rstd)
        ctx.BLOCK_D = BLOCK_D
        ctx.BLOCK_H = BLOCK_H
        ctx.num_warps = num_warps
        ctx.eps = eps
        ctx.seed = seed
        ctx.training = training
        ctx.concat_ux = concat_ux
        ctx.dropout_ratio = dropout_ratio
        ctx.num_heads = num_heads
        ctx.linear_dim = linear_dim
        return y

    @staticmethod
    # pyre-ignore[14]
    def backward(
        ctx, dy: torch.Tensor
    ) -> Tuple[
        torch.Tensor,
        torch.Tensor,
        torch.Tensor,
        torch.Tensor,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
    ]:
        x, u, weight, bias, mean, rstd = ctx.saved_tensors
        dx, du, dweight, dbias, _ = triton_group_norm_mul_dropout_bwd(
            dy=dy,
            x=x,
            u=u,
            weight=weight,
            bias=bias,
            mean=mean,
            rstd=rstd,
            BLOCK_D=ctx.BLOCK_D,
            BLOCK_H=ctx.BLOCK_H,
            num_warps=ctx.num_warps,
            eps=ctx.eps,
            training=ctx.training,
            dropout_ratio=ctx.dropout_ratio,
            seed=ctx.seed,
            concat_ux=ctx.concat_ux,
            num_heads=ctx.num_heads,
            linear_dim=ctx.linear_dim,
            compute_y=False,
        )
        return (
            dx,
            du,
            dweight,
            dbias,
            None,
            None,
            None,
            None,
            None,
            None,
            None,
        )


def triton_addmm_fwd(
    x: torch.Tensor,
    w: torch.Tensor,
    y: torch.Tensor,
) -> torch.Tensor:
    M, K = x.shape
    KB, N = w.shape
    assert K == KB, f"incompatible dimensions {K}, {KB}"

    is_y_1d = y.dim() == 1
    NY = y.shape[0] if is_y_1d else y.shape[1]
    assert N == NY, f"incompatible dimensions {N}, {NY}"

    # Allocate output
    z = torch.empty((M, N), device=x.device, dtype=x.dtype)
    if M == 0 or N == 0:
        return z

    grid = lambda meta: (  # noqa E731
        triton.cdiv(M, meta["BLOCK_M"]),
        triton.cdiv(N, meta["BLOCK_N"]),
    )

    _addmm_fwd[grid](
        x,
        w,
        y,
        z,
        M,
        N,
        K,
        x.stride(0),
        x.stride(1),
        w.stride(0),
        w.stride(1),
        y.stride(0) if not is_y_1d else 0,
        y.stride(1) if not is_y_1d else y.stride(0),
        z.stride(0),
        z.stride(1),
        ALLOW_TF32=torch.backends.cuda.matmul.allow_tf32,
        BROADCAST_Y=is_y_1d,
    )
    return z


def triton_addmm_bwd(
    x: torch.Tensor,
    w: torch.Tensor,
    dz: torch.Tensor,
    is_y_1d: bool,
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    if is_y_1d:
        dy = torch.sum(dz, dim=0)
    else:
        dy = dz
    dw = torch.mm(x.t(), dz)
    dx = torch.mm(dz, w.t())

    return dx, dw, dy


class _AddMmFunction(torch.autograd.Function):
    @staticmethod
    # pyre-ignore[14]
    def forward(
        ctx,
        x: torch.Tensor,
        w: torch.Tensor,
        y: torch.Tensor,
    ) -> torch.Tensor:
        ctx.save_for_backward(x, w)
        ctx.is_y_1d = y.dim() == 1
        return triton_addmm_fwd(x, w, y)

    @staticmethod
    # pyre-ignore[14]
    def backward(
        ctx, dz: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        (x, w) = ctx.saved_tensors
        return triton_addmm_bwd(x, w, dz, ctx.is_y_1d)


class HSTUComputeOutputFunction(torch.autograd.Function):
    @staticmethod
    # pyre-ignore[14]
    def forward(
        ctx,
        attn: torch.Tensor,
        u: torch.Tensor,
        x: torch.Tensor,
        norm_weight: torch.Tensor,
        norm_bias: torch.Tensor,
        output_weight: torch.Tensor,
        eps: float,
        dropout_ratio: float,
        training: bool,
        concat_ux: bool = False,
        group_norm: bool = False,
        num_heads: int = 1,
        linear_dim: int = -1,
        seed: Optional[int] = None,
        recompute_y_in_backward: bool = False,
    ) -> torch.Tensor:
        if group_norm:
            y, mean, rstd, BLOCK_D, BLOCK_H, num_warps, seed = (
                triton_group_norm_mul_dropout_fwd(
                    x=attn,
                    u=u,
                    weight=norm_weight,
                    bias=norm_bias,
                    eps=eps,
                    dropout_ratio=dropout_ratio,
                    training=training,
                    concat_ux=concat_ux,
                    num_heads=num_heads,
                    linear_dim=linear_dim,
                    seed=seed,
                )
            )
            ctx.BLOCK_H = BLOCK_H
        else:
            y, mean, rstd, BLOCK_D, num_warps, seed = triton_layer_norm_mul_dropout_fwd(
                x=attn,
                u=u,
                weight=norm_weight,
                bias=norm_bias,
                eps=eps,
                dropout_ratio=dropout_ratio,
                training=training,
                concat_ux=concat_ux,
                seed=seed,
            )

        # NOTE: for AMD training, we go with torch.addmm instead of the triton
        # version before Triton on AMD achieves on-par perf with NV GPU.
        if torch.version.hip:
            out = torch.addmm(x, y, output_weight)
        else:
            out = triton_addmm_fwd(x=y, w=output_weight, y=x)

        saved_tensors = [attn, u, norm_weight, norm_bias, mean, rstd, output_weight]
        if not recompute_y_in_backward:
            saved_tensors.append(y)
        ctx.save_for_backward(*saved_tensors)
        ctx.BLOCK_D = BLOCK_D
        ctx.num_warps = num_warps
        ctx.eps = eps
        ctx.seed = seed
        ctx.training = training
        ctx.concat_ux = concat_ux
        ctx.dropout_ratio = dropout_ratio
        ctx.num_heads = num_heads
        ctx.linear_dim = linear_dim
        ctx.group_norm = group_norm
        ctx.recompute_y_in_backward = recompute_y_in_backward
        return out

    @staticmethod
    # pyre-ignore[14]
    def backward(
        ctx, dout: torch.Tensor
    ) -> Tuple[
        torch.Tensor,  # dattn
        torch.Tensor,  # du
        torch.Tensor,  # dx
        torch.Tensor,  # d_norm_weight
        torch.Tensor,  # d_norm_bias
        torch.Tensor,  # d_output_weight
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
    ]:
        attn, u, norm_weight, norm_bias, mean, rstd, output_weight = ctx.saved_tensors[
            :7
        ]
        dy = torch.mm(dout, output_weight.t())

        if ctx.group_norm:
            dattn, du, d_norm_weight, d_norm_bias, y = (
                triton_group_norm_mul_dropout_bwd(
                    dy=dy,
                    x=attn,
                    u=u,
                    weight=norm_weight,
                    bias=norm_bias,
                    mean=mean,
                    rstd=rstd,
                    BLOCK_D=ctx.BLOCK_D,
                    BLOCK_H=ctx.BLOCK_H,
                    num_warps=ctx.num_warps,
                    eps=ctx.eps,
                    training=ctx.training,
                    dropout_ratio=ctx.dropout_ratio,
                    seed=ctx.seed,
                    concat_ux=ctx.concat_ux,
                    num_heads=ctx.num_heads,
                    linear_dim=ctx.linear_dim,
                    compute_y=ctx.recompute_y_in_backward,
                )
            )
        else:
            dattn, du, d_norm_weight, d_norm_bias, y = (
                triton_layer_norm_mul_dropout_bwd(
                    dy=dy,
                    x=attn,
                    u=u,
                    weight=norm_weight,
                    bias=norm_bias,
                    mean=mean,
                    rstd=rstd,
                    BLOCK_D=ctx.BLOCK_D,
                    num_warps=ctx.num_warps,
                    eps=ctx.eps,
                    training=ctx.training,
                    dropout_ratio=ctx.dropout_ratio,
                    seed=ctx.seed,
                    concat_ux=ctx.concat_ux,
                    compute_y=ctx.recompute_y_in_backward,
                )
            )
        if not ctx.recompute_y_in_backward:
            y = ctx.saved_tensors[7]
        d_output_weight = torch.mm(y.t(), dout)
        return (
            dattn,
            du,
            dout,
            d_norm_weight,
            d_norm_bias,
            d_output_weight,
            None,
            None,
            None,
            None,
            None,
            None,
            None,
            None,
            None,
        )


@torch.fx.wrap
def triton_norm_mul_dropout(
    x: torch.Tensor,
    u: torch.Tensor,
    weight: torch.Tensor,
    bias: torch.Tensor,
    eps: float,
    dropout_ratio: float,
    training: bool,
    concat_ux: bool = False,
    group_norm: bool = False,
    num_heads: int = 1,
    linear_dim: int = -1,
    seed: Optional[int] = None,
) -> torch.Tensor:
    if group_norm:
        return GroupNormMulDropoutFunction.apply(
            x,
            u,
            weight,
            bias,
            eps,
            dropout_ratio,
            training,
            concat_ux,
            num_heads,
            linear_dim,
            seed,
        )
    else:
        return LayerNormMulDropoutFunction.apply(
            x, u, weight, bias, eps, dropout_ratio, training, concat_ux, seed
        )


@torch.fx.wrap
def triton_addmm(
    input: torch.Tensor,
    mat1: torch.Tensor,
    mat2: torch.Tensor,
) -> torch.Tensor:
    return _AddMmFunction.apply(mat1, mat2, input)


@torch.fx.wrap
def triton_hstu_compute_output(
    attn: torch.Tensor,
    u: torch.Tensor,
    x: torch.Tensor,
    norm_weight: torch.Tensor,
    norm_bias: torch.Tensor,
    output_weight: torch.Tensor,
    eps: float,
    dropout_ratio: float,
    training: bool,
    concat_ux: bool = False,
    group_norm: bool = False,
    num_heads: int = 1,
    linear_dim: int = -1,
    seed: Optional[int] = None,
    recompute_y_in_backward: bool = False,
) -> torch.Tensor:
    return HSTUComputeOutputFunction.apply(
        attn,
        u,
        x,
        norm_weight,
        norm_bias,
        output_weight,
        eps,
        dropout_ratio,
        training,
        concat_ux,
        group_norm,
        num_heads,
        linear_dim,
        seed,
        recompute_y_in_backward,
    )

</content>

<content full_path="generative_recommenders/ops/triton/triton_jagged.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

#!/usr/bin/env python3


from typing import List, Optional, Tuple

import torch

# @manual=//triton:triton
import triton

# @manual=//triton:triton
import triton.language as tl

from generative_recommenders.common import (
    autotune_max_seq_len,
    switch_to_contiguous_if_needed,
    triton_autotune,
)


def _get_bmm_configs() -> List[triton.Config]:
    configs = []
    for BLOCK_M in [64, 128]:
        for BLOCK_N in [64, 128]:
            for BLOCK_K in [32, 64]:
                for num_stages in [2, 3]:
                    for num_warps in [4, 8]:
                        configs.append(
                            triton.Config(
                                {
                                    "BLOCK_M": BLOCK_M,
                                    "BLOCK_N": BLOCK_N,
                                    "BLOCK_K": BLOCK_K,
                                },
                                num_stages=num_stages,
                                num_warps=num_warps,
                            )
                        )
    return configs


@triton_autotune(
    configs=_get_bmm_configs(),
    key=["AUTOTUNE_MAX_SEQ_LEN", "N", "K"],
)
@triton.jit
def jagged_dense_bmm_broadcast_add_kernel(
    seq_offsets,
    Jagged,
    Dense,
    Bias,
    Out,
    AUTOTUNE_MAX_SEQ_LEN,
    N,
    K,
    stride_jm,
    stride_db,
    stride_dk,
    stride_dn,
    stride_bias_b,
    stride_om,
    HAS_BIAS: tl.constexpr,
    ALLOW_TF32: tl.constexpr,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    BLOCK_K: tl.constexpr,
):
    """
    Computing bmm Out = Jagged x Dense + Bias
    M is the jagged dimension
    Jagged has shape (sum_B(M_i), K), Dense has shape (B, K, N), Bias has shape (B, N), and Out has shape (sum_B(M_i), N)
    """

    off_n = tl.program_id(0)
    off_m = tl.program_id(1)
    off_b = tl.program_id(2)

    seq_start = tl.load(seq_offsets + off_b).to(tl.int64)
    seq_end = tl.load(seq_offsets + off_b + 1)
    seq_len = seq_end - seq_start
    start_m = off_m * BLOCK_M
    start_n = off_n * BLOCK_N
    if start_m >= seq_len:
        return

    Jagged += seq_start * stride_jm
    Dense += off_b.to(tl.int64) * stride_db
    Out += seq_start * stride_om

    offs_m = start_m + tl.arange(0, BLOCK_M)
    offs_n = start_n + tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, BLOCK_K)
    jg_ptrs = Jagged + offs_m[:, None] * stride_jm + offs_k[None, :]
    dn_ptrs = Dense + offs_k[:, None] * stride_dk + offs_n[None, :] * stride_dn

    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)
    for k in range(0, K, BLOCK_K):
        jg = tl.load(
            jg_ptrs,
            # pyre-fixme[16]: `int` has no attribute `__getitem__`.
            mask=(offs_m[:, None] < seq_len) and ((k + offs_k)[None, :] < K),
            other=0.0,
        )
        dn = tl.load(
            dn_ptrs,
            mask=((k + offs_k)[:, None] < K) and (offs_n[None, :] < N),
            other=0.0,
        )
        accumulator += tl.dot(jg, dn, allow_tf32=ALLOW_TF32)
        jg_ptrs += BLOCK_K
        dn_ptrs += BLOCK_K * stride_dk

    if HAS_BIAS:
        bias_ptrs = Bias + off_b * stride_bias_b + offs_n
        bias = tl.load(bias_ptrs, mask=offs_n < N)
        accumulator += bias[None, :].to(tl.float32)

    out = accumulator.to(Out.dtype.element_ty)

    offs_m = start_m + tl.arange(0, BLOCK_M)
    offs_n = start_n + tl.arange(0, BLOCK_N)
    out_ptrs = Out + offs_m[:, None] * stride_om + offs_n[None, :]
    tl.store(out_ptrs, out, mask=(offs_m[:, None] < seq_len) & (offs_n[None, :] < N))


@triton_autotune(
    configs=_get_bmm_configs(),
    key=["M", "N", "AUTOTUNE_MAX_SEQ_LEN"],
)
@triton.jit
def _jagged_jagged_bmm_reduce_sum(
    seq_offsets,
    JaggedA,
    JaggedB,
    Out,
    ReduceOut,
    M,
    N,
    AUTOTUNE_MAX_SEQ_LEN,
    stride_ak,
    stride_bk,
    stride_ob,
    stride_om,
    stride_on,
    stride_orb,
    stride_orn,
    REDUCE_JAGGEDB: tl.constexpr,
    ALLOW_TF32: tl.constexpr,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    BLOCK_K: tl.constexpr,
):
    """
    Computing bmm Out = Jagged x Jagged
    K is the jagged dimension
    JaggedA has shape (sum_B(K_i), M), JaggedB has shape (sum_B(K_i), N), and Out has shape (B, M, N)
    """

    off_b = tl.program_id(0)
    off_m = tl.program_id(1)
    off_n = tl.program_id(2)

    seq_start = tl.load(seq_offsets + off_b).to(tl.int64)
    seq_end = tl.load(seq_offsets + off_b + 1)
    seq_len = seq_end - seq_start

    start_m = off_m * BLOCK_M
    start_n = off_n * BLOCK_N

    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)
    Out += off_b.to(tl.int64) * stride_ob
    offs_m = start_m + tl.arange(0, BLOCK_M)
    offs_n = start_n + tl.arange(0, BLOCK_N)
    out_ptrs = Out + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on
    if REDUCE_JAGGEDB:
        out_reduce_ptrs = ReduceOut + off_b * stride_orb + offs_n * stride_orn
        acc_reduce = tl.zeros((BLOCK_N,), dtype=tl.float32)
    if seq_len == 0:
        out = accumulator.to(Out.dtype.element_ty)
        tl.store(out_ptrs, out, mask=(offs_m[:, None] < M) & (offs_n[None, :] < N))
        if REDUCE_JAGGEDB:
            if off_m == 0:
                tl.store(
                    out_reduce_ptrs,  # pyre-ignore [61]
                    acc_reduce.to(ReduceOut.dtype.element_ty),
                    mask=(offs_n < N),
                )
        return

    JaggedA += seq_start * stride_ak
    JaggedB += seq_start * stride_bk
    offs_k = tl.arange(0, BLOCK_K)
    jg_a_ptrs = JaggedA + offs_k[None, :] * stride_ak + offs_m[:, None]
    jg_b_ptrs = JaggedB + offs_k[:, None] * stride_bk + offs_n[None, :]

    for k in range(0, seq_len, BLOCK_K):
        jg_a = tl.load(
            jg_a_ptrs,
            # pyre-fixme[16]: `int` has no attribute `__getitem__`.
            mask=(offs_m[:, None] < M) and ((k + offs_k)[None, :] < seq_len),
            other=0.0,
        )
        jg_b = tl.load(
            jg_b_ptrs,
            mask=(offs_n[None, :] < N) and ((k + offs_k)[:, None] < seq_len),
            other=0.0,
        )

        accumulator += tl.dot(jg_a, jg_b, allow_tf32=ALLOW_TF32)
        if REDUCE_JAGGEDB:
            if off_m == 0:
                acc_reduce += tl.sum(jg_b.to(tl.float32), axis=0)

        jg_a_ptrs += BLOCK_K * stride_ak
        jg_b_ptrs += BLOCK_K * stride_bk

    out = accumulator.to(Out.dtype.element_ty)
    tl.store(out_ptrs, out, mask=(offs_m[:, None] < M) & (offs_n[None, :] < N))
    if REDUCE_JAGGEDB:
        if off_m == 0:
            tl.store(
                out_reduce_ptrs,  # pyre-ignore [61]
                acc_reduce.to(ReduceOut.dtype.element_ty),
                mask=(offs_n < N),
            )


class _JaggedDenseBmmFunction(torch.autograd.Function):
    @staticmethod
    # pyre-ignore[14]
    def forward(
        ctx,
        max_seq_len: int,
        seq_offsets: torch.Tensor,
        jagged: torch.Tensor,
        dense: torch.Tensor,
    ):
        jagged = switch_to_contiguous_if_needed(jagged)
        L, D = jagged.shape
        B, _, K = dense.shape
        bmm_out = torch.empty((L, K), dtype=jagged.dtype, device=jagged.device)

        grid = lambda meta: (  # noqa E731
            triton.cdiv(K, meta["BLOCK_N"]),
            triton.cdiv(max_seq_len, meta["BLOCK_M"]),
            B,
        )

        jagged_dense_bmm_broadcast_add_kernel[grid](
            seq_offsets=seq_offsets,
            Jagged=jagged,
            Dense=dense,
            Bias=None,
            Out=bmm_out,
            AUTOTUNE_MAX_SEQ_LEN=autotune_max_seq_len(max_seq_len),
            N=K,
            K=D,
            stride_jm=jagged.stride(0),
            stride_db=dense.stride(0),
            stride_dk=dense.stride(1),
            stride_dn=dense.stride(2),
            stride_bias_b=0,
            stride_om=bmm_out.stride(0),
            HAS_BIAS=False,
            ALLOW_TF32=torch.backends.cuda.matmul.allow_tf32,
        )

        ctx.save_for_backward(seq_offsets, jagged, dense)
        ctx.B = B
        ctx.max_seq_len = max_seq_len
        ctx.K = K
        ctx.D = D
        return bmm_out

    @staticmethod
    # pyre-ignore[14]
    def backward(
        ctx, d_bmm_out: torch.Tensor
    ) -> Tuple[None, None, torch.Tensor, torch.Tensor]:
        seq_offsets, jagged, dense = ctx.saved_tensors
        d_jagged = torch.empty_like(jagged)
        d_dense = torch.empty_like(dense)

        grid = lambda meta: (  # noqa E731
            triton.cdiv(ctx.D, meta["BLOCK_N"]),
            triton.cdiv(ctx.max_seq_len, meta["BLOCK_M"]),
            ctx.B,
        )
        jagged_dense_bmm_broadcast_add_kernel[grid](
            seq_offsets=seq_offsets,
            Jagged=d_bmm_out,
            Dense=dense,
            Bias=None,
            Out=d_jagged,
            AUTOTUNE_MAX_SEQ_LEN=autotune_max_seq_len(ctx.max_seq_len),
            N=ctx.D,
            K=ctx.K,
            stride_jm=d_bmm_out.stride(0),
            stride_db=dense.stride(0),
            stride_dk=dense.stride(2),
            stride_dn=dense.stride(1),
            stride_bias_b=0,
            stride_om=d_jagged.stride(0),
            HAS_BIAS=False,
            ALLOW_TF32=torch.backends.cuda.matmul.allow_tf32,
        )

        grid = lambda meta: (  # noqa E731
            ctx.B,
            triton.cdiv(ctx.D, meta["BLOCK_M"]),
            triton.cdiv(ctx.K, meta["BLOCK_N"]),
        )
        _jagged_jagged_bmm_reduce_sum[grid](
            seq_offsets=seq_offsets,
            JaggedA=jagged,
            JaggedB=d_bmm_out,
            Out=d_dense,
            ReduceOut=None,
            M=ctx.D,
            N=ctx.K,
            AUTOTUNE_MAX_SEQ_LEN=autotune_max_seq_len(ctx.max_seq_len),
            stride_ak=jagged.stride(0),
            stride_bk=d_bmm_out.stride(0),
            stride_ob=d_dense.stride(0),
            stride_om=d_dense.stride(1),
            stride_on=d_dense.stride(2),
            stride_orb=0,
            stride_orn=0,
            REDUCE_JAGGEDB=False,
            ALLOW_TF32=torch.backends.cuda.matmul.allow_tf32,
        )

        return None, None, d_jagged, d_dense


def _get_jagged_dense_broadcast_add_configs() -> List[triton.Config]:
    configs = []
    for BLOCK_N in [16, 32, 64]:
        for num_stages in [1, 2]:
            for num_warps in [2, 4, 8]:
                configs.append(
                    triton.Config(
                        {
                            "BLOCK_N": BLOCK_N,
                        },
                        num_stages=num_stages,
                        num_warps=num_warps,
                    )
                )
    return configs


@triton_autotune(
    configs=_get_jagged_dense_broadcast_add_configs(),
    key=["AUTOTUNE_MAX_SEQ_LEN"],
)
@triton.jit
def jagged_dense_broadcast_add_kernel(
    seq_offsets,
    Jagged,
    Dense,
    Out,
    AUTOTUNE_MAX_SEQ_LEN,
    D,
    stride_jn,
    stride_db,
    stride_on,
    BLOCK_N: tl.constexpr,
    BLOCK_D: tl.constexpr,
):
    """
    Computing Out = Jagged + Dense
    JaggedA has shape (sum_B(N_i), D), Dense has shape (B, D), and Out has shape (sum_B(N_i), D)
    """

    off_b = tl.program_id(0)
    off_n = tl.program_id(1)
    seq_start = tl.load(seq_offsets + off_b)
    seq_end = tl.load(seq_offsets + off_b + 1)
    seq_len = seq_end - seq_start
    start_n = off_n * BLOCK_N
    if start_n >= seq_len:
        return
    Jagged += seq_start * stride_jn
    Dense += off_b * stride_db
    Out += seq_start * stride_on
    offs_n = start_n + tl.arange(0, BLOCK_N)
    offs_d = tl.arange(0, BLOCK_D)
    jagged_ptrs = Jagged + offs_n[:, None] * stride_jn + offs_d[None, :]
    dense_ptrs = Dense + offs_d
    out_ptrs = Out + offs_n[:, None] * stride_jn + offs_d[None, :]
    for d in range(0, D, BLOCK_D):
        jg = tl.load(
            jagged_ptrs,
            # pyre-fixme[16]: `int` has no attribute `__getitem__`.
            mask=(offs_n[:, None] < seq_len) and (d + offs_d)[None, :] < D,
        )
        dn = tl.load(dense_ptrs, mask=d + offs_d < D)
        out = jg + dn[None, :]
        tl.store(
            out_ptrs,
            out,
            mask=(offs_n[:, None] < seq_len) and (d + offs_d)[None, :] < D,
        )
        dense_ptrs += BLOCK_D
        jagged_ptrs += BLOCK_D
        out_ptrs += BLOCK_D


@triton.jit
def jagged_reduce_sum(
    seq_offsets,
    Jagged,
    Out,
    D,
    stride_jn,
    stride_ob,
    BLOCK_D: tl.constexpr,
):
    """
    Computing Out = Jagged + Dense
    JaggedA has shape (sum_B(N_i), D), Dense has shape (B, D), and Out has shape (sum_B(N_i), D)
    """
    off_b = tl.program_id(0)
    off_d = tl.program_id(1) * BLOCK_D
    seq_start = tl.load(seq_offsets + off_b)
    seq_end = tl.load(seq_offsets + off_b + 1)
    seq_len = seq_end - seq_start
    Jagged += seq_start * stride_jn
    Out += off_b * stride_ob
    offs_d = off_d + tl.arange(0, BLOCK_D)
    jagged_ptrs = Jagged + offs_d
    out_ptrs = Out + offs_d
    accumulator = tl.zeros((BLOCK_D,), dtype=tl.float32)
    for _ in range(0, seq_len):
        jg = tl.load(
            jagged_ptrs,
            mask=offs_d < D,
        )
        accumulator += jg
        jagged_ptrs += stride_jn
    out = accumulator.to(Out.dtype.element_ty)
    tl.store(
        out_ptrs,
        out,
        mask=offs_d < D,
    )


class _JaggedDenseBroadcastAddFunction(torch.autograd.Function):
    @staticmethod
    # pyre-ignore[14]
    def forward(
        ctx,
        max_seq_len: int,
        seq_offsets: torch.Tensor,
        jagged: torch.Tensor,
        dense: torch.Tensor,
    ):
        jagged = switch_to_contiguous_if_needed(jagged)
        dense = switch_to_contiguous_if_needed(dense)
        L, D = jagged.shape
        B, _ = dense.shape
        out = torch.empty_like(jagged)

        grid = lambda meta: (  # noqa E731
            B,
            triton.cdiv(max_seq_len, meta["BLOCK_N"]),
        )
        BLOCK_D = triton.next_power_of_2(D) if D < 64 else 64
        jagged_dense_broadcast_add_kernel[grid](
            seq_offsets=seq_offsets,
            Jagged=jagged,
            Dense=dense,
            Out=out,
            AUTOTUNE_MAX_SEQ_LEN=autotune_max_seq_len(max_seq_len),
            D=D,
            stride_jn=jagged.stride(0),
            stride_db=dense.stride(0),
            stride_on=out.stride(0),
            BLOCK_D=BLOCK_D,
        )

        ctx.save_for_backward(seq_offsets)
        ctx.max_seq_len = max_seq_len
        ctx.B = B
        ctx.D = D
        return out

    @staticmethod
    # pyre-ignore[14]
    def backward(
        ctx, d_out: torch.Tensor
    ) -> Tuple[None, None, torch.Tensor, torch.Tensor]:
        seq_offsets = ctx.saved_tensors[0]
        d_dense = torch.empty((ctx.B, ctx.D), device=d_out.device, dtype=d_out.dtype)
        BLOCK_D = triton.next_power_of_2(ctx.D) if ctx.D < 64 else 64
        jagged_reduce_sum[(ctx.B, triton.cdiv(ctx.D, BLOCK_D))](
            seq_offsets=seq_offsets,
            Jagged=d_out,
            Out=d_dense,
            D=ctx.D,
            stride_jn=d_out.stride(0),
            stride_ob=d_dense.stride(0),
            BLOCK_D=BLOCK_D,
        )
        return None, None, d_out, d_dense


class _JaggedDenseBmmBroadcastAddFunction(torch.autograd.Function):
    @staticmethod
    # pyre-ignore[14]
    def forward(
        ctx,
        max_seq_len: int,
        seq_offsets: torch.Tensor,
        jagged: torch.Tensor,
        dense: torch.Tensor,
        bias: torch.Tensor,
    ):
        jagged = switch_to_contiguous_if_needed(jagged)
        bias = switch_to_contiguous_if_needed(bias)
        L, K = jagged.shape
        B, _, N = dense.shape
        out = torch.empty((L, N), dtype=jagged.dtype, device=jagged.device)

        grid = lambda meta: (  # noqa E731
            triton.cdiv(N, meta["BLOCK_N"]),
            triton.cdiv(max_seq_len, meta["BLOCK_M"]),
            B,
        )

        jagged_dense_bmm_broadcast_add_kernel[grid](
            seq_offsets=seq_offsets,
            Jagged=jagged,
            Dense=dense,
            Bias=bias,
            Out=out,
            AUTOTUNE_MAX_SEQ_LEN=autotune_max_seq_len(max_seq_len),
            N=N,
            K=K,
            stride_jm=jagged.stride(0),
            stride_db=dense.stride(0),
            stride_dk=dense.stride(1),
            stride_dn=dense.stride(2),
            stride_bias_b=bias.stride(0),
            stride_om=out.stride(0),
            HAS_BIAS=True,
            ALLOW_TF32=torch.backends.cuda.matmul.allow_tf32,
        )

        ctx.save_for_backward(seq_offsets, jagged, dense)
        ctx.B = B
        ctx.max_seq_len = max_seq_len
        ctx.K = K
        ctx.N = N
        return out

    @staticmethod
    # pyre-ignore[14]
    def backward(
        ctx, d_out: torch.Tensor
    ) -> Tuple[None, None, torch.Tensor, torch.Tensor, torch.Tensor]:
        seq_offsets, jagged, dense = ctx.saved_tensors
        d_jagged = torch.empty_like(jagged)
        d_dense = torch.empty_like(dense)
        d_bias = torch.empty((ctx.B, ctx.N), device=d_out.device, dtype=d_out.dtype)

        grid = lambda meta: (  # noqa E731
            triton.cdiv(ctx.K, meta["BLOCK_N"]),
            triton.cdiv(ctx.max_seq_len, meta["BLOCK_M"]),
            ctx.B,
        )
        jagged_dense_bmm_broadcast_add_kernel[grid](
            seq_offsets=seq_offsets,
            Jagged=d_out,
            Dense=dense,
            Bias=None,
            Out=d_jagged,
            AUTOTUNE_MAX_SEQ_LEN=autotune_max_seq_len(ctx.max_seq_len),
            N=ctx.K,
            K=ctx.N,
            stride_jm=d_out.stride(0),
            stride_db=dense.stride(0),
            stride_dk=dense.stride(2),
            stride_dn=dense.stride(1),
            stride_bias_b=0,
            stride_om=d_jagged.stride(0),
            HAS_BIAS=False,
            ALLOW_TF32=torch.backends.cuda.matmul.allow_tf32,
        )

        grid = lambda meta: (  # noqa E731
            ctx.B,
            triton.cdiv(ctx.K, meta["BLOCK_M"]),
            triton.cdiv(ctx.N, meta["BLOCK_N"]),
        )
        _jagged_jagged_bmm_reduce_sum[grid](
            seq_offsets=seq_offsets,
            JaggedA=jagged,
            JaggedB=d_out,
            Out=d_dense,
            ReduceOut=d_bias,
            M=ctx.K,
            N=ctx.N,
            AUTOTUNE_MAX_SEQ_LEN=autotune_max_seq_len(ctx.max_seq_len),
            stride_ak=jagged.stride(0),
            stride_bk=d_out.stride(0),
            stride_ob=d_dense.stride(0),
            stride_om=d_dense.stride(1),
            stride_on=d_dense.stride(2),
            stride_orb=d_bias.stride(0),
            stride_orn=d_bias.stride(1),
            REDUCE_JAGGEDB=True,
            ALLOW_TF32=torch.backends.cuda.matmul.allow_tf32,
        )

        return None, None, d_jagged, d_dense, d_bias


@triton.jit
def concat_2D_jagged_w_prefix(
    OffsetsA,
    ValuesA,
    OffsetsB,
    ValuesB,
    DenseSize,
    Out,
    D,
    stride_ad,
    stride_bd,
    stride_dense_batch,
    stride_od,
    n_prefix_from_B,  # nonzero is not supported when IS_REPLACE=True
    IS_DENSE_A: tl.constexpr,
    IS_DENSE_B: tl.constexpr,
    BLOCK_D: tl.constexpr,
    IS_REPLACE: tl.constexpr,
):
    off_z = tl.program_id(1)
    off_n = tl.program_id(0)
    if IS_DENSE_A:
        seq_start_a = off_z * DenseSize
        seq_len_a = DenseSize
        seq_start_b = tl.load(OffsetsB + off_z)
        seq_end_b = tl.load(OffsetsB + off_z + 1)
        seq_len_b = seq_end_b - seq_start_b
    elif IS_DENSE_B:
        seq_start_a = tl.load(OffsetsA + off_z)
        seq_end_a = tl.load(OffsetsA + off_z + 1)
        seq_len_a = seq_end_a - seq_start_a
        seq_start_b = off_z * DenseSize
        seq_len_b = DenseSize
    else:
        seq_start_a = tl.load(OffsetsA + off_z)
        seq_end_a = tl.load(OffsetsA + off_z + 1)
        seq_len_a = seq_end_a - seq_start_a
        seq_start_b = tl.load(OffsetsB + off_z)
        seq_end_b = tl.load(OffsetsB + off_z + 1)
        seq_len_b = seq_end_b - seq_start_b

    if IS_REPLACE:
        seq_len = seq_len_a
    else:
        seq_len = seq_len_a + seq_len_b
    if off_n >= seq_len:
        return

    offs_d = tl.arange(0, BLOCK_D)
    if IS_REPLACE:
        out_seq_start = seq_start_a + off_n
        out_seq_b_start = seq_len_a - seq_len_b
    else:
        out_seq_start = seq_start_a + seq_start_b + off_n
        out_seq_b_start = seq_len_a + n_prefix_from_B

    out_ptrs = Out + out_seq_start.to(tl.int64) * stride_od + offs_d
    if off_n < out_seq_b_start and off_n >= n_prefix_from_B:
        off_a = off_n - n_prefix_from_B
        if IS_DENSE_A:
            in_ptrs = (
                ValuesA
                + off_a.to(tl.int64) * stride_ad
                + off_z.to(tl.int64) * stride_dense_batch
                + offs_d
            )
        else:
            in_ptrs = ValuesA + (off_a + seq_start_a).to(tl.int64) * stride_ad + offs_d
    else:
        off_b = off_n - out_seq_b_start + n_prefix_from_B
        if off_n < n_prefix_from_B:
            off_b += out_seq_b_start - n_prefix_from_B
        if IS_DENSE_B:
            in_ptrs = (
                ValuesB
                + off_b.to(tl.int64) * stride_bd
                + off_z.to(tl.int64) * stride_dense_batch
                + offs_d
            )
        else:
            in_ptrs = ValuesB + (off_b + seq_start_b).to(tl.int64) * stride_bd + offs_d
    v = tl.load(in_ptrs, mask=offs_d < D)
    tl.store(out_ptrs, v, mask=offs_d < D)


@triton.jit
def concat_2D_jagged(
    OffsetsA,
    ValuesA,
    OffsetsB,
    ValuesB,
    DenseSize,
    Out,
    D,
    stride_ad,
    stride_bd,
    stride_dense_batch,
    stride_od,
    IS_DENSE_A: tl.constexpr,
    IS_DENSE_B: tl.constexpr,
    BLOCK_D: tl.constexpr,
    IS_REPLACE: tl.constexpr,
):
    concat_2D_jagged_w_prefix(
        OffsetsA,
        ValuesA,
        OffsetsB,
        ValuesB,
        DenseSize,
        Out,
        D,
        stride_ad,
        stride_bd,
        stride_dense_batch,
        stride_od,
        0,
        IS_DENSE_A,
        IS_DENSE_B,
        BLOCK_D,
        IS_REPLACE,
    )


@triton.jit
def concat_2D_jagged_jagged_w_prefix(
    OffsetsA,
    ValuesA,
    OffsetsB,
    ValuesB,
    Out,
    D,
    stride_ad,
    stride_bd,
    stride_od,
    n_prefix_from_B,
    BLOCK_D: tl.constexpr,
):
    concat_2D_jagged_w_prefix(
        OffsetsA,
        ValuesA,
        OffsetsB,
        ValuesB,
        0,
        Out,
        D,
        stride_ad,
        stride_bd,
        0,
        stride_od,
        n_prefix_from_B,
        IS_DENSE_A=False,
        IS_DENSE_B=False,
        BLOCK_D=BLOCK_D,
        IS_REPLACE=False,
    )


@triton.jit
def split_2D_jagged_w_prefix(
    JaggedIn,
    DenseSize,
    OffsetsA,
    OffsetsB,
    OutA,
    OutB,
    D,
    stride_id,
    stride_ad,
    stride_bd,
    n_prefix_to_B,
    IS_DENSE_A: tl.constexpr,
    IS_DENSE_B: tl.constexpr,
    BLOCK_D: tl.constexpr,
    IS_REPLACE: tl.constexpr,
):
    off_z = tl.program_id(1)
    off_n = tl.program_id(0)
    if IS_DENSE_A:
        seq_start_b = tl.load(OffsetsB + off_z)
        seq_end_b = tl.load(OffsetsB + off_z + 1)
        seq_start_a = off_z * DenseSize
        seq_len_a = DenseSize
        seq_len_b = seq_end_b - seq_start_b
    elif IS_DENSE_B:
        seq_start_a = tl.load(OffsetsA + off_z)
        seq_end_a = tl.load(OffsetsA + off_z + 1)
        seq_len_a = seq_end_a - seq_start_a
        seq_start_b = off_z * DenseSize
        seq_len_b = DenseSize
    else:
        seq_start_a = tl.load(OffsetsA + off_z)
        seq_end_a = tl.load(OffsetsA + off_z + 1)
        seq_len_a = seq_end_a - seq_start_a
        seq_start_b = tl.load(OffsetsB + off_z)
        seq_end_b = tl.load(OffsetsB + off_z + 1)
        seq_len_b = seq_end_b - seq_start_b
    if IS_REPLACE:
        seq_len = seq_len_a
    else:
        seq_len = seq_len_a + seq_len_b
    if off_n >= seq_len:
        return

    if IS_REPLACE:
        seq_start = seq_start_a
        out_seq_b_start = seq_len_a - seq_len_b
    else:
        seq_start = seq_start_a + seq_start_b
        out_seq_b_start = seq_len_a + n_prefix_to_B

    offs_d = tl.arange(0, BLOCK_D)
    in_ptrs = JaggedIn + (seq_start + off_n).to(tl.int64) * stride_id + offs_d
    if off_n < out_seq_b_start and off_n >= n_prefix_to_B:
        off_a = off_n - n_prefix_to_B
        out_ptrs = OutA + (off_a + seq_start_a).to(tl.int64) * stride_ad + offs_d
    else:
        off_b = off_n - out_seq_b_start + n_prefix_to_B
        if off_n < n_prefix_to_B:
            off_b += out_seq_b_start - n_prefix_to_B
        out_ptrs = OutB + (off_b + seq_start_b).to(tl.int64) * stride_bd + offs_d
    v = tl.load(in_ptrs, mask=offs_d < D)
    tl.store(out_ptrs, v, mask=offs_d < D)


@triton.jit
def split_2D_jagged(
    JaggedIn,
    DenseSize,
    OffsetsA,
    OffsetsB,
    OutA,
    OutB,
    D,
    stride_id,
    stride_ad,
    stride_bd,
    IS_DENSE_A: tl.constexpr,
    IS_DENSE_B: tl.constexpr,
    BLOCK_D: tl.constexpr,
    IS_REPLACE: tl.constexpr,
):
    split_2D_jagged_w_prefix(
        JaggedIn,
        DenseSize,
        OffsetsA,
        OffsetsB,
        OutA,
        OutB,
        D,
        stride_id,
        stride_ad,
        stride_bd,
        0,
        IS_DENSE_A,
        IS_DENSE_B,
        BLOCK_D,
        IS_REPLACE,
    )


@triton.jit
def split_2D_jagged_jagged_w_prefix(
    JaggedIn,
    OffsetsA,
    OffsetsB,
    OutA,
    OutB,
    D,
    stride_id,
    stride_ad,
    stride_bd,
    n_prefix_to_B,
    BLOCK_D: tl.constexpr,
):
    split_2D_jagged_w_prefix(
        JaggedIn,
        0,
        OffsetsA,
        OffsetsB,
        OutA,
        OutB,
        D,
        stride_id,
        stride_ad,
        stride_bd,
        n_prefix_to_B,
        IS_DENSE_A=False,
        IS_DENSE_B=False,
        BLOCK_D=BLOCK_D,
        IS_REPLACE=False,
    )


class _Concat2DJaggedFunction(torch.autograd.Function):
    @staticmethod
    # pyre-ignore[14]
    def forward(
        ctx,
        max_seq_len: int,
        values_a: torch.Tensor,
        values_b: torch.Tensor,
        offsets_a: Optional[torch.Tensor] = None,
        offsets_b: Optional[torch.Tensor] = None,
        is_replace: bool = False,
        n_prefix_from_right: int = 0,
    ):
        values_a = switch_to_contiguous_if_needed(values_a)
        values_b = switch_to_contiguous_if_needed(values_b)
        is_dense_a = offsets_a is None
        is_dense_b = offsets_b is None
        dense_size: int = 0
        if is_dense_a:
            assert offsets_b is not None
            B, dense_size, D = values_a.shape
            seq_len_a = dense_size * B
            seq_len_b, _ = values_b.shape
            device = values_b.device
            dtype = values_b.dtype
            stride_dense_batch = values_a.stride(0)
        elif is_dense_b:
            assert offsets_a is not None
            B, dense_size, D = values_b.shape
            seq_len_a, _ = values_a.shape
            seq_len_b = dense_size * B
            device = values_a.device
            dtype = values_a.dtype
            stride_dense_batch = values_b.stride(0)
        else:
            assert offsets_a is not None and offsets_b is not None
            B = offsets_a.shape[0] - 1
            seq_len_a, D = values_a.shape
            seq_len_b, _ = values_b.shape
            device = values_a.device
            dtype = values_a.dtype
            stride_dense_batch = 0

        BLOCK_D = triton.next_power_of_2(D)
        if is_replace:
            values_out = torch.empty_like(values_a)
        else:
            values_out = torch.empty(
                (seq_len_a + seq_len_b, D), device=device, dtype=dtype
            )
        if n_prefix_from_right == 0:
            concat_2D_jagged[(max_seq_len, B)](
                OffsetsA=offsets_a,
                ValuesA=values_a,
                OffsetsB=offsets_b,
                ValuesB=values_b,
                DenseSize=dense_size,
                Out=values_out,
                D=D,
                stride_ad=values_a.stride(-2),
                stride_bd=values_b.stride(-2),
                stride_dense_batch=stride_dense_batch,
                stride_od=values_out.stride(0),
                IS_DENSE_A=is_dense_a,  # pyre-ignore[6]
                IS_DENSE_B=is_dense_b,  # pyre-ignore[6]
                BLOCK_D=BLOCK_D,
                IS_REPLACE=is_replace,  # pyre-ignore[6]
            )
        else:
            concat_2D_jagged_jagged_w_prefix[(max_seq_len, B)](
                OffsetsA=offsets_a,
                ValuesA=values_a,
                OffsetsB=offsets_b,
                ValuesB=values_b,
                Out=values_out,
                D=D,
                stride_ad=values_a.stride(-2),
                stride_bd=values_b.stride(-2),
                stride_od=values_out.stride(0),
                n_prefix_from_B=n_prefix_from_right,
                BLOCK_D=BLOCK_D,
            )
        ctx.save_for_backward(offsets_a, offsets_b)
        ctx.max_seq_len = max_seq_len
        ctx.seq_len_a = seq_len_a
        ctx.seq_len_b = seq_len_b
        ctx.is_dense_a = is_dense_a
        ctx.is_dense_b = is_dense_b
        ctx.dense_size = dense_size
        ctx.is_replace = is_replace
        ctx.n_prefix_from_right = n_prefix_from_right
        return values_out

    @staticmethod
    # pyre-ignore[14]
    def backward(
        ctx, d_out: torch.Tensor
    ) -> Tuple[None, torch.Tensor, torch.Tensor, None, None, None, None]:
        offsets_a, offsets_b = ctx.saved_tensors
        is_dense_a, is_dense_b, is_replace = (
            ctx.is_dense_a,
            ctx.is_dense_b,
            ctx.is_replace,
        )
        dense_size = ctx.dense_size
        if is_dense_a:
            B = offsets_b.shape[0] - 1
        else:
            B = offsets_a.shape[0] - 1
        _, D = d_out.shape
        BLOCK_D = triton.next_power_of_2(D)
        values_a = torch.zeros(
            (ctx.seq_len_a, D), device=d_out.device, dtype=d_out.dtype
        )
        values_b = torch.empty(
            (ctx.seq_len_b, D), device=d_out.device, dtype=d_out.dtype
        )
        if ctx.n_prefix_from_right == 0:
            split_2D_jagged[(ctx.max_seq_len, B)](
                JaggedIn=d_out,
                DenseSize=dense_size,
                OffsetsA=offsets_a,
                OffsetsB=offsets_b,
                OutA=values_a,
                OutB=values_b,
                D=D,
                stride_id=d_out.stride(0),
                stride_ad=values_a.stride(0),
                stride_bd=values_b.stride(0),
                BLOCK_D=BLOCK_D,
                IS_DENSE_A=is_dense_a,
                IS_DENSE_B=is_dense_b,
                IS_REPLACE=is_replace,
            )
        else:
            split_2D_jagged_jagged_w_prefix[(ctx.max_seq_len, B)](
                JaggedIn=d_out,
                OffsetsA=offsets_a,
                OffsetsB=offsets_b,
                OutA=values_a,
                OutB=values_b,
                D=D,
                stride_id=d_out.stride(0),
                stride_ad=values_a.stride(0),
                stride_bd=values_b.stride(0),
                n_prefix_to_B=ctx.n_prefix_from_right,
                BLOCK_D=BLOCK_D,
            )

        if is_dense_a:
            values_a = values_a.reshape((B, dense_size, D))
        elif is_dense_b:
            values_b = values_b.reshape((B, dense_size, D))
        return None, values_a, values_b, None, None, None, None


class _Split2DJaggedFunction(torch.autograd.Function):
    @staticmethod
    # pyre-ignore[14]
    def forward(
        ctx,
        values: torch.Tensor,
        max_seq_len: int,
        offsets_a: Optional[torch.Tensor] = None,
        offsets_b: Optional[torch.Tensor] = None,
        dense_size: int = 0,
        n_prefix_to_right: int = 0,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        values = switch_to_contiguous_if_needed(values)
        is_dense_a: bool = offsets_a is None
        is_dense_b: bool = offsets_b is None
        if is_dense_a:
            L, _ = values.shape
            assert offsets_b is not None
            B = offsets_b.shape[0] - 1
            seq_len_a = dense_size * B
            seq_len_b = L - seq_len_a
            offsets_a = offsets_b.new_empty(0)
        elif is_dense_b:
            L, _ = values.shape
            assert offsets_a is not None
            B = offsets_a.shape[0] - 1
            seq_len_b = dense_size * B
            seq_len_a = L - seq_len_b
            offsets_b = offsets_a.new_empty(0)
        else:
            assert offsets_a is not None and offsets_b is not None
            B = offsets_a.shape[0] - 1
            seq_len_a = int(offsets_a[-1].item())
            seq_len_b = int(offsets_b[-1].item())
        _, D = values.shape
        BLOCK_D = triton.next_power_of_2(D)
        values_a = torch.empty((seq_len_a, D), device=values.device, dtype=values.dtype)
        values_b = torch.empty((seq_len_b, D), device=values.device, dtype=values.dtype)
        if n_prefix_to_right == 0:
            split_2D_jagged[(max_seq_len, B)](
                JaggedIn=values,
                DenseSize=dense_size,
                OffsetsA=offsets_a,
                OffsetsB=offsets_b,
                OutA=values_a,
                OutB=values_b,
                D=D,
                stride_id=values.stride(0),
                stride_ad=values_a.stride(0),
                stride_bd=values_b.stride(0),
                IS_DENSE_A=is_dense_a,  # pyre-ignore[6]
                IS_DENSE_B=is_dense_b,  # pyre-ignore[6]
                BLOCK_D=BLOCK_D,
                IS_REPLACE=False,  # pyre-ignore[6]
            )
        else:
            split_2D_jagged_jagged_w_prefix[(max_seq_len, B)](
                JaggedIn=values,
                OffsetsA=offsets_a,
                OffsetsB=offsets_b,
                OutA=values_a,
                OutB=values_b,
                D=D,
                stride_id=values.stride(0),
                stride_ad=values_a.stride(0),
                stride_bd=values_b.stride(0),
                n_prefix_to_B=n_prefix_to_right,
                BLOCK_D=BLOCK_D,
            )
        if is_dense_a:
            values_a = values_a.reshape(B, dense_size, D)
        if is_dense_b:
            values_b = values_b.reshape(B, dense_size, D)
        ctx.save_for_backward(offsets_a, offsets_b)
        ctx.max_seq_len = max_seq_len
        ctx.seq_len_a = seq_len_a
        ctx.seq_len_b = seq_len_b
        ctx.is_dense_a = is_dense_a
        ctx.is_dense_b = is_dense_b
        ctx.dense_size = dense_size
        ctx.B = B
        ctx.D = D
        ctx.n_prefix_to_right = n_prefix_to_right
        return values_a, values_b

    @staticmethod
    def backward(ctx, *d_values) -> Tuple[torch.Tensor, None, None, None, None, None]:
        offsets_a, offsets_b = ctx.saved_tensors
        is_dense_a, is_dense_b = ctx.is_dense_a, ctx.is_dense_b
        values_a, values_b = d_values
        if is_dense_a:
            stride_dense_batch = values_a.stride(0)
        elif is_dense_b:
            stride_dense_batch = values_b.stride(0)
        else:
            stride_dense_batch = 0

        BLOCK_D = triton.next_power_of_2(ctx.D)
        dvalues = torch.empty(
            (ctx.seq_len_a + ctx.seq_len_b, ctx.D),
            device=values_a.device,
            dtype=values_b.dtype,
        )
        if ctx.n_prefix_to_right == 0:
            concat_2D_jagged[(ctx.max_seq_len, ctx.B)](
                OffsetsA=offsets_a,
                ValuesA=values_a,
                OffsetsB=offsets_b,
                ValuesB=values_b,
                DenseSize=ctx.dense_size,
                Out=dvalues,
                D=ctx.D,
                stride_ad=values_a.stride(-2),
                stride_bd=values_b.stride(-2),
                stride_dense_batch=stride_dense_batch,
                stride_od=dvalues.stride(0),
                IS_DENSE_A=is_dense_a,
                IS_DENSE_B=is_dense_b,
                BLOCK_D=BLOCK_D,
                IS_REPLACE=False,  # pyre-ignore[6]
            )
        else:
            concat_2D_jagged_jagged_w_prefix[(ctx.max_seq_len, ctx.B)](
                OffsetsA=offsets_a,
                ValuesA=values_a,
                OffsetsB=offsets_b,
                ValuesB=values_b,
                Out=dvalues,
                D=ctx.D,
                stride_ad=values_a.stride(-2),
                stride_bd=values_b.stride(-2),
                stride_od=dvalues.stride(0),
                n_prefix_from_B=ctx.n_prefix_to_right,
                BLOCK_D=BLOCK_D,
            )

        return dvalues, None, None, None, None, None


@torch.fx.wrap
def triton_jagged_dense_bmm_broadcast_add(
    max_seq_len: int,
    seq_offsets: torch.Tensor,
    jagged: torch.Tensor,
    dense: torch.Tensor,
    bias: torch.Tensor,
) -> torch.Tensor:
    return _JaggedDenseBmmBroadcastAddFunction.apply(
        max_seq_len, seq_offsets, jagged, dense, bias
    )


@torch.fx.wrap
def triton_concat_2D_jagged(
    max_seq_len: int,
    values_a: torch.Tensor,
    values_b: torch.Tensor,
    offsets_a: Optional[torch.Tensor] = None,
    offsets_b: Optional[torch.Tensor] = None,
    is_replace: bool = False,
    n_prefix_from_right: int = 0,
) -> torch.Tensor:
    return _Concat2DJaggedFunction.apply(
        max_seq_len,
        values_a,
        values_b,
        offsets_a,
        offsets_b,
        is_replace,
        n_prefix_from_right,
    )


@torch.fx.wrap
def triton_concat_2D_jagged_jagged(
    max_seq_len_left: int,
    offsets_left: torch.Tensor,
    values_left: torch.Tensor,
    max_seq_len_right: int,
    offsets_right: torch.Tensor,
    values_right: torch.Tensor,
    is_replace: bool,
    n_prefix_from_right: int,
) -> torch.Tensor:
    return triton_concat_2D_jagged(
        max_seq_len=max_seq_len_left + max_seq_len_right,
        values_a=values_left,
        values_b=values_right,
        offsets_a=offsets_left,
        offsets_b=offsets_right,
        is_replace=is_replace,
        n_prefix_from_right=n_prefix_from_right,
    )


@torch.fx.wrap
def triton_concat_2D_dense_jagged(
    jagged_max_seq_len: int,
    jagged_offsets: torch.Tensor,
    jagged_values: torch.Tensor,
    dense_values: torch.Tensor,
) -> torch.Tensor:
    B, dense_size, D = dense_values.size()
    max_seq_len = jagged_max_seq_len + dense_size
    return triton_concat_2D_jagged(
        max_seq_len=max_seq_len,
        values_a=dense_values,
        values_b=jagged_values,
        offsets_a=None,
        offsets_b=jagged_offsets,
    )


def triton_jagged_dense_bmm(
    max_seq_len: int,
    seq_offsets: torch.Tensor,
    jagged: torch.Tensor,
    dense: torch.Tensor,
) -> torch.Tensor:
    return _JaggedDenseBmmFunction.apply(max_seq_len, seq_offsets, jagged, dense)


def triton_split_2D_jagged(
    values: torch.Tensor,
    max_seq_len: int,
    offsets_a: Optional[torch.Tensor] = None,
    offsets_b: Optional[torch.Tensor] = None,
    dense_size: int = 0,
    n_prefix_to_right: int = 0,
) -> Tuple[torch.Tensor, torch.Tensor]:
    return _Split2DJaggedFunction.apply(
        values,
        max_seq_len,
        offsets_a,
        offsets_b,
        dense_size,
        n_prefix_to_right,
    )


def triton_jagged_dense_broadcast_add(
    max_seq_len: int,
    seq_offsets: torch.Tensor,
    jagged: torch.Tensor,
    dense: torch.Tensor,
) -> torch.Tensor:
    return _JaggedDenseBroadcastAddFunction.apply(
        max_seq_len, seq_offsets, jagged, dense
    )

</content>

<content full_path="generative_recommenders/ops/cpp/sort_kv_pairs_cuda_kernels_template.h">
#pragma once

#include <ATen/ATen.h>
#include <optional>

namespace hstu {

template <typename key_t, typename val_t>
std::tuple<at::Tensor, at::Tensor> sort_kv_pairs_cuda_dispatched(
    const at::Tensor& keys_contig,
    const at::Tensor& values_contig,
    const std::optional<int64_t>& end_bit,
    const bool descending);

} // namespace hstu

</content>

<content full_path="generative_recommenders/ops/cpp/concat_1d_jagged_jagged.cu">
/* Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#include <ATen/ATen.h>
#include <ATen/Dispatch.h>
#include <ATen/core/op_registration/op_registration.h>
#include <ATen/cuda/CUDAContext.h>
#include <ATen/cuda/Exceptions.h>
#include <c10/cuda/CUDAGuard.h>
#include <torch/library.h>

#include "common.h"
#include "fbgemm_gpu/sparse_ops.h" // @manual
#include "fbgemm_gpu/utils/fixed_divisor.cuh" // @manual

namespace hstu {

static constexpr int32_t kMaxThreads = 1024;

template <typename index_t, typename val_t>
__global__
__launch_bounds__(kMaxThreads) void _concat_1d_jagged_jagged_cuda_kernel(
    int32_t B,
    const at::PackedTensorAccessor32<index_t, 1, at::RestrictPtrTraits>
        offsets_left,
    const at::PackedTensorAccessor32<val_t, 1, at::RestrictPtrTraits>
        values_left,
    const at::PackedTensorAccessor32<index_t, 1, at::RestrictPtrTraits>
        offsets_right,
    const at::PackedTensorAccessor32<val_t, 1, at::RestrictPtrTraits>
        values_right,
    at::PackedTensorAccessor32<val_t, 1, at::RestrictPtrTraits>
        combined_values) {
  for (int32_t b = blockIdx.x * blockDim.y + threadIdx.y; b < B;
       b += gridDim.x * blockDim.y) {
    auto left_start = offsets_left[b];
    auto left_len = offsets_left[b + 1] - left_start;
    auto right_start = offsets_right[b];
    auto right_len = offsets_right[b + 1] - right_start;
    auto combined_start = left_start + right_start;
    for (int32_t i = threadIdx.x; i < left_len + right_len; i += blockDim.x) {
      if (i < left_len) {
        combined_values[combined_start + i] = values_left[left_start + i];
      } else {
        combined_values[combined_start + i] =
            values_right[right_start + i - left_len];
      }
    }
  }
}

at::Tensor concat_1d_jagged_jagged_cuda(
    const at::Tensor& lengths_left,
    const at::Tensor& values_left,
    const at::Tensor& lengths_right,
    const at::Tensor& values_right) {
  at::cuda::OptionalCUDAGuard device_guard;
  device_guard.set_index(values_left.get_device());
  TORCH_INTERNAL_ASSERT(lengths_left.device().type() == at::DeviceType::CUDA);
  TORCH_INTERNAL_ASSERT(values_left.device().type() == at::DeviceType::CUDA);
  TORCH_INTERNAL_ASSERT(lengths_right.device().type() == at::DeviceType::CUDA);
  TORCH_INTERNAL_ASSERT(values_right.device().type() == at::DeviceType::CUDA);
  auto L = values_left.numel() + values_right.numel();
  TORCH_CHECK(L < std::numeric_limits<int32_t>::max());
  TORCH_CHECK(values_left.get_device() == lengths_left.get_device());
  TORCH_CHECK(values_left.get_device() == lengths_right.get_device());
  TORCH_CHECK(values_left.get_device() == values_right.get_device());
  auto B = lengths_left.size(0);
  auto combined_values = at::empty({L}, values_left.options());
  if (L == 0) {
    return combined_values;
  }
  const auto offsets_left =
      fbgemm_gpu::asynchronous_complete_cumsum_gpu(lengths_left.view({-1}));
  const auto offsets_right =
      fbgemm_gpu::asynchronous_complete_cumsum_gpu(lengths_right.view({-1}));
  uint32_t B_blocks = 32;
  dim3 threads(32, B_blocks);
  auto blocks = div_round_up(B, B_blocks);
  AT_DISPATCH_INTEGRAL_TYPES(
      lengths_left.scalar_type(),
      "concat_1d_jagged_jagged_values_cuda_kernel_input1",
      [&] {
        using index_t = scalar_t;
        AT_DISPATCH_ALL_TYPES_AND2(
            at::ScalarType::BFloat16,
            at::ScalarType::Half,
            values_left.scalar_type(),
            "concat_1d_jagged_jagged_values_cuda_kernel_input2",
            [&] {
              using val_t = scalar_t;
              _concat_1d_jagged_jagged_cuda_kernel<index_t, val_t>
                  <<<blocks, threads, 0, at::cuda::getCurrentCUDAStream()>>>(
                      B,
                      offsets_left.packed_accessor32<
                          index_t,
                          1,
                          at::RestrictPtrTraits>(),
                      values_left
                          .packed_accessor32<val_t, 1, at::RestrictPtrTraits>(),
                      offsets_right.packed_accessor32<
                          index_t,
                          1,
                          at::RestrictPtrTraits>(),
                      values_right
                          .packed_accessor32<val_t, 1, at::RestrictPtrTraits>(),
                      combined_values.packed_accessor32<
                          val_t,
                          1,
                          at::RestrictPtrTraits>());
            });
      });
  return combined_values;
}
} // namespace hstu

</content>

<content full_path="generative_recommenders/ops/cpp/concat_1d_jagged_jagged.cpp">
/* Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#include <ATen/ATen.h>
#include <ATen/Dispatch.h>
#include <ATen/core/op_registration/op_registration.h>
#include <ATen/cuda/CUDAContext.h>
#include <ATen/cuda/Exceptions.h>
#include <c10/cuda/CUDAGuard.h>
#include <torch/extension.h>
#include <torch/library.h>

#include "fbgemm_gpu/sparse_ops.h" // @manual

namespace hstu {

template <typename index_t, typename val_t>
void _concat_1d_jagged_jagged_cpu_kernel(
    int32_t B,
    const at::TensorAccessor<index_t, 1>& offsets_left,
    const at::TensorAccessor<val_t, 1>& values_left,
    const at::TensorAccessor<index_t, 1>& offsets_right,
    const at::TensorAccessor<val_t, 1>& values_right,
    at::TensorAccessor<val_t, 1> combined_values) {
  for (auto b : c10::irange(B)) {
    auto left_start = offsets_left[b];
    auto left_len = offsets_left[b + 1] - left_start;
    auto right_start = offsets_right[b];
    auto right_len = offsets_right[b + 1] - right_start;
    auto combined_start = left_start + right_start;
    for (auto i = 0; i < left_len; ++i) {
      combined_values[combined_start + i] = values_left[left_start + i];
    }
    for (auto i = 0; i < right_len; ++i) {
      combined_values[left_len + combined_start + i] =
          values_right[right_start + i];
    }
  }
}

at::Tensor concat_1d_jagged_jagged_cpu(
    const at::Tensor& lengths_left,
    const at::Tensor& values_left,
    const at::Tensor& lengths_right,
    const at::Tensor& values_right) {
  TORCH_INTERNAL_ASSERT(lengths_left.device().type() == at::DeviceType::CPU);
  TORCH_INTERNAL_ASSERT(values_left.device().type() == at::DeviceType::CPU);
  TORCH_INTERNAL_ASSERT(lengths_right.device().type() == at::DeviceType::CPU);
  TORCH_INTERNAL_ASSERT(values_right.device().type() == at::DeviceType::CPU);
  auto L = values_left.numel() + values_right.numel();
  TORCH_CHECK(L < std::numeric_limits<int32_t>::max());
  TORCH_CHECK(lengths_left.size(0) == lengths_right.size(0));
  auto B = lengths_left.size(0);
  auto combined_values = at::empty({L}, values_left.options());
  if (L == 0) {
    return combined_values;
  }
  const auto offsets_left =
      fbgemm_gpu::asynchronous_complete_cumsum_cpu(lengths_left.view({-1}));
  const auto offsets_right =
      fbgemm_gpu::asynchronous_complete_cumsum_cpu(lengths_right.view({-1}));
  AT_DISPATCH_INTEGRAL_TYPES(
      lengths_left.scalar_type(),
      "concat_1d_jagged_jagged_values_cpu_kernel_input1",
      [&] {
        using index_t = scalar_t;
        AT_DISPATCH_ALL_TYPES_AND2(
            at::ScalarType::BFloat16,
            at::ScalarType::Half,
            values_left.scalar_type(),
            "concat_1d_jagged_jagged_values_cpu_kernel_input2",
            [&] {
              using val_t = scalar_t;
              _concat_1d_jagged_jagged_cpu_kernel<index_t, val_t>(
                  B,
                  offsets_left.accessor<index_t, 1>(),
                  values_left.accessor<val_t, 1>(),
                  offsets_right.accessor<index_t, 1>(),
                  values_right.accessor<val_t, 1>(),
                  combined_values.accessor<val_t, 1>());
            });
      });
  return combined_values;
}

at::Tensor concat_1d_jagged_jagged_meta(
    const at::Tensor& lengths_left,
    const at::Tensor& values_left,
    const at::Tensor& lengths_right,
    const at::Tensor& values_right) {
  auto L = values_left.numel() + values_right.numel();
  return at::native::empty_meta_symint(
      {L},
      /*dtype=*/::std::make_optional(values_left.scalar_type()),
      /*layout=*/::std::make_optional(values_left.layout()),
      /*device=*/::std::make_optional(c10::Device(c10::kMeta)),
      /*pin_memory=*/::std::nullopt);
}
} // namespace hstu

</content>

<content full_path="generative_recommenders/ops/cpp/expand_1d_jagged_to_dense.cu">
/* Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#include <ATen/ATen.h>
#include <ATen/Dispatch.h>
#include <ATen/core/op_registration/op_registration.h>
#include <ATen/cuda/CUDAContext.h>
#include <ATen/cuda/Exceptions.h>
#include <c10/cuda/CUDAGuard.h>
#include <torch/library.h>

#include "common.h"

static constexpr int32_t kMaxThreads = 1024;

namespace hstu {

template <typename index_t, typename val_t>
__global__
__launch_bounds__(kMaxThreads) void expand_1d_jagged_to_dense_cuda_kernel_(
    int64_t B,
    int64_t max_len,
    const at::PackedTensorAccessor32<val_t, 1, at::RestrictPtrTraits> values,
    const at::PackedTensorAccessor32<index_t, 1, at::RestrictPtrTraits> offsets,
    at::PackedTensorAccessor32<val_t, 2, at::RestrictPtrTraits> output) {
  int64_t b = blockIdx.y;
  int64_t begin = offsets[b];
  int64_t i = blockIdx.x * blockDim.x + threadIdx.x;
  int64_t end = offsets[b + 1];
  if (end - begin == 0) {
    if (i < max_len) {
      output[b][i] = 0;
    }
  } else {
    if (i < std::min(end - begin, max_len)) {
      output[b][i] = values[i + begin];
    } else if (i < max_len) {
      output[b][i] = values[end - 1];
    }
  }
}

at::Tensor expand_1d_jagged_to_dense_cuda(
    const at::Tensor& values,
    const at::Tensor& offsets,
    const int64_t max_len) {
  at::cuda::OptionalCUDAGuard device_guard;
  device_guard.set_index(values.get_device());
  TORCH_INTERNAL_ASSERT(values.device().type() == at::DeviceType::CUDA);
  TORCH_INTERNAL_ASSERT(offsets.device().type() == at::DeviceType::CUDA);
  TORCH_CHECK(values.numel() < std::numeric_limits<int32_t>::max());
  TORCH_CHECK(values.get_device() == offsets.get_device());
  TORCH_CHECK(max_len > 0);
  auto B = offsets.size(0) - 1;
  auto output = at::empty({B, max_len}, values.options());
  if (values.numel() == 0) {
    return output;
  }
  uint32_t nthreads_per_block = max_len > 64 ? 64 : max_len;
  dim3 grid_size = dim3(div_round_up(max_len, nthreads_per_block), B);
  AT_DISPATCH_ALL_TYPES_AND2(
      at::ScalarType::BFloat16,
      at::ScalarType::Half,
      values.scalar_type(),
      "expand_1d_jagged_to_dense_cuda_input1",
      [&] {
        using val_t = scalar_t;
        AT_DISPATCH_INTEGRAL_TYPES(
            offsets.scalar_type(),
            "expand_1d_jagged_to_dense_cuda_input2",
            [&] {
              using index_t = scalar_t;
              expand_1d_jagged_to_dense_cuda_kernel_<index_t, val_t><<<
                  grid_size,
                  nthreads_per_block,
                  0,
                  at::cuda::getCurrentCUDAStream()>>>(
                  B,
                  max_len,
                  values.packed_accessor32<val_t, 1, at::RestrictPtrTraits>(),
                  offsets
                      .packed_accessor32<index_t, 1, at::RestrictPtrTraits>(),
                  output.packed_accessor32<val_t, 2, at::RestrictPtrTraits>());
              C10_CUDA_KERNEL_LAUNCH_CHECK();
            });
      });

  return output;
}

} // namespace hstu

</content>

<content full_path="generative_recommenders/ops/cpp/batched_complete_cumsum.cu">
/* Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#include <cub/block/block_scan.cuh>

#include "common.h"

static constexpr uint32_t kMaxThreads = 1024;

namespace hstu {

template <
    typename val_t,
    typename = std::enable_if_t<std::is_integral<val_t>::value>>
struct BlockPrefixCallbackOp {
  val_t running_total;

  __device__ BlockPrefixCallbackOp(val_t running_total)
      : running_total(running_total) {}

  __device__ val_t operator()(val_t block_aggregate) {
    val_t old_prefix = running_total;
    running_total += block_aggregate;
    return old_prefix;
  }
};

template <
    typename val_t,
    uint32_t nthreads_per_block,
    typename = std::enable_if_t<std::is_integral<val_t>::value>>
__global__ __launch_bounds__(kMaxThreads) void _batched_complete_cumsum_kernel(
    const at::PackedTensorAccessor64<val_t, 2, at::RestrictPtrTraits> values,
    const uint32_t len,
    const uint32_t items_per_thread,
    at::PackedTensorAccessor64<val_t, 2, at::RestrictPtrTraits> out) {
  using BlockScan = cub::BlockScan<val_t, nthreads_per_block>;
  __shared__ typename BlockScan::TempStorage temp_storage;

  BlockPrefixCallbackOp<val_t> prefix_op(0);
  if (threadIdx.x == 0) {
    out[blockIdx.x][0] = 0;
  }

  for (uint32_t offset = 0; offset < items_per_thread; offset++) {
    uint32_t i = offset * nthreads_per_block + threadIdx.x;
    val_t data = 0;
    if (i < len) {
      data = (val_t)values[blockIdx.x][i];
    }
    BlockScan(temp_storage).InclusiveSum(data, data, prefix_op);
    cub::CTA_SYNC();
    if (i < len) {
      out[blockIdx.x][i + 1] = data;
    }
  }
}

at::Tensor batched_complete_cumsum_cuda(const at::Tensor& values) {
  at::cuda::OptionalCUDAGuard device_guard;
  device_guard.set_index(values.get_device());

  TORCH_CHECK(values.dim() == 2, "values of batched_complete_cumsum must be 2")
  TORCH_CHECK(
      values.size(0) <= UINT32_MAX,
      "values.size(0) must be no higher than UINT32_MAX")
  TORCH_CHECK(
      values.size(1) <= UINT32_MAX,
      "values.size(1) must be no higher than UINT32_MAX")

  const uint32_t B = values.size(0);
  const uint32_t len = values.size(1);
  const uint32_t nthreads_per_block =
      min(max(next_power_of_2(len), 64), kMaxThreads);
  const uint32_t items_per_thread = div_round_up(len, nthreads_per_block);

  auto cumsum = at::empty({B, len + 1}, values.options());

  AT_DISPATCH_INTEGRAL_TYPES(
      values.scalar_type(), "batched_complete_cumsum_cuda_input1", [&] {
        using val_t = scalar_t;
        if (nthreads_per_block == 64) {
          _batched_complete_cumsum_kernel<val_t, 64>
              <<<B, 64, 0, at::cuda::getCurrentCUDAStream()>>>(
                  values.packed_accessor64<val_t, 2, at::RestrictPtrTraits>(),
                  len,
                  items_per_thread,
                  cumsum.packed_accessor64<val_t, 2, at::RestrictPtrTraits>());
        } else if (nthreads_per_block == 128) {
          _batched_complete_cumsum_kernel<val_t, 128>
              <<<B, 128, 0, at::cuda::getCurrentCUDAStream()>>>(
                  values.packed_accessor64<val_t, 2, at::RestrictPtrTraits>(),
                  len,
                  items_per_thread,
                  cumsum.packed_accessor64<val_t, 2, at::RestrictPtrTraits>());
        } else if (nthreads_per_block == 256) {
          _batched_complete_cumsum_kernel<val_t, 256>
              <<<B, 256, 0, at::cuda::getCurrentCUDAStream()>>>(
                  values.packed_accessor64<val_t, 2, at::RestrictPtrTraits>(),
                  len,
                  items_per_thread,
                  cumsum.packed_accessor64<val_t, 2, at::RestrictPtrTraits>());
        } else if (nthreads_per_block == 512) {
          _batched_complete_cumsum_kernel<val_t, 512>
              <<<B, 512, 0, at::cuda::getCurrentCUDAStream()>>>(
                  values.packed_accessor64<val_t, 2, at::RestrictPtrTraits>(),
                  len,
                  items_per_thread,
                  cumsum.packed_accessor64<val_t, 2, at::RestrictPtrTraits>());
        } else {
          _batched_complete_cumsum_kernel<val_t, 1024>
              <<<B, 1024, 0, at::cuda::getCurrentCUDAStream()>>>(
                  values.packed_accessor64<val_t, 2, at::RestrictPtrTraits>(),
                  len,
                  items_per_thread,
                  cumsum.packed_accessor64<val_t, 2, at::RestrictPtrTraits>());
        }
        C10_CUDA_KERNEL_LAUNCH_CHECK();
      });

  return cumsum;
}

} // namespace hstu

</content>

<content full_path="generative_recommenders/ops/cpp/sort_kv_pairs_cuda.cpp">
#include "common.h"
#include "sort_kv_pairs_cuda_kernels_template.h"

namespace hstu {

DLL_PUBLIC std::tuple<at::Tensor, at::Tensor> sort_kv_pairs_cuda(
    const at::Tensor& keys,
    const at::Tensor& values,
    const std::optional<int64_t>& end_bit,
    const bool descending = false) {
  at::cuda::OptionalCUDAGuard device_guard;
  device_guard.set_index(keys.get_device());
  TORCH_CHECK(
      keys.dtype() == at::kInt || keys.dtype() == at::kLong ||
      keys.dtype() == at::kByte || keys.dtype() == at::kShort);
  TORCH_CHECK(keys.numel() < std::numeric_limits<int32_t>::max());
  TORCH_CHECK(keys.dim() == 1);
  TORCH_CHECK(values.dim() == 1);
  at::Tensor sorted_keys;
  at::Tensor sorted_values;

  AT_DISPATCH_INTEGRAL_TYPES(keys.scalar_type(), "sort_pairs_cuda_input1", [&] {
    using key_t = scalar_t;
    AT_DISPATCH_ALL_TYPES_AND2(
        at::ScalarType::Half,
        at::ScalarType::BFloat16,
        values.scalar_type(),
        "sort_pairs_cuda_input2",
        [&] {
          using val_t = scalar_t;
          std::tie(sorted_keys, sorted_values) =
              sort_kv_pairs_cuda_dispatched<key_t, val_t>(
                  keys, values, end_bit, descending);
        });
  });

  return {std::move(sorted_keys), std::move(sorted_values)};
}

} // namespace hstu

</content>

<content full_path="generative_recommenders/ops/cpp/batched_complete_cumsum.cpp">
/* Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#include <ATen/ATen.h>
#include <ATen/core/op_registration/op_registration.h>
#include <c10/cuda/CUDAGuard.h>
#include <torch/extension.h>

#include "fbgemm_gpu/sparse_ops.h" // @manual

namespace hstu {

at::Tensor batched_complete_cumsum_cpu(const at::Tensor& values) {
  auto B = values.size(0);
  auto len = values.size(1);
  auto output = at::empty({B, len + 1}, values.options());
  const torch::Tensor index = at::range(0, len, at::kLong).cpu();
  for (auto i : c10::irange(B)) {
    torch::Tensor t = output[i];
    at::index_put_(
        t, {index}, fbgemm_gpu::asynchronous_complete_cumsum_cpu(values[i]));
  }
  return output;
}

at::Tensor batched_complete_cumsum_meta(const at::Tensor& values) {
  auto B = values.sym_size(0);
  auto len = values.sym_size(1);
  auto output = at::native::empty_meta_symint(
      {B, len + 1},
      /*dtype=*/::std::make_optional(values.scalar_type()),
      /*layout=*/::std::make_optional(values.layout()),
      /*device=*/::std::make_optional(c10::Device(c10::kMeta)),
      /*pin_memory=*/::std::nullopt);
  return output;
}

} // namespace hstu

</content>

<content full_path="generative_recommenders/ops/cpp/common.h">
/* Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#pragma once

#include <ATen/ATen.h>
#include <ATen/cuda/Exceptions.h>
#include <c10/cuda/CUDAGuard.h>

#define AT_DISPATCH_CASE_FLOATING_TYPES_AND4(                \
    SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, SCALARTYPE4, ...) \
  AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)               \
  AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)                 \
  AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)                 \
  AT_DISPATCH_CASE(SCALARTYPE3, __VA_ARGS__)                 \
  AT_DISPATCH_CASE(SCALARTYPE4, __VA_ARGS__)

#define AT_DISPATCH_FLOATING_TYPES_AND4(                                 \
    SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, SCALARTYPE4, TYPE, NAME, ...) \
  AT_DISPATCH_SWITCH(                                                    \
      TYPE,                                                              \
      NAME,                                                              \
      AT_DISPATCH_CASE_FLOATING_TYPES_AND4(                              \
          SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, SCALARTYPE4, __VA_ARGS__))

inline __attribute__((always_inline)) uint32_t
div_round_up(uint32_t a, uint32_t b) {
  return (a + b - 1) / b;
};

inline __attribute__((always_inline)) uint32_t next_power_of_2(uint32_t n) {
  n--;
  n |= n >> 1;
  n |= n >> 2;
  n |= n >> 4;
  n |= n >> 8;
  n |= n >> 16;
  n++;
  return n;
}

/*
 * Because different .SO may include the same CUDA CUB kernels, this results in
 * confusion, where libA may end up calling libB's cub kernel and causing
 * failures when we static link libcudart_static.a. To avoid this, we annotate
 * only the public functions and hide the rest.
 */
#define DLL_PUBLIC __attribute__((visibility("default")))

</content>

<content full_path="generative_recommenders/ops/cpp/cpp_ops.cpp">
/* Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#include <ATen/ATen.h>
#include <ATen/Dispatch.h>
#include <ATen/core/op_registration/op_registration.h>
#include <ATen/cuda/CUDAContext.h>
#include <ATen/cuda/Exceptions.h>
#include <c10/cuda/CUDAGuard.h>
#include <torch/extension.h>
#include <torch/library.h>

/*
 * Because different .SO may include the same CUDA CUB kernels, this results in
 * confusion, where libA may end up calling libB's cub kernel and causing
 * failures when we static link libcudart_static.a. To avoid this, we annotate
 * only the public functions and hide the rest.
 */
#define DLL_PUBLIC __attribute__((visibility("default")))

namespace hstu {
at::Tensor expand_1d_jagged_to_dense_cpu(
    const at::Tensor& values,
    const at::Tensor& offsets,
    const int64_t max_len);

at::Tensor expand_1d_jagged_to_dense_meta(
    const at::Tensor& values,
    const at::Tensor& offsets,
    const c10::SymInt max_len);

at::Tensor expand_1d_jagged_to_dense_cuda(
    const at::Tensor& values,
    const at::Tensor& offsets,
    const int64_t max_len);

at::Tensor batched_complete_cumsum_cpu(const at::Tensor& values);

at::Tensor batched_complete_cumsum_cuda(const at::Tensor& values);

at::Tensor batched_complete_cumsum_meta(const at::Tensor& values);

at::Tensor complete_cumsum_cpu(const at::Tensor& values);

at::Tensor complete_cumsum_cuda(const at::Tensor& values);

at::Tensor complete_cumsum_meta(const at::Tensor& values);

at::Tensor concat_1d_jagged_jagged_cpu(
    const at::Tensor& lengths_left,
    const at::Tensor& values_left,
    const at::Tensor& lengths_right,
    const at::Tensor& values_right);

at::Tensor concat_1d_jagged_jagged_cuda(
    const at::Tensor& lengths_left,
    const at::Tensor& values_left,
    const at::Tensor& lengths_right,
    const at::Tensor& values_right);

at::Tensor concat_1d_jagged_jagged_meta(
    const at::Tensor& lengths_left,
    const at::Tensor& values_left,
    const at::Tensor& lengths_right,
    const at::Tensor& values_right);

DLL_PUBLIC std::tuple<at::Tensor, at::Tensor> sort_kv_pairs_meta(
    const at::Tensor& keys,
    const at::Tensor& values,
    const std::optional<int64_t>& end_bit,
    const bool descending = false) {
  TORCH_CHECK(
      keys.dtype() == at::kInt || keys.dtype() == at::kLong ||
      keys.dtype() == at::kByte || keys.dtype() == at::kShort);
  TORCH_CHECK(keys.dim() == 1);
  TORCH_CHECK(values.dim() == 1);
  return {at::empty_like(keys), at::empty_like(values)};
}

std::tuple<at::Tensor, at::Tensor> sort_kv_pairs_cuda(
    const at::Tensor& keys,
    const at::Tensor& values,
    const std::optional<int64_t>& end_bit,
    const bool descending = false);

} // namespace hstu

TORCH_LIBRARY_FRAGMENT(hstu, m) {
  m.def(
      "expand_1d_jagged_to_dense(Tensor values, Tensor offsets, SymInt max_len) -> Tensor");
  m.def("batched_complete_cumsum(Tensor values) -> Tensor");
  m.def(
      "concat_1d_jagged_jagged(Tensor lengths_left, Tensor values_left, Tensor lengths_right, Tensor values_right) -> Tensor");
  m.def("complete_cumsum(Tensor values) -> Tensor");
  m.def(
      "sort_kv_pairs(Tensor keys, Tensor values, int? end_bit=None, bool descending=False) -> (Tensor, Tensor)");
}

TORCH_LIBRARY_IMPL(hstu, CPU, m) {
  m.impl("expand_1d_jagged_to_dense", hstu::expand_1d_jagged_to_dense_cpu);
  m.impl("batched_complete_cumsum", hstu::batched_complete_cumsum_cpu);
  m.impl("concat_1d_jagged_jagged", hstu::concat_1d_jagged_jagged_cpu);
  m.impl("complete_cumsum", hstu::complete_cumsum_cpu);
}

TORCH_LIBRARY_IMPL(hstu, CUDA, m) {
  m.impl("expand_1d_jagged_to_dense", hstu::expand_1d_jagged_to_dense_cuda);
  m.impl("batched_complete_cumsum", hstu::batched_complete_cumsum_cuda);
  m.impl("concat_1d_jagged_jagged", hstu::concat_1d_jagged_jagged_cuda);
  m.impl("complete_cumsum", hstu::complete_cumsum_cuda);
  m.impl(
      "sort_kv_pairs",
      torch::dispatch(
          c10::DispatchKey::CUDA, TORCH_FN(hstu::sort_kv_pairs_cuda)));
}

TORCH_LIBRARY_IMPL(hstu, Meta, m) {
  m.impl("expand_1d_jagged_to_dense", hstu::expand_1d_jagged_to_dense_meta);
  m.impl("batched_complete_cumsum", hstu::batched_complete_cumsum_meta);
  m.impl("concat_1d_jagged_jagged", hstu::concat_1d_jagged_jagged_meta);
  m.impl("complete_cumsum", hstu::complete_cumsum_meta);
  m.impl(
      "sort_kv_pairs",
      torch::dispatch(
          c10::DispatchKey::Meta, TORCH_FN(hstu::sort_kv_pairs_meta)));
}

TORCH_LIBRARY_IMPL(hstu, Autograd, m) {
  m.impl(
      "expand_1d_jagged_to_dense",
      torch::autograd::autogradNotImplementedFallback());
  m.impl(
      "batched_complete_cumsum",
      torch::autograd::autogradNotImplementedFallback());
  m.impl("complete_cumsum", torch::autograd::autogradNotImplementedFallback());
}

</content>

<content full_path="generative_recommenders/ops/cpp/complete_cumsum.cpp">
/* Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#include <ATen/ATen.h>
#include <ATen/core/op_registration/op_registration.h>
#include <c10/cuda/CUDAGuard.h>
#include <torch/extension.h>

#include "fbgemm_gpu/sparse_ops.h" // @manual

namespace hstu {

at::Tensor complete_cumsum_cpu(const at::Tensor& values) {
  TORCH_CHECK(values.dim() == 1);
  auto len = values.size(0);
  const torch::Tensor index = at::range(0, len, at::kLong).cpu();
  auto output = fbgemm_gpu::asynchronous_complete_cumsum_cpu(values);
  return output;
}

at::Tensor complete_cumsum_meta(const at::Tensor& values) {
  auto len = values.sym_size(0);
  auto output = at::native::empty_meta_symint(
      {len + 1},
      /*dtype=*/::std::make_optional(values.scalar_type()),
      /*layout=*/::std::make_optional(values.layout()),
      /*device=*/::std::make_optional(c10::Device(c10::kMeta)),
      /*pin_memory=*/::std::nullopt);
  return output;
}

} // namespace hstu

</content>

<content full_path="generative_recommenders/ops/cpp/complete_cumsum.cu">
#include "common.h"

#include <cub/device/device_scan.cuh>

namespace hstu {

DLL_PUBLIC at::Tensor complete_cumsum_cuda(const at::Tensor& values) {
  at::cuda::OptionalCUDAGuard device_guard;
  device_guard.set_index(values.get_device());

  TORCH_CHECK(values.numel() < std::numeric_limits<int32_t>::max());
  TORCH_CHECK(values.dim() == 1);
  const auto values_contig = values.contiguous();

  auto cumsum = at::empty({values_contig.numel() + 1}, values_contig.options());
  cumsum[0].zero_();

  AT_DISPATCH_FLOATING_TYPES_AND4(
      at::ScalarType::Int,
      at::ScalarType::Long,
      at::ScalarType::Half,
      at::ScalarType::BFloat16,
      values_contig.scalar_type(),
      "complete_cumsum_cuda",
      [&] {
        size_t temp_storage_bytes = 0;
        AT_CUDA_CHECK(cub::DeviceScan::InclusiveSum(
            nullptr,
            temp_storage_bytes,
            values_contig.data_ptr<scalar_t>(),
            cumsum.data_ptr<scalar_t>() + 1,
            values_contig.numel(),
            at::cuda::getCurrentCUDAStream()));
        auto temp_storage = at::empty(
            {static_cast<int64_t>(temp_storage_bytes)},
            values_contig.options().dtype(at::kByte));
        AT_CUDA_CHECK(cub::DeviceScan::InclusiveSum(
            temp_storage.data_ptr(),
            temp_storage_bytes,
            values_contig.data_ptr<scalar_t>(),
            cumsum.data_ptr<scalar_t>() + 1,
            values_contig.numel(),
            at::cuda::getCurrentCUDAStream()));
      });

  return cumsum;
}

} // namespace hstu

</content>

<content full_path="generative_recommenders/ops/cpp/expand_1d_jagged_to_dense.cpp">
/* Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#include <ATen/ATen.h>
#include <ATen/Dispatch.h>
#include <ATen/core/op_registration/op_registration.h>
#include <ATen/cuda/CUDAContext.h>
#include <ATen/cuda/Exceptions.h>
#include <c10/cuda/CUDAGuard.h>
#include <torch/extension.h>
#include <torch/library.h>

namespace hstu {

template <typename index_t, typename val_t>
void expand_1d_jagged_to_dense_cpu_kernel_(
    int64_t B,
    int64_t max_len,
    const at::TensorAccessor<val_t, 1>& values,
    const at::TensorAccessor<index_t, 1>& offsets,
    at::TensorAccessor<val_t, 2> output) {
  for (auto i : c10::irange(B)) {
    int64_t begin = offsets[i];
    int64_t end = offsets[i + 1];
    if (end - begin == 0) {
      for (int64_t j : c10::irange(max_len)) {
        output[i][j] = 0;
        continue;
      }
    } else {
      int64_t j = 0;
      for (; j < std::min(end - begin, max_len); ++j) {
        output[i][j] = values[begin + j];
      }
      for (; j < max_len; ++j) {
        output[i][j] = values[end - 1];
      }
    }
  } // for each i
}

at::Tensor expand_1d_jagged_to_dense_cpu(
    const at::Tensor& values,
    const at::Tensor& offsets,
    const int64_t max_len) {
  TORCH_INTERNAL_ASSERT(values.device().type() == at::DeviceType::CPU);
  TORCH_INTERNAL_ASSERT(offsets.device().type() == at::DeviceType::CPU);
  TORCH_CHECK(values.numel() < std::numeric_limits<int32_t>::max());
  TORCH_CHECK(max_len > 0);
  auto B = offsets.size(0) - 1;
  auto output = at::empty({B, max_len}, values.options());
  if (values.numel() == 0) {
    return output;
  }
  AT_DISPATCH_ALL_TYPES_AND2(
      at::ScalarType::BFloat16,
      at::ScalarType::Half,
      values.scalar_type(),
      "expand_1d_jagged_to_dense_cpu_input1",
      [&] {
        using val_t = scalar_t;
        AT_DISPATCH_INTEGRAL_TYPES(
            offsets.scalar_type(), "expand_1d_jagged_to_dense_cpu_input2", [&] {
              using index_t = scalar_t;
              expand_1d_jagged_to_dense_cpu_kernel_<index_t, val_t>(
                  B,
                  max_len,
                  values.accessor<val_t, 1>(),
                  offsets.accessor<index_t, 1>(),
                  output.accessor<val_t, 2>());
            });
      });
  return output;
}

at::Tensor expand_1d_jagged_to_dense_meta(
    const at::Tensor& values,
    const at::Tensor& offsets,
    const c10::SymInt max_len) {
  auto B = offsets.sym_size(0) - 1;
  auto output = at::empty_symint({B, max_len}, values.options());
  return output;
}

} // namespace hstu

</content>

<content full_path="generative_recommenders/ops/cpp/sort_kv_pairs_cuda_kernels_template.cu">
#include <generative_recommenders/ops/cpp/common.h>
#include <generative_recommenders/ops/cpp/sort_kv_pairs_cuda_kernels_template.h>

#include <cub/device/device_radix_sort.cuh>

namespace hstu {

template <>
DLL_PUBLIC std::tuple<at::Tensor, at::Tensor>
sort_kv_pairs_cuda_dispatched<SUB_KEY_T, SUB_VALUE_T>(
    const at::Tensor& keys,
    const at::Tensor& values,
    const std::optional<int64_t>& end_bit,
    const bool descending) {
  size_t temp_storage_bytes = 0;
  auto keys_contig = keys.contiguous();
  auto values_contig = values.contiguous();
  auto sorted_keys = at::empty_like(keys_contig);
  auto sorted_values = at::empty_like(values_contig);

  if (descending) {
    AT_CUDA_CHECK(cub::DeviceRadixSort::SortPairsDescending(
        nullptr,
        temp_storage_bytes,
        keys_contig.data_ptr<SUB_KEY_T>(),
        sorted_keys.data_ptr<SUB_KEY_T>(),
        values_contig.data_ptr<SUB_VALUE_T>(),
        sorted_values.data_ptr<SUB_VALUE_T>(),
        keys_contig.numel(),
        0,
        end_bit.has_value() ? end_bit.value() : sizeof(SUB_KEY_T) * 8,
        at::cuda::getCurrentCUDAStream(),
        false));
    auto temp_storage = at::empty(
        {static_cast<int64_t>(temp_storage_bytes)},
        keys_contig.options().dtype(at::kByte));
    AT_CUDA_CHECK(cub::DeviceRadixSort::SortPairsDescending(
        temp_storage.data_ptr(),
        temp_storage_bytes,
        keys_contig.data_ptr<SUB_KEY_T>(),
        sorted_keys.data_ptr<SUB_KEY_T>(),
        values_contig.data_ptr<SUB_VALUE_T>(),
        sorted_values.data_ptr<SUB_VALUE_T>(),
        keys_contig.numel(),
        0,
        end_bit.has_value() ? end_bit.value() : sizeof(SUB_KEY_T) * 8,
        at::cuda::getCurrentCUDAStream(),
        false));
  } else {
    AT_CUDA_CHECK(cub::DeviceRadixSort::SortPairs(
        nullptr,
        temp_storage_bytes,
        keys_contig.data_ptr<SUB_KEY_T>(),
        sorted_keys.data_ptr<SUB_KEY_T>(),
        values_contig.data_ptr<SUB_VALUE_T>(),
        sorted_values.data_ptr<SUB_VALUE_T>(),
        keys_contig.numel(),
        0,
        end_bit.has_value() ? end_bit.value() : sizeof(SUB_KEY_T) * 8,
        at::cuda::getCurrentCUDAStream(),
        false));
    auto temp_storage = at::empty(
        {static_cast<int64_t>(temp_storage_bytes)},
        keys_contig.options().dtype(at::kByte));
    AT_CUDA_CHECK(cub::DeviceRadixSort::SortPairs(
        temp_storage.data_ptr(),
        temp_storage_bytes,
        keys_contig.data_ptr<SUB_KEY_T>(),
        sorted_keys.data_ptr<SUB_KEY_T>(),
        values_contig.data_ptr<SUB_VALUE_T>(),
        sorted_values.data_ptr<SUB_VALUE_T>(),
        keys_contig.numel(),
        0,
        end_bit.has_value() ? end_bit.value() : sizeof(SUB_KEY_T) * 8,
        at::cuda::getCurrentCUDAStream(),
        false));
  }

  return {std::move(sorted_keys), std::move(sorted_values)};
}

} // namespace hstu

</content>

<content full_path="generative_recommenders/ops/benchmarks/hstu_attention_bench.py">
# pyre-strict
import os
from typing import List, Optional, Tuple

import click
import pandas as pd
import torch

# @manual=//triton:triton
import triton
from generative_recommenders.common import (
    apply_sampling,
    generate_sparse_seq_len,
    HammerKernel,
)
from generative_recommenders.ops.hstu_attention import cached_hstu_mha, hstu_mha


def _get_kernel(provider: str) -> HammerKernel:
    if provider == "triton":
        return HammerKernel.TRITON
    elif provider == "pytorch":
        return HammerKernel.PYTORCH
    else:
        raise ValueError(f"Unknown provider {provider}")


def _flops(
    batch_size: int,
    max_seqlen: int,
    attn_dim: int,
    hidden_dim: int,
    nheads: int,
    seq_offsets: torch.Tensor,
    mode: str = "fwd",
) -> float:
    assert mode in ["fwd", "bwd", "fwd_bwd"]
    ratio = 2.0  # triangular masking
    f1 = 0.0
    f2 = 0.0
    for i in range(batch_size):
        seq_len = int((seq_offsets[i + 1] - seq_offsets[i]).item())
        # (QK^T), dQ = d(QK^T)K, dK^T = Q^Td(QK^T)
        f1 += 2 * nheads * attn_dim * seq_len**2 // ratio
        # (QK^T)V, d(QK^T) = dOV^T, dV = (QK^T)^TdO,
        f2 += 2 * nheads * hidden_dim * seq_len**2 // ratio
    if mode == "fwd":
        return f1 + f2  # computes (QK^T) and (QK^T)V
    elif mode == "bwd":
        return 3 * f1 + 2 * f2  # computes (QK^T), dQ, dK, dV, d(QK^T)
    else:
        return 4 * f1 + 3 * f2


@click.command()
@click.option(
    "--batch-size",
    type=int,
    default=512,
)
@click.option("--heads", type=int, default=4)
@click.option("--attn-dim", type=int, default=128)
@click.option("--hidden-dim", type=int, default=128)
@click.option("--max-seq-len-log2", type=int, default=13)
@click.option("--data-type", type=str, default="bf16")
@click.option("--seq-sparsity", type=float, default=0.95)
@click.option("--has-delta-q", type=bool, default=False)
@click.option("--delta-size", type=int, default=256)
@click.option("--target-size", type=int, default=20)
@click.option("--bench-backward", type=bool, default=True)
@click.option("--bench-forward", type=bool, default=True)
@click.option("--bench-pytorch", type=bool, default=False)
@click.option("--report-flops", type=bool, default=False)
@click.option("--return-result", type=bool, default=False)
@click.option("--max-attn-len", type=int, default=0)
@click.option("--contextual-seq-len", type=int, default=0)
@click.option("--sampling-alpha", type=float, default=2.0)
def main(  # noqa: C901
    batch_size: int,
    heads: int,
    attn_dim: int,
    hidden_dim: int,
    max_seq_len_log2: int,
    data_type: str,
    seq_sparsity: float,
    has_delta_q: bool,
    delta_size: int,
    target_size: int,
    bench_backward: bool,
    bench_forward: bool,
    bench_pytorch: bool,
    report_flops: bool,
    return_result: bool,
    max_attn_len: int,
    contextual_seq_len: int,
    sampling_alpha: float,
) -> Optional[Tuple[List[triton.testing.Benchmark], List[pd.DataFrame]]]:
    torch.backends.cudnn.allow_tf32 = True
    torch.backends.cuda.matmul.allow_tf32 = True
    if data_type == "fp32":
        dtype = torch.float32
    elif data_type == "fp16":
        dtype = torch.float16
    elif data_type == "bf16":
        dtype = torch.bfloat16
    else:
        raise ValueError(f"Unsupported data type: {data_type}.")

    line_vals = ["triton"]
    line_names = ["Triton"]
    styles = [("red", "-")]
    if bench_pytorch:
        line_vals.append("pytorch")
        line_names.append("PyTorch")
        styles.append(("green", "-"))

    bench_backward = False if has_delta_q else bench_backward
    modes = []
    if bench_forward:
        modes.append("fwd")
    if bench_backward:
        modes.append("bwd")
    assert len(modes) > 0

    configs: List[triton.testing.Benchmark] = [
        triton.testing.Benchmark(
            x_names=["seq_len"],
            x_vals=[2**i for i in range(8, max_seq_len_log2)],
            line_arg="provider",
            line_vals=line_vals,
            line_names=line_names,
            styles=styles,
            ylabel="ms",
            plot_name=f"hstu-attn-b{batch_size}-h{heads}-d{attn_dim}-v{hidden_dim}--sparsity{seq_sparsity}-{mode}-{dtype}-target{target_size}-mattn{max_attn_len}-c{contextual_seq_len}-sl_alpha{sampling_alpha}",
            args={
                "batch_size": batch_size,
                "heads": heads,
                "attn_dim": attn_dim,
                "hidden_dim": hidden_dim,
                "dtype": dtype,
                "mode": mode,
                "seq_sparsity": seq_sparsity,
                "has_delta_q": has_delta_q,
                "delta_size": delta_size,
                "target_size": target_size,
                "bench_backward": bench_backward,
                "report_flops": report_flops,
                "max_attn_len": max_attn_len,
                "contextual_seq_len": contextual_seq_len,
                "sampling_alpha": sampling_alpha,
            },
        )
        for mode in modes
    ]

    @triton.testing.perf_report(configs)
    def _bench_hstu_attention(
        batch_size: int,
        heads: int,
        seq_len: int,
        attn_dim: int,
        hidden_dim: int,
        mode: str,
        provider: str,
        dtype: torch.dtype,
        seq_sparsity: float,
        has_delta_q: bool,
        delta_size: int,
        target_size: int,
        bench_backward: bool,
        report_flops: bool,
        max_attn_len: int,
        contextual_seq_len: int,
        sampling_alpha: float,
    ) -> float:
        assert mode in ["fwd", "bwd"]
        warmup = 25
        rep = 1000
        torch.manual_seed(1001)  # for reproducibility
        alpha = 1.0 / attn_dim
        causal = True
        lengths = generate_sparse_seq_len(
            size=batch_size,
            max_seq_len=seq_len,
            sparsity=seq_sparsity,
            device=torch.device("cuda"),
        )
        lengths = apply_sampling(lengths, sampling_alpha, max_seq_len=seq_len)
        if has_delta_q:
            lengths = lengths + delta_size
            num_targets = torch.ones_like(lengths) * delta_size
            seq_len = seq_len + delta_size
        else:
            delta_size = 0
            num_targets = None
            if target_size != 0:
                num_targets = torch.randint(
                    1,
                    target_size + 1,
                    (batch_size,),
                    device=lengths.device,
                    dtype=lengths.dtype,
                )
                num_targets = torch.where(num_targets > lengths, lengths, num_targets)
        max_attn_len = max_attn_len if max_attn_len < seq_len else seq_len
        seq_offsets = torch.zeros(
            (batch_size + 1,), dtype=torch.int64, device=torch.device("cuda")
        )
        seq_offsets[1:] = torch.cumsum(lengths, dim=0)
        L = int(seq_offsets[-1].item())
        x = torch.empty(
            (L, heads, attn_dim * 2 + hidden_dim),
            dtype=dtype,
            device=torch.device("cuda"),
        ).uniform_(-0.01, 0.01)
        q, k, v = torch.split(x, [attn_dim, attn_dim, hidden_dim], dim=-1)
        delta_q = torch.empty(
            (batch_size * delta_size, heads, attn_dim),
            dtype=dtype,
            device=torch.device("cuda"),
        ).uniform_(-0.1, 0.1)
        delta_x_offsets = torch.arange(0, delta_size, device=torch.device("cuda"))
        delta_x_offsets = (seq_offsets[1:] - delta_size).view(
            batch_size, 1
        ) + delta_x_offsets.view(1, delta_size)
        delta_x_offsets = delta_x_offsets.view(-1)

        if bench_backward:
            q = q.requires_grad_(True)
            k = k.requires_grad_(True)
            v = v.requires_grad_(True)
        assert provider in ["triton", "pytorch"]
        if has_delta_q:
            fn = lambda: cached_hstu_mha(  # noqa E731
                max_seq_len=seq_len,
                alpha=alpha,
                delta_q=delta_q,
                k=k,
                v=v,
                delta_x_offsets=delta_x_offsets,
                seq_offsets=seq_offsets,
                num_targets=num_targets,
                kernel=_get_kernel(provider),
            )
        else:
            fn = lambda: hstu_mha(  # noqa E73
                max_seq_len=seq_len,
                alpha=alpha,
                q=q,
                k=k,
                v=v,
                seq_offsets=seq_offsets,
                causal=causal,
                dropout_pr=0.0,
                training=True,
                num_targets=num_targets,
                max_attn_len=max_attn_len if max_attn_len > 0 else None,
                contextual_seq_len=contextual_seq_len,
                sort_by_length=True,
                kernel=_get_kernel(provider),
            )
        if mode == "bwd":
            o = fn()
            do = torch.randn_like(o)
            fn = lambda: o.backward(do, retain_graph=True)  # noqa E731
        ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)
        all_flops = _flops(
            batch_size, seq_len, attn_dim, hidden_dim, heads, seq_offsets, mode
        )
        if has_delta_q:
            all_flops = all_flops / seq_len * delta_size
        if report_flops:
            return all_flops / ms / 1e9
        else:
            return ms

    df = _bench_hstu_attention.run(
        print_data=True,
        show_plots=False,
        save_path="/tmp/" + os.environ["USER"],
        return_df=return_result,
    )

    if return_result:
        return configs, df


if __name__ == "__main__":
    main()

</content>

<content full_path="generative_recommenders/ops/pytorch/pt_hstu_attention.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

#!/usr/bin/env python3

# pyre-strict

from typing import Optional, Tuple

import torch
import torch.nn.functional as F


try:
    torch.ops.load_library("//deeplearning/fbgemm/fbgemm_gpu:sparse_ops")
    torch.ops.load_library("//deeplearning/fbgemm/fbgemm_gpu:sparse_ops_cpu")
except OSError:
    pass


@torch.fx.wrap
def _get_invalid_attn_mask(
    device: torch.device,
    causal: bool,
    N: int,
    seq_lengths: torch.Tensor,
    num_targets: Optional[torch.Tensor] = None,
    max_attn_len: Optional[int] = None,
    contextual_seq_len: int = 0,
    min_full_attn_seq_len: int = 0,
) -> torch.Tensor:
    ids = torch.arange(0, N, device=device).view(1, N)
    max_ids = seq_lengths.view(-1, 1, 1)
    if contextual_seq_len > 0:
        ids = ids - contextual_seq_len + 1
        ids = torch.clamp(ids, min=0)
        max_ids = max_ids - contextual_seq_len + 1
    if num_targets is not None:
        max_ids = max_ids - num_targets.view(-1, 1, 1)
        ids = torch.clamp(
            ids,
            max=max_ids,
        )
        row_ids = ids.view(-1, N, 1).expand(-1, N, N)
        col_ids = ids.view(-1, 1, N).expand(-1, N, N)
    else:
        row_ids = ids.view(N, 1).expand(N, N)
        col_ids = row_ids.t()
        row_ids = row_ids.view(1, N, N)
        col_ids = col_ids.view(1, N, N)
    row_col_dist = row_ids - col_ids
    invalid_attn_mask = torch.eye(N, device=device, dtype=torch.bool).view(1, N, N)
    if not causal:
        row_col_dist = torch.where(row_col_dist > 0, row_col_dist, -row_col_dist)
    invalid_attn_mask = torch.logical_or(invalid_attn_mask, row_col_dist > 0)
    if max_attn_len is not None and max_attn_len > 0:
        if min_full_attn_seq_len > 0:
            invalid_attn_mask = torch.logical_and(
                invalid_attn_mask,
                torch.logical_or(
                    row_col_dist <= max_attn_len,
                    row_ids >= max_ids - min_full_attn_seq_len,
                ),
            )
        else:
            invalid_attn_mask = torch.logical_and(
                invalid_attn_mask, row_col_dist <= max_attn_len
            )
    if contextual_seq_len > 0:
        invalid_attn_mask = torch.logical_or(
            invalid_attn_mask, torch.logical_and(row_ids == 0, col_ids < max_ids)
        )
    return invalid_attn_mask


def _pad_qkv(
    q: torch.Tensor,
    k: torch.Tensor,
    v: torch.Tensor,
    seq_offsets: torch.Tensor,
    N: int,
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    L, H, D = q.shape
    V = v.shape[2]
    padded_q = (
        torch.ops.fbgemm.jagged_to_padded_dense(
            values=q.reshape(L, H * D),
            offsets=[seq_offsets],
            max_lengths=[N],
            padding_value=0.0,
        )
        .view(-1, N, H, D)
        .transpose(1, 2)
    )  # [B, H, N, A]
    padded_k = (
        torch.ops.fbgemm.jagged_to_padded_dense(
            values=k.reshape(L, H * D),
            offsets=[seq_offsets],
            max_lengths=[N],
            padding_value=0.0,
        )
        .view(-1, N, H, D)
        .transpose(1, 2)
    )  # [B, H, N, A]
    padded_v = (
        torch.ops.fbgemm.jagged_to_padded_dense(
            values=v.reshape(L, H * V),
            offsets=[seq_offsets],
            max_lengths=[N],
            padding_value=0.0,
        )
        .view(-1, N, H, V)
        .transpose(1, 2)
    )  # [B, H, N, D]
    return padded_q, padded_k, padded_v


@torch.fx.wrap
def pytorch_hstu_mha(
    max_seq_len: int,
    alpha: float,
    q: torch.Tensor,
    k: torch.Tensor,
    v: torch.Tensor,
    seq_offsets: torch.Tensor,
    causal: bool = True,
    dropout_pr: float = 0.0,
    training: bool = True,
    num_targets: Optional[torch.Tensor] = None,
    max_attn_len: Optional[int] = None,
    contextual_seq_len: int = 0,
) -> torch.Tensor:
    L, H, _ = q.shape
    V = v.shape[2]
    q, k, v = _pad_qkv(
        q, k, v, seq_offsets, max_seq_len
    )  # [B, H, N, D) and [B, H, N, V]
    qk_attn = torch.einsum("bhxa,bhya->bhxy", q, k) * alpha
    qk_attn = F.silu(qk_attn) / max_seq_len
    invalid_attn_mask = _get_invalid_attn_mask(
        device=q.device,
        causal=causal,
        N=max_seq_len,
        seq_lengths=seq_offsets[1:] - seq_offsets[:-1],
        num_targets=num_targets,
        max_attn_len=max_attn_len,
        contextual_seq_len=contextual_seq_len,
    )
    qk_attn = qk_attn * invalid_attn_mask.unsqueeze(1)
    if dropout_pr > 0.0:
        qk_attn = F.dropout(qk_attn, p=dropout_pr, training=training)
    attn_dense = torch.einsum("bhxd,bhdv->bhxv", qk_attn, v)  # [B, H, N, V]
    return torch.ops.fbgemm.dense_to_jagged(
        attn_dense.transpose(1, 2).flatten(2, 3),  # [B, N, H, V]->[B, N, H * V]
        [seq_offsets],
        L,
    )[0].view(L, H, V)


@torch.fx.wrap
def _get_delta_invalid_attn_mask(
    max_seq_len: int,
    delta_x_offsets: torch.Tensor,
    seq_lengths: torch.Tensor,
    seq_offsets: torch.Tensor,
    num_targets: Optional[torch.Tensor] = None,
    max_attn_len: Optional[int] = None,
) -> torch.Tensor:
    B = seq_lengths.size(0)
    ids = torch.arange(0, max_seq_len, device=delta_x_offsets.device)
    col_ids = ids.view(1, 1, max_seq_len)
    row_ids = delta_x_offsets.view(B, -1) - seq_offsets[:-1].view(-1, 1)
    row_ids = row_ids.view(B, -1, 1)
    invalid_attn_mask = col_ids == row_ids
    if num_targets is not None:
        seq_lengths = seq_lengths.view(-1, 1, 1)
        num_targets = num_targets.view(-1, 1, 1)
        row_ids = torch.clamp(row_ids, max=seq_lengths - num_targets)
        col_ids = torch.clamp(col_ids, max=seq_lengths - num_targets)
    row_col_dist = row_ids - col_ids
    invalid_attn_mask = torch.logical_or(invalid_attn_mask, row_col_dist > 0)
    if max_attn_len is not None:
        invalid_attn_mask = torch.logical_and(
            invalid_attn_mask, row_col_dist <= max_attn_len
        )
    return invalid_attn_mask.unsqueeze(1)


@torch.fx.wrap
def pytorch_cached_hstu_mha(
    N: int,
    alpha: float,
    delta_q: torch.Tensor,
    k: torch.Tensor,
    v: torch.Tensor,
    delta_x_offsets: torch.Tensor,
    seq_offsets: torch.Tensor,
    num_targets: Optional[torch.Tensor] = None,
    attn_bias: Optional[torch.Tensor] = None,
    max_attn_len: Optional[int] = None,
) -> torch.Tensor:
    L, H, D = delta_q.shape
    _, _, V = v.shape
    B = seq_offsets.size(0) - 1
    delta_q = delta_q.view(B, -1, H, D).transpose(1, 2)
    full_k = (
        torch.ops.fbgemm.jagged_to_padded_dense(
            values=k.reshape(-1, H * D),
            offsets=[seq_offsets],
            max_lengths=[N],
            padding_value=0.0,
        )
        .view(B, -1, H, D)
        .transpose(1, 2)
    )
    full_v = (
        torch.ops.fbgemm.jagged_to_padded_dense(
            values=v.reshape(-1, H * V),
            offsets=[seq_offsets],
            max_lengths=[N],
            padding_value=0.0,
        )
        .view(B, -1, H, V)
        .transpose(1, 2)
    )
    qk_attn = torch.einsum("bhxa,bhya->bhxy", delta_q, full_k) * alpha
    if attn_bias is not None:
        qk_attn = qk_attn + attn_bias
    qk_attn = F.silu(qk_attn) / N
    invalid_attn_mask = _get_delta_invalid_attn_mask(
        max_seq_len=N,
        delta_x_offsets=delta_x_offsets,
        seq_lengths=seq_offsets[1:] - seq_offsets[:-1],
        seq_offsets=seq_offsets,
        num_targets=num_targets,
        max_attn_len=max_attn_len,
    )
    qk_attn = qk_attn * invalid_attn_mask
    attn_output = torch.einsum("bhxd,bhdv->bhxv", qk_attn, full_v)

    return attn_output.transpose(1, 2).reshape(-1, H, V)

</content>

<content full_path="generative_recommenders/ops/pytorch/pt_hstu_linear.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

#!/usr/bin/env python3
# pyre-strict

from typing import Optional

import torch
import torch.nn.functional as F


def pytorch_norm_mul_dropout(
    x: torch.Tensor,
    u: torch.Tensor,
    weight: torch.Tensor,
    bias: torch.Tensor,
    eps: float,
    dropout_ratio: float,
    training: bool,
    concat_ux: bool = False,
    group_norm: bool = False,
    num_heads: int = 1,
    linear_dim: int = -1,
) -> torch.Tensor:
    if group_norm:
        y = u * F.group_norm(
            x.view(-1, num_heads, linear_dim),
            num_groups=num_heads,
            weight=weight,
            bias=bias,
            eps=eps,
        ).view(-1, num_heads * linear_dim)
    else:
        y = u * F.layer_norm(
            x,
            normalized_shape=(x.shape[-1],),
            weight=weight,
            bias=bias,
            eps=eps,
        )
    if concat_ux:
        y = torch.cat([u, x, y], dim=1)
    y = F.dropout(
        y,
        p=dropout_ratio,
        training=training,
    )
    return y


def pytorch_hstu_compute_output(
    attn: torch.Tensor,
    u: torch.Tensor,
    x: torch.Tensor,
    norm_weight: torch.Tensor,
    norm_bias: torch.Tensor,
    output_weight: torch.Tensor,
    eps: float,
    dropout_ratio: float,
    training: bool,
    concat_ux: bool = False,
    group_norm: bool = False,
    num_heads: int = 1,
    linear_dim: int = -1,
    seed: Optional[int] = None,
    recompute_y_in_backward: bool = False,
) -> torch.Tensor:
    y = pytorch_norm_mul_dropout(
        x=attn,
        u=u,
        weight=norm_weight,
        bias=norm_bias,
        eps=eps,
        dropout_ratio=dropout_ratio,
        training=training,
        concat_ux=concat_ux,
        group_norm=group_norm,
        num_heads=num_heads,
        linear_dim=linear_dim,
    )
    return torch.addmm(x, y, output_weight)

</content>

<content full_path="generative_recommenders/ops/pytorch/pt_layer_norm.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


# pyre-strict


from typing import List

import torch


def pytorch_layer_norm(
    x: torch.Tensor,
    normalized_shape: List[int],
    weight: torch.Tensor,
    bias: torch.Tensor,
    eps: float,
) -> torch.Tensor:
    return torch.nn.functional.layer_norm(
        x,
        normalized_shape,
        weight.to(x.dtype),
        bias.to(x.dtype),
        eps,
    )


def pytorch_swish_layer_norm(
    x: torch.Tensor,
    normalized_shape: List[int],
    weight: torch.Tensor,
    bias: torch.Tensor,
    eps: float,
) -> torch.Tensor:
    return x * torch.sigmoid(
        torch.nn.functional.layer_norm(
            x,
            normalized_shape,
            weight.to(x.dtype),
            bias.to(x.dtype),
            eps,
        )
    )

</content>

<content full_path="generative_recommenders/ops/pytorch/pt_jagged.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

#!/usr/bin/env python3

# pyre-strict

from typing import Tuple

import torch


try:
    torch.ops.load_library("//deeplearning/fbgemm/fbgemm_gpu:sparse_ops")
    torch.ops.load_library("//deeplearning/fbgemm/fbgemm_gpu:sparse_ops_cpu")
except OSError:
    pass


def pytorch_jagged_dense_bmm(
    max_seq_len: int,
    seq_offsets: torch.Tensor,
    jagged: torch.Tensor,
    dense: torch.Tensor,
) -> torch.Tensor:
    padded_jagged = torch.ops.fbgemm.jagged_to_padded_dense(
        values=jagged,
        offsets=[seq_offsets],
        max_lengths=[max_seq_len],
        padding_value=0.0,
    )
    bmm_out = torch.bmm(padded_jagged, dense)
    jagged_bmm_out = torch.ops.fbgemm.dense_to_jagged(bmm_out, [seq_offsets])[0]

    return jagged_bmm_out


def pytorch_jagged_dense_broadcast_add(
    max_seq_len: int,
    seq_offsets: torch.Tensor,
    jagged: torch.Tensor,
    dense: torch.Tensor,
) -> torch.Tensor:
    padded_jagged = torch.ops.fbgemm.jagged_to_padded_dense(
        values=jagged,
        offsets=[seq_offsets],
        max_lengths=[max_seq_len],
        padding_value=0.0,
    )
    out = padded_jagged + dense.unsqueeze(1)
    jagged_out = torch.ops.fbgemm.dense_to_jagged(out, [seq_offsets])[0]
    return jagged_out


def pytorch_jagged_dense_bmm_broadcast_add(
    max_seq_len: int,
    seq_offsets: torch.Tensor,
    jagged: torch.Tensor,
    dense: torch.Tensor,
    bias: torch.Tensor,
) -> torch.Tensor:
    jagged = pytorch_jagged_dense_bmm(max_seq_len, seq_offsets, jagged, dense)
    return pytorch_jagged_dense_broadcast_add(
        max_seq_len=max_seq_len,
        seq_offsets=seq_offsets,
        jagged=jagged,
        dense=bias,
    )


@torch.fx.wrap
def _arange(len: int, device: torch.device) -> torch.Tensor:
    return torch.arange(len, device=device)


def pytorch_concat_2D_dense_jagged(
    jagged_max_seq_len: int,
    jagged_offsets: torch.Tensor,
    jagged_values: torch.Tensor,
    dense_values: torch.Tensor,
) -> torch.Tensor:
    B, dense_size, D = dense_values.size()
    jagged_dense = torch.ops.fbgemm.jagged_to_padded_dense(
        values=jagged_values,
        offsets=[jagged_offsets],
        max_lengths=[jagged_max_seq_len],
        padding_value=0.0,
    )
    concatted_dense = torch.cat([dense_values, jagged_dense], dim=1)
    concatted_offsets = (
        dense_size * _arange(B + 1, device=jagged_offsets.device) + jagged_offsets
    )
    return torch.ops.fbgemm.dense_to_jagged(
        concatted_dense,
        [concatted_offsets],
    )[0]


def pytorch_concat_2D_jagged_jagged(
    max_seq_len_left: int,
    offsets_left: torch.Tensor,
    values_left: torch.Tensor,
    max_seq_len_right: int,
    offsets_right: torch.Tensor,
    values_right: torch.Tensor,
    n_prefix_from_right: int,
) -> torch.Tensor:
    _, D = values_left.shape
    max_seq_len = max_seq_len_left + max_seq_len_right
    B = offsets_left.shape[0] - 1

    lengths_a = offsets_left[1:] - offsets_left[:-1]
    lengths_b = offsets_right[1:] - offsets_right[:-1]
    dense_a = torch.ops.fbgemm.jagged_to_padded_dense(
        values=values_left,
        offsets=[offsets_left],
        max_lengths=[max_seq_len_left],
        padding_value=0.0,
    )
    dense_b = torch.ops.fbgemm.jagged_to_padded_dense(
        values=values_right,
        offsets=[offsets_right],
        max_lengths=[max_seq_len_right],
        padding_value=0.0,
    )
    dense_b_prefix, dense_b_suffix = torch.split(
        dense_b, [n_prefix_from_right, max_seq_len_right - n_prefix_from_right], dim=1
    )
    dense = torch.cat([dense_b_prefix, dense_a, dense_b_suffix], dim=1)
    mask = _arange(max_seq_len, device=offsets_left.device).expand(B, max_seq_len)
    mask = torch.logical_or(
        mask < lengths_a.view(B, 1) + n_prefix_from_right,
        torch.logical_and(
            mask >= max_seq_len_left + n_prefix_from_right,
            mask < max_seq_len_left + lengths_b.view(B, 1),
        ),
    )
    return dense.view(-1, D)[mask.view(-1), :]


def pytorch_jagged_remove_first_or_last_1D(
    values: torch.Tensor,
    lengths: torch.Tensor,
    offsets: torch.Tensor,
    max_seq_len: int,
) -> Tuple[torch.Tensor, torch.Tensor]:
    values = values.view(-1, 1)
    shrunk_lengths = lengths - 1
    k_lengths = torch.stack([shrunk_lengths, torch.ones_like(lengths)], dim=1).view(-1)
    q_lengths = torch.stack([torch.ones_like(lengths), shrunk_lengths], dim=1).view(-1)
    all_indices = torch.arange(
        start=0, end=q_lengths.numel(), device=values.device
    ).reshape(-1, 2)
    q_indices, k_indices = all_indices[:, 1], all_indices[:, 0]
    values_no_first, _ = torch.ops.fbgemm.jagged_index_select(
        values, q_lengths, q_indices
    )
    values_no_last, _ = torch.ops.fbgemm.jagged_index_select(
        values, k_lengths, k_indices
    )
    return values_no_first.squeeze(), values_no_last.squeeze()


def pytorch_replace_last_n_with_jagged(
    max_seq_len_left: int,
    offsets_left: torch.Tensor,
    values_left: torch.Tensor,
    offsets_right: torch.Tensor,
    values_right: torch.Tensor,
) -> torch.Tensor:
    B = offsets_left.shape[0] - 1
    lengths_a = offsets_left[1:] - offsets_left[:-1]
    lengths_b = offsets_right[1:] - offsets_right[:-1]
    dense_a = torch.ops.fbgemm.jagged_to_padded_dense(
        values=values_left,
        offsets=[offsets_left],
        max_lengths=[max_seq_len_left],
        padding_value=0.0,
    )
    raw_mask = torch.arange(max_seq_len_left, device=offsets_left.device).expand(
        B, max_seq_len_left
    )
    mask = torch.logical_and(
        raw_mask >= (lengths_a - lengths_b).unsqueeze(1),
        raw_mask < lengths_a.unsqueeze(1),
    )
    dense_a[mask] = values_right
    jagged_a = torch.ops.fbgemm.dense_to_jagged(
        dense_a,
        [offsets_left],
    )[0]
    return jagged_a

</content>

<content full_path="generative_recommenders/ops/pytorch/pt_position.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

#!/usr/bin/env python3

# pyre-strict

from typing import Optional

import torch
from hammer.fx_utils import fx_unwrap_optional_tensor


torch.ops.load_library("//deeplearning/fbgemm/fbgemm_gpu:sparse_ops")
torch.ops.load_library("//deeplearning/fbgemm/fbgemm_gpu:sparse_ops_cpu")


@torch.fx.wrap
def torch_arange(end: int, device: torch.device) -> torch.Tensor:
    return torch.arange(end, device=device)


def pytorch_add_position_embeddings(
    jagged: torch.Tensor,
    jagged_offsets: torch.Tensor,
    high_inds: torch.Tensor,
    max_seq_len: int,
    dense: torch.Tensor,
    scale: float = 1.0,
) -> torch.Tensor:
    jagged = jagged * scale
    B = high_inds.size(0)
    col_indices = torch_arange(max_seq_len, device=high_inds.device).expand(
        B, max_seq_len
    )
    col_indices = torch.clamp(col_indices, max=high_inds.view(-1, 1))
    dense_values = torch.index_select(dense, 0, col_indices.reshape(-1)).view(
        B, max_seq_len, -1
    )
    return torch.ops.fbgemm.jagged_dense_elementwise_add_jagged_output(
        jagged,
        [jagged_offsets],
        dense_values,
    )[0]


@torch.fx.wrap
def _get_col_indices(
    max_seq_len: int,
    max_contextual_seq_len: int,
    max_pos_ind: int,
    seq_lengths: torch.Tensor,
    num_targets: Optional[torch.Tensor],
    interleave_targets: bool,
) -> torch.Tensor:
    B = seq_lengths.size(0)
    col_indices = torch.arange(max_seq_len, device=seq_lengths.device).expand(
        B, max_seq_len
    )
    if num_targets is not None:
        if interleave_targets:
            high_inds = seq_lengths - fx_unwrap_optional_tensor(num_targets) * 2
        else:
            high_inds = seq_lengths - fx_unwrap_optional_tensor(num_targets)
        col_indices = torch.clamp(col_indices, max=high_inds.view(-1, 1))
        col_indices = high_inds.view(-1, 1) - col_indices
    else:
        col_indices = seq_lengths.view(-1, 1) - col_indices
    col_indices = col_indices + max_contextual_seq_len
    col_indices = torch.clamp(col_indices, max=max_pos_ind - 1)
    if max_contextual_seq_len > 0:
        col_indices[:, :max_contextual_seq_len] = torch.arange(
            0,
            max_contextual_seq_len,
            device=col_indices.device,
            dtype=col_indices.dtype,
        ).view(1, -1)
    return col_indices


def pytorch_add_timestamp_positional_embeddings(
    seq_embeddings: torch.Tensor,
    seq_offsets: torch.Tensor,
    pos_embeddings: torch.Tensor,
    ts_embeddings: torch.Tensor,
    timestamps: torch.Tensor,
    max_seq_len: int,
    max_contextual_seq_len: int,
    seq_lengths: torch.Tensor,
    num_targets: Optional[torch.Tensor],
    interleave_targets: bool,
    time_bucket_fn: str,
) -> torch.Tensor:
    max_pos_ind = pos_embeddings.size(0)
    # position encoding
    pos_inds = _get_col_indices(
        max_seq_len=max_seq_len,
        max_contextual_seq_len=max_contextual_seq_len,
        max_pos_ind=max_pos_ind,
        seq_lengths=seq_lengths,
        num_targets=num_targets,
        interleave_targets=interleave_targets,
    )
    B, _ = pos_inds.shape
    # timestamp encoding
    num_time_buckets = 2048
    time_bucket_increments = 60.0
    time_bucket_divisor = 1.0
    time_delta = 0
    timestamps = timestamps[:, :max_seq_len]
    query_time = torch.gather(
        timestamps, dim=1, index=(seq_lengths - 1).unsqueeze(1).clamp(min=0)
    )
    ts = query_time - timestamps
    ts = ts + time_delta
    ts = ts.clamp(min=1e-6) / time_bucket_increments
    if time_bucket_fn == "log":
        ts = torch.log(ts)
    else:
        ts = torch.sqrt(ts)
    ts = (ts / time_bucket_divisor).clamp(min=0).int()
    ts = torch.clamp(
        ts,
        min=0,
        max=num_time_buckets,
    )
    position_embeddings = torch.index_select(
        pos_embeddings, 0, pos_inds.reshape(-1)
    ).view(B, max_seq_len, -1)
    time_embeddings = torch.index_select(ts_embeddings, 0, ts.reshape(-1)).view(
        B, max_seq_len, -1
    )
    return torch.ops.fbgemm.jagged_dense_elementwise_add_jagged_output(
        seq_embeddings,
        [seq_offsets],
        (time_embeddings + position_embeddings).to(seq_embeddings.dtype),
    )[0]

</content>

<content full_path="generative_recommenders/indexing/candidate_index.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# pyre-unsafe

import abc
from typing import Optional, Tuple

import torch

from generative_recommenders.modeling.sequential.utils import batch_gather_embeddings
from generative_recommenders.rails.indexing.candidate_index import TopKModule


class CandidateIndex(object):
    def __init__(
        self,
        ids: torch.Tensor,
        embeddings: torch.Tensor,
        invalid_ids: Optional[torch.Tensor] = None,
        debug_path: Optional[str] = None,
    ) -> None:
        super().__init__()

        self._ids: torch.Tensor = ids
        self._embeddings: torch.Tensor = embeddings
        self._invalid_ids: Optional[torch.Tensor] = invalid_ids
        self._debug_path: Optional[str] = debug_path

    @property
    def ids(self) -> torch.Tensor:
        """
        Returns:
            (1, X) or (B, X), where valid ids are positive integers.
        """
        return self._ids

    @property
    def num_objects(self) -> int:
        return self._ids.size(1)

    @property
    def embeddings(self) -> torch.Tensor:
        """
        Returns:
            (1, X, D) or (B, X, D) with the same shape as `ids'.
        """
        return self._embeddings

    def filter_invalid_ids(
        self,
        invalid_ids: torch.Tensor,
    ) -> "CandidateIndex":
        """
        Filters invalid_ids (batch dimension dependent) from the current index.

        Args:
            invalid_ids: (B, N) x int64.

        Returns:
            CandidateIndex with invalid_ids filtered.
        """
        X = self._ids.size(1)
        if self._ids.size(0) == 1:
            # ((1, X, 1) == (B, 1, N)) -> (B, X)
            invalid_mask, _ = (self._ids.unsqueeze(2) == invalid_ids.unsqueeze(1)).max(
                dim=2
            )
            lengths = (~invalid_mask).int().sum(-1)  # (B,)
            valid_1d_mask = (~invalid_mask).view(-1)
            B: int = lengths.size(0)
            D: int = self._embeddings.size(-1)
            jagged_ids = self._ids.expand(B, -1).reshape(-1)[valid_1d_mask]
            jagged_embeddings = self._embeddings.expand(B, -1, -1).reshape(-1, D)[
                valid_1d_mask
            ]
            X_prime: int = lengths.max(-1)[0].item()
            jagged_offsets = torch.ops.fbgemm.asynchronous_complete_cumsum(lengths)
            return CandidateIndex(
                ids=torch.ops.fbgemm.jagged_to_padded_dense(
                    values=jagged_ids.unsqueeze(-1),
                    offsets=[jagged_offsets],
                    max_lengths=[X_prime],
                    padding_value=0,
                ).squeeze(-1),
                embeddings=torch.ops.fbgemm.jagged_to_padded_dense(
                    values=jagged_embeddings,
                    offsets=[jagged_offsets],
                    max_lengths=[X_prime],
                    padding_value=0.0,
                ),
                debug_path=self._debug_path,
            )
        else:
            assert self._invalid_ids == None
            return CandidateIndex(
                ids=self.ids,
                embeddings=self.embeddings,
                invalid_ids=invalid_ids,
                debug_path=self._debug_path,
            )

    def get_top_k_outputs(
        self,
        query_embeddings: torch.Tensor,
        k: int,
        top_k_module: TopKModule,
        invalid_ids: Optional[torch.Tensor],
        r: int = 1,
        return_embeddings: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]:
        """
        Gets top-k outputs specified by `policy_fn', while filtering out
        invalid ids per row as specified by `invalid_ids'.

        Args:
            k: int. top k to return.
            policy_fn: lambda that takes in item-side embeddings (B, X, D,) and user-side
                embeddings (B * r, ...), and returns predictions (unnormalized logits)
                of shape (B * r, X,).
            invalid_ids: (B * r, N_0) x int64. The list of ids (if > 0) to filter from
                results if present. Expect N_0 to be a small constant.
            return_embeddings: bool if we should additionally return embeddings for the
                top k results.

        Returns:
            A tuple of (top_k_ids, top_k_prs, top_k_embeddings) of shape (B * r, k, ...).
        """
        B: int = query_embeddings.size(0)
        max_num_invalid_ids = 0
        if invalid_ids is not None:
            max_num_invalid_ids = invalid_ids.size(1)

        k_prime = min(k + max_num_invalid_ids, self.num_objects)
        top_k_prime_scores, top_k_prime_ids = top_k_module(
            query_embeddings=query_embeddings, k=k_prime
        )
        # Masks out invalid items rowwise.
        if invalid_ids is not None:
            id_is_valid = ~(
                (top_k_prime_ids.unsqueeze(2) == invalid_ids.unsqueeze(1)).max(2)[0]
            )  # [B, K + N_0]
            id_is_valid = torch.logical_and(
                id_is_valid, torch.cumsum(id_is_valid.int(), dim=1) <= k
            )
            # [[1, 0, 1, 0], [0, 1, 1, 1]], k=2 -> [[0, 2], [1, 2]]
            top_k_rowwise_offsets = torch.nonzero(id_is_valid, as_tuple=True)[1].view(
                -1, k
            )
            top_k_scores = torch.gather(
                top_k_prime_scores, dim=1, index=top_k_rowwise_offsets
            )
            top_k_ids = torch.gather(
                top_k_prime_ids, dim=1, index=top_k_rowwise_offsets
            )
        else:
            top_k_scores = top_k_prime_scores
            top_k_ids = top_k_prime_ids

        # TODO: this should be decoupled from candidate_index.
        if return_embeddings:
            raise ValueError("return_embeddings not supported yet.")
        else:
            top_k_embeddings = None
        return top_k_ids, top_k_scores, top_k_embeddings

    def apply_object_filter(self) -> "CandidateIndex":
        """
        Applies general per batch filters.
        """
        raise NotImplementedError("not implemented.")

</content>

<content full_path="generative_recommenders/indexing/__init__.py">

</content>

<content full_path="generative_recommenders/indexing/utils.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# pyre-unsafe

import torch

from generative_recommenders.rails.indexing.candidate_index import TopKModule
from generative_recommenders.rails.indexing.mips_top_k import MIPSBruteForceTopK
from generative_recommenders.rails.indexing.mol_top_k import MoLBruteForceTopK


def get_top_k_module(
    top_k_method: str,
    model: torch.nn.Module,
    item_embeddings: torch.Tensor,
    item_ids: torch.Tensor,
) -> TopKModule:
    if top_k_method == "MIPSBruteForceTopK":
        top_k_module = MIPSBruteForceTopK(
            item_embeddings=item_embeddings,
            item_ids=item_ids,
        )
    elif top_k_method == "MoLBruteForceTopK":
        top_k_module = MoLBruteForceTopK(
            item_embeddings=item_embeddings,
            item_ids=item_ids,
        )
    else:
        raise ValueError(f"Invalid top-k method {top_k_method}")
    return top_k_module

</content>

<content full_path="generative_recommenders/data/item_features.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# pyre-unsafe

from dataclasses import dataclass
from typing import List

import torch


@dataclass
class ItemFeatures:
    num_items: int
    max_jagged_dimension: int
    max_ind_range: List[int]  # [(,)] x num_features
    lengths: List[torch.Tensor]  # [(num_items,)] x num_features
    values: List[torch.Tensor]  # [(num_items, max_jagged_dimension)] x num_features

</content>

<content full_path="generative_recommenders/data/reco_dataset.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# pyre-unsafe

from dataclasses import dataclass
from typing import List

import pandas as pd

import torch

from generative_recommenders.data.dataset import DatasetV2, MultiFileDatasetV2
from generative_recommenders.data.item_features import ItemFeatures
from generative_recommenders.data.preprocessor import get_common_preprocessors


@dataclass
class RecoDataset:
    max_sequence_length: int
    num_unique_items: int
    max_item_id: int
    all_item_ids: List[int]
    train_dataset: torch.utils.data.Dataset
    eval_dataset: torch.utils.data.Dataset


def get_reco_dataset(
    dataset_name: str,
    max_sequence_length: int,
    chronological: bool,
    positional_sampling_ratio: float = 1.0,
) -> RecoDataset:
    if dataset_name == "ml-1m":
        dp = get_common_preprocessors()[dataset_name]
        train_dataset = DatasetV2(
            ratings_file=dp.output_format_csv(),
            padding_length=max_sequence_length + 1,  # target
            ignore_last_n=1,
            chronological=chronological,
            sample_ratio=positional_sampling_ratio,
        )
        eval_dataset = DatasetV2(
            ratings_file=dp.output_format_csv(),
            padding_length=max_sequence_length + 1,  # target
            ignore_last_n=0,
            chronological=chronological,
            sample_ratio=1.0,  # do not sample
        )
    elif dataset_name == "ml-20m":
        dp = get_common_preprocessors()[dataset_name]
        train_dataset = DatasetV2(
            ratings_file=dp.output_format_csv(),
            padding_length=max_sequence_length + 1,  # target
            ignore_last_n=1,
            chronological=chronological,
        )
        eval_dataset = DatasetV2(
            ratings_file=dp.output_format_csv(),
            padding_length=max_sequence_length + 1,  # target
            ignore_last_n=0,
            chronological=chronological,
        )
    elif dataset_name == "ml-3b":
        dp = get_common_preprocessors()[dataset_name]
        train_dataset = MultiFileDatasetV2(
            file_prefix="tmp/ml-3b/16x32",
            num_files=16,
            padding_length=max_sequence_length + 1,  # target
            ignore_last_n=1,
            chronological=chronological,
        )
        eval_dataset = MultiFileDatasetV2(
            file_prefix="tmp/ml-3b/16x32",
            num_files=16,
            padding_length=max_sequence_length + 1,  # target
            ignore_last_n=0,
            chronological=chronological,
        )
    elif dataset_name == "amzn-books":
        dp = get_common_preprocessors()[dataset_name]
        train_dataset = DatasetV2(
            ratings_file=dp.output_format_csv(),
            padding_length=max_sequence_length + 1,  # target
            ignore_last_n=1,
            shift_id_by=1,  # [0..n-1] -> [1..n]
            chronological=chronological,
        )
        eval_dataset = DatasetV2(
            ratings_file=dp.output_format_csv(),
            padding_length=max_sequence_length + 1,  # target
            ignore_last_n=0,
            shift_id_by=1,  # [0..n-1] -> [1..n]
            chronological=chronological,
        )
    else:
        raise ValueError(f"Unknown dataset {dataset_name}")

    if dataset_name == "ml-1m" or dataset_name == "ml-20m":
        items = pd.read_csv(dp.processed_item_csv(), delimiter=",")
        max_jagged_dimension = 16
        expected_max_item_id = dp.expected_max_item_id()
        assert expected_max_item_id is not None
        item_features: ItemFeatures = ItemFeatures(
            max_ind_range=[63, 16383, 511],
            num_items=expected_max_item_id + 1,
            max_jagged_dimension=max_jagged_dimension,
            lengths=[
                torch.zeros((expected_max_item_id + 1,), dtype=torch.int64),
                torch.zeros((expected_max_item_id + 1,), dtype=torch.int64),
                torch.zeros((expected_max_item_id + 1,), dtype=torch.int64),
            ],
            values=[
                torch.zeros(
                    (expected_max_item_id + 1, max_jagged_dimension),
                    dtype=torch.int64,
                ),
                torch.zeros(
                    (expected_max_item_id + 1, max_jagged_dimension),
                    dtype=torch.int64,
                ),
                torch.zeros(
                    (expected_max_item_id + 1, max_jagged_dimension),
                    dtype=torch.int64,
                ),
            ],
        )
        all_item_ids = []
        for df_index, row in items.iterrows():
            # print(f"index {df_index}: {row}")
            movie_id = int(row["movie_id"])
            genres = row["genres"].split("|")
            titles = row["cleaned_title"].split(" ")
            # print(f"{index}: genres{genres}, title{titles}")
            genres_vector = [hash(x) % item_features.max_ind_range[0] for x in genres]
            titles_vector = [hash(x) % item_features.max_ind_range[1] for x in titles]
            years_vector = [hash(row["year"]) % item_features.max_ind_range[2]]
            item_features.lengths[0][movie_id] = min(
                len(genres_vector), max_jagged_dimension
            )
            item_features.lengths[1][movie_id] = min(
                len(titles_vector), max_jagged_dimension
            )
            item_features.lengths[2][movie_id] = min(
                len(years_vector), max_jagged_dimension
            )
            for f, f_values in enumerate([genres_vector, titles_vector, years_vector]):
                for j in range(min(len(f_values), max_jagged_dimension)):
                    item_features.values[f][movie_id][j] = f_values[j]
            all_item_ids.append(movie_id)
        max_item_id = dp.expected_max_item_id()
        for x in all_item_ids:
            assert x > 0, "x in all_item_ids should be positive"
    else:
        # expected_max_item_id and item_features are not set for Amazon datasets.
        item_features = None
        max_item_id = dp.expected_num_unique_items()
        all_item_ids = [x + 1 for x in range(max_item_id)]  # pyre-ignore [6]

    return RecoDataset(
        max_sequence_length=max_sequence_length,
        num_unique_items=dp.expected_num_unique_items(),  # pyre-ignore [6]
        max_item_id=max_item_id,  # pyre-ignore [6]
        all_item_ids=all_item_ids,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
    )

</content>

<content full_path="generative_recommenders/data/preprocessor.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# pyre-unsafe

import abc
import logging
import os
import sys
import tarfile
from typing import Dict, Optional, Union

from urllib.request import urlretrieve
from zipfile import ZipFile

import numpy as np

import pandas as pd


logging.basicConfig(stream=sys.stdout, level=logging.INFO)


class DataProcessor:
    """
    This preprocessor does not remap item_ids. This is intended so that we can easily join other
    side-information based on item_ids later.
    """

    def __init__(
        self,
        prefix: str,
        expected_num_unique_items: Optional[int],
        expected_max_item_id: Optional[int],
    ) -> None:
        self._prefix: str = prefix
        self._expected_num_unique_items = expected_num_unique_items
        self._expected_max_item_id = expected_max_item_id

    @abc.abstractmethod
    def expected_num_unique_items(self) -> Optional[int]:
        return self._expected_num_unique_items

    @abc.abstractmethod
    def expected_max_item_id(self) -> Optional[int]:
        return self._expected_max_item_id

    @abc.abstractmethod
    def processed_item_csv(self) -> str:
        pass

    def output_format_csv(self) -> str:
        return f"tmp/{self._prefix}/sasrec_format.csv"

    def to_seq_data(
        self,
        ratings_data: pd.DataFrame,
        user_data: Optional[pd.DataFrame] = None,
    ) -> pd.DataFrame:
        if user_data is not None:
            ratings_data_transformed = ratings_data.join(
                user_data.set_index("user_id"), on="user_id"
            )
        else:
            ratings_data_transformed = ratings_data
        ratings_data_transformed.item_ids = ratings_data_transformed.item_ids.apply(
            lambda x: ",".join([str(v) for v in x])
        )
        ratings_data_transformed.ratings = ratings_data_transformed.ratings.apply(
            lambda x: ",".join([str(v) for v in x])
        )
        ratings_data_transformed.timestamps = ratings_data_transformed.timestamps.apply(
            lambda x: ",".join([str(v) for v in x])
        )
        ratings_data_transformed.rename(
            columns={
                "item_ids": "sequence_item_ids",
                "ratings": "sequence_ratings",
                "timestamps": "sequence_timestamps",
            },
            inplace=True,
        )
        return ratings_data_transformed

    def file_exists(self, name: str) -> bool:
        return os.path.isfile("%s/%s" % (os.getcwd(), name))


class MovielensSyntheticDataProcessor(DataProcessor):
    def __init__(
        self,
        prefix: str,
        expected_num_unique_items: Optional[int] = None,
        expected_max_item_id: Optional[int] = None,
    ) -> None:
        super().__init__(prefix, expected_num_unique_items, expected_max_item_id)

    def preprocess_rating(self) -> None:
        return


class MovielensDataProcessor(DataProcessor):
    def __init__(
        self,
        download_path: str,
        saved_name: str,
        prefix: str,
        convert_timestamp: bool,
        expected_num_unique_items: Optional[int] = None,
        expected_max_item_id: Optional[int] = None,
    ) -> None:
        super().__init__(prefix, expected_num_unique_items, expected_max_item_id)
        self._download_path = download_path
        self._saved_name = saved_name
        self._convert_timestamp: bool = convert_timestamp

    def download(self) -> None:
        if not self.file_exists(self._saved_name):
            urlretrieve(self._download_path, self._saved_name)
        if self._saved_name[-4:] == ".zip":
            ZipFile(self._saved_name, "r").extractall(path="tmp/")
        else:
            with tarfile.open(self._saved_name, "r:*") as tar_ref:
                tar_ref.extractall("tmp/")

    def processed_item_csv(self) -> str:
        return f"tmp/processed/{self._prefix}/movies.csv"

    def sasrec_format_csv_by_user_train(self) -> str:
        return f"tmp/{self._prefix}/sasrec_format_by_user_train.csv"

    def sasrec_format_csv_by_user_test(self) -> str:
        return f"tmp/{self._prefix}/sasrec_format_by_user_test.csv"

    def preprocess_rating(self) -> int:
        self.download()

        if self._prefix == "ml-1m":
            users = pd.read_csv(
                f"tmp/{self._prefix}/users.dat",
                sep="::",
                names=["user_id", "sex", "age_group", "occupation", "zip_code"],
            )
            ratings = pd.read_csv(
                f"tmp/{self._prefix}/ratings.dat",
                sep="::",
                names=["user_id", "movie_id", "rating", "unix_timestamp"],
            )
            movies = pd.read_csv(
                f"tmp/{self._prefix}/movies.dat",
                sep="::",
                names=["movie_id", "title", "genres"],
                encoding="iso-8859-1",
            )
        elif self._prefix == "ml-20m":
            # ml-20m
            # ml-20m doesn't have user data.
            users = None
            # ratings: userId,movieId,rating,timestamp
            ratings = pd.read_csv(
                f"tmp/{self._prefix}/ratings.csv",
                sep=",",
            )
            ratings.rename(
                columns={
                    "userId": "user_id",
                    "movieId": "movie_id",
                    "timestamp": "unix_timestamp",
                },
                inplace=True,
            )
            # movieId,title,genres
            # 1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy
            # 2,Jumanji (1995),Adventure|Children|Fantasy
            movies = pd.read_csv(
                f"tmp/{self._prefix}/movies.csv",
                sep=",",
                encoding="iso-8859-1",
            )
            movies.rename(columns={"movieId": "movie_id"}, inplace=True)
        else:
            assert self._prefix == "ml-20mx16x32"
            # ml-1b
            user_ids = []
            movie_ids = []
            for i in range(16):
                train_file = f"tmp/{self._prefix}/trainx16x32_{i}.npz"
                with np.load(train_file) as data:
                    user_ids.extend([x[0] for x in data["arr_0"]])
                    movie_ids.extend([x[1] for x in data["arr_0"]])
            ratings = pd.DataFrame(
                data={
                    "user_id": user_ids,
                    "movie_id": movie_ids,
                    "rating": user_ids,  # placeholder
                    "unix_timestamp": movie_ids,  # placeholder
                }
            )
            users = None
            movies = None

        if movies is not None:
            # ML-1M and ML-20M only
            movies["year"] = movies["title"].apply(lambda x: x[-5:-1])
            movies["cleaned_title"] = movies["title"].apply(lambda x: x[:-7])
            # movies.year = pd.Categorical(movies.year)
            # movies["year"] = movies.year.cat.codes

        if users is not None:
            ## Users (ml-1m only)
            users.sex = pd.Categorical(users.sex)
            users["sex"] = users.sex.cat.codes

            users.age_group = pd.Categorical(users.age_group)
            users["age_group"] = users.age_group.cat.codes

            users.occupation = pd.Categorical(users.occupation)
            users["occupation"] = users.occupation.cat.codes

            users.zip_code = pd.Categorical(users.zip_code)
            users["zip_code"] = users.zip_code.cat.codes

        # Normalize movie ids to speed up training
        print(
            f"{self._prefix} #item before normalize: {len(set(ratings['movie_id'].values))}"
        )
        print(
            f"{self._prefix} max item id before normalize: {max(set(ratings['movie_id'].values))}"
        )
        # print(f"ratings.movie_id.cat.categories={ratings.movie_id.cat.categories}; {type(ratings.movie_id.cat.categories)}")
        # print(f"ratings.movie_id.cat.codes={ratings.movie_id.cat.codes}; {type(ratings.movie_id.cat.codes)}")
        # print(movie_id_to_cat)
        # ratings["movie_id"] = ratings.movie_id.cat.codes
        # print(f"{self._prefix} #item after normalize: {len(set(ratings['movie_id'].values))}")
        # print(f"{self._prefix} max item id after normalize: {max(set(ratings['movie_id'].values))}")
        # movies["remapped_id"] = movies["movie_id"].apply(lambda x: movie_id_to_cat[x])

        if self._convert_timestamp:
            ratings["unix_timestamp"] = pd.to_datetime(
                ratings["unix_timestamp"], unit="s"
            )

        # Save primary csv's
        if not os.path.exists(f"tmp/processed/{self._prefix}"):
            os.makedirs(f"tmp/processed/{self._prefix}")
        if users is not None:
            users.to_csv(f"tmp/processed/{self._prefix}/users.csv", index=False)
        if movies is not None:
            movies.to_csv(f"tmp/processed/{self._prefix}/movies.csv", index=False)
        ratings.to_csv(f"tmp/processed/{self._prefix}/ratings.csv", index=False)

        num_unique_users = len(set(ratings["user_id"].values))
        num_unique_items = len(set(ratings["movie_id"].values))

        # SASRec version
        ratings_group = ratings.sort_values(by=["unix_timestamp"]).groupby("user_id")
        seq_ratings_data = pd.DataFrame(
            data={
                "user_id": list(ratings_group.groups.keys()),
                "item_ids": list(ratings_group.movie_id.apply(list)),
                "ratings": list(ratings_group.rating.apply(list)),
                "timestamps": list(ratings_group.unix_timestamp.apply(list)),
            }
        )

        result = pd.DataFrame([[]])
        for col in ["item_ids"]:
            result[col + "_mean"] = seq_ratings_data[col].apply(len).mean()
            result[col + "_min"] = seq_ratings_data[col].apply(len).min()
            result[col + "_max"] = seq_ratings_data[col].apply(len).max()
        print(self._prefix)
        print(result)

        seq_ratings_data = self.to_seq_data(seq_ratings_data, users)
        seq_ratings_data.sample(frac=1).reset_index().to_csv(
            self.output_format_csv(), index=False, sep=","
        )

        # Split by user ids (not tested yet)
        user_id_split = int(num_unique_users * 0.9)
        seq_ratings_data_train = seq_ratings_data[
            seq_ratings_data["user_id"] <= user_id_split
        ]
        seq_ratings_data_train.sample(frac=1).reset_index().to_csv(
            self.sasrec_format_csv_by_user_train(),
            index=False,
            sep=",",
        )
        seq_ratings_data_test = seq_ratings_data[
            seq_ratings_data["user_id"] > user_id_split
        ]
        seq_ratings_data_test.sample(frac=1).reset_index().to_csv(
            self.sasrec_format_csv_by_user_test(), index=False, sep=","
        )
        print(
            f"{self._prefix}: train num user: {len(set(seq_ratings_data_train['user_id'].values))}"
        )
        print(
            f"{self._prefix}: test num user: {len(set(seq_ratings_data_test['user_id'].values))}"
        )

        # print(seq_ratings_data)
        if self.expected_num_unique_items() is not None:
            assert (
                self.expected_num_unique_items() == num_unique_items
            ), f"Expected items: {self.expected_num_unique_items()}, got: {num_unique_items}"

        return num_unique_items


class AmazonDataProcessor(DataProcessor):
    def __init__(
        self,
        download_path: str,
        saved_name: str,
        prefix: str,
        expected_num_unique_items: Optional[int],
    ) -> None:
        super().__init__(
            prefix,
            expected_num_unique_items=expected_num_unique_items,
            expected_max_item_id=None,
        )
        self._download_path = download_path
        self._saved_name = saved_name
        self._prefix = prefix

    def download(self) -> None:
        if not self.file_exists(self._saved_name):
            urlretrieve(self._download_path, self._saved_name)

    def preprocess_rating(self) -> int:
        self.download()

        ratings = pd.read_csv(
            self._saved_name,
            sep=",",
            names=["user_id", "item_id", "rating", "timestamp"],
        )
        print(f"{self._prefix} #data points before filter: {ratings.shape[0]}")
        print(
            f"{self._prefix} #user before filter: {len(set(ratings['user_id'].values))}"
        )
        print(
            f"{self._prefix} #item before filter: {len(set(ratings['item_id'].values))}"
        )

        # filter users and items with presence < 5
        item_id_count = (
            ratings["item_id"]
            .value_counts()
            .rename_axis("unique_values")
            .reset_index(name="item_count")
        )
        user_id_count = (
            ratings["user_id"]
            .value_counts()
            .rename_axis("unique_values")
            .reset_index(name="user_count")
        )
        ratings = ratings.join(item_id_count.set_index("unique_values"), on="item_id")
        ratings = ratings.join(user_id_count.set_index("unique_values"), on="user_id")
        ratings = ratings[ratings["item_count"] >= 5]
        ratings = ratings[ratings["user_count"] >= 5]
        print(f"{self._prefix} #data points after filter: {ratings.shape[0]}")

        # categorize user id and item id
        ratings["item_id"] = pd.Categorical(ratings["item_id"])
        ratings["item_id"] = ratings["item_id"].cat.codes
        ratings["user_id"] = pd.Categorical(ratings["user_id"])
        ratings["user_id"] = ratings["user_id"].cat.codes
        print(
            f"{self._prefix} #user after filter: {len(set(ratings['user_id'].values))}"
        )
        print(
            f"{self._prefix} #item ater filter: {len(set(ratings['item_id'].values))}"
        )

        num_unique_items = len(set(ratings["item_id"].values))

        # SASRec version
        ratings_group = ratings.sort_values(by=["timestamp"]).groupby("user_id")

        seq_ratings_data = pd.DataFrame(
            data={
                "user_id": list(ratings_group.groups.keys()),
                "item_ids": list(ratings_group.item_id.apply(list)),
                "ratings": list(ratings_group.rating.apply(list)),
                "timestamps": list(ratings_group.timestamp.apply(list)),
            }
        )

        seq_ratings_data = seq_ratings_data[
            seq_ratings_data["item_ids"].apply(len) >= 5
        ]

        result = pd.DataFrame([[]])
        for col in ["item_ids"]:
            result[col + "_mean"] = seq_ratings_data[col].apply(len).mean()
            result[col + "_min"] = seq_ratings_data[col].apply(len).min()
            result[col + "_max"] = seq_ratings_data[col].apply(len).max()
        print(self._prefix)
        print(result)

        if not os.path.exists(f"tmp/{self._prefix}"):
            os.makedirs(f"tmp/{self._prefix}")

        seq_ratings_data = self.to_seq_data(seq_ratings_data)
        seq_ratings_data.sample(frac=1).reset_index().to_csv(
            self.output_format_csv(), index=False, sep=","
        )

        if self.expected_num_unique_items() is not None:
            assert (
                self.expected_num_unique_items() == num_unique_items
            ), f"expected: {self.expected_num_unique_items()}, actual: {num_unique_items}"
            logging.info(f"{self.expected_num_unique_items()} unique items.")

        return num_unique_items


def get_common_preprocessors() -> (
    Dict[
        str,
        Union[
            AmazonDataProcessor, MovielensDataProcessor, MovielensSyntheticDataProcessor
        ],
    ]
):
    ml_1m_dp = MovielensDataProcessor(  # pyre-ignore [45]
        "http://files.grouplens.org/datasets/movielens/ml-1m.zip",
        "tmp/movielens1m.zip",
        prefix="ml-1m",
        convert_timestamp=False,
        expected_num_unique_items=3706,
        expected_max_item_id=3952,
    )
    ml_20m_dp = MovielensDataProcessor(  # pyre-ignore [45]
        "http://files.grouplens.org/datasets/movielens/ml-20m.zip",
        "tmp/movielens20m.zip",
        prefix="ml-20m",
        convert_timestamp=False,
        expected_num_unique_items=26744,
        expected_max_item_id=131262,
    )
    ml_1b_dp = MovielensDataProcessor(  # pyre-ignore [45]
        "https://files.grouplens.org/datasets/movielens/ml-20mx16x32.tar",
        "tmp/movielens1b.tar",
        prefix="ml-20mx16x32",
        convert_timestamp=False,
    )
    ml_3b_dp = MovielensSyntheticDataProcessor(  # pyre-ignore [45]
        prefix="ml-3b",
        expected_num_unique_items=26743 * 32,
        expected_max_item_id=26743 * 32,
    )
    amzn_books_dp = AmazonDataProcessor(  # pyre-ignore [45]
        "http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Books.csv",
        "tmp/ratings_Books.csv",
        prefix="amzn_books",
        expected_num_unique_items=695762,
    )
    return {
        "ml-1m": ml_1m_dp,
        "ml-20m": ml_20m_dp,
        "ml-1b": ml_1b_dp,
        "ml-3b": ml_3b_dp,
        "amzn-books": amzn_books_dp,
    }

</content>

<content full_path="generative_recommenders/data/dataset.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# pyre-unsafe

import csv
import linecache

from typing import Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
import torch


class DatasetV2(torch.utils.data.Dataset):
    """In reverse chronological order."""

    def __init__(
        self,
        ratings_file: str,
        padding_length: int,
        ignore_last_n: int,  # used for creating train/valid/test sets
        shift_id_by: int = 0,
        chronological: bool = False,
        sample_ratio: float = 1.0,
    ) -> None:
        """
        Args:
            csv_file (string): Path to the csv file.
        """
        super().__init__()

        self.ratings_frame: pd.DataFrame = pd.read_csv(
            ratings_file,
            delimiter=",",
            # iterator=True,
        )
        self._padding_length: int = padding_length
        self._ignore_last_n: int = ignore_last_n
        self._cache: Dict[int, Dict[str, torch.Tensor]] = dict()
        self._shift_id_by: int = shift_id_by
        self._chronological: bool = chronological
        self._sample_ratio: float = sample_ratio

    def __len__(self) -> int:
        return len(self.ratings_frame)

    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        if idx in self._cache.keys():
            return self._cache[idx]
        data = self.ratings_frame.iloc[idx]
        sample = self.load_item(data)
        self._cache[idx] = sample
        return sample

    def load_item(self, data) -> Dict[str, torch.Tensor]:
        user_id = data.user_id

        def eval_as_list(x: str, ignore_last_n: int) -> List[int]:
            y = eval(x)
            y_list = [y] if type(y) == int else list(y)
            if ignore_last_n > 0:
                # for training data creation
                y_list = y_list[:-ignore_last_n]
            return y_list

        def eval_int_list(
            x: str,
            target_len: int,
            ignore_last_n: int,
            shift_id_by: int,
            sampling_kept_mask: Optional[List[bool]],
        ) -> Tuple[List[int], int]:
            y = eval_as_list(x, ignore_last_n=ignore_last_n)
            if sampling_kept_mask is not None:
                y = [x for x, kept in zip(y, sampling_kept_mask) if kept]
            y_len = len(y)
            y.reverse()
            if shift_id_by > 0:
                y = [x + shift_id_by for x in y]
            return y, y_len

        if self._sample_ratio < 1.0:
            raw_length = len(eval_as_list(data.sequence_item_ids, self._ignore_last_n))
            sampling_kept_mask = (
                torch.rand((raw_length,), dtype=torch.float32) < self._sample_ratio
            ).tolist()
        else:
            sampling_kept_mask = None

        movie_history, movie_history_len = eval_int_list(
            data.sequence_item_ids,
            self._padding_length,
            self._ignore_last_n,
            shift_id_by=self._shift_id_by,
            sampling_kept_mask=sampling_kept_mask,
        )
        movie_history_ratings, ratings_len = eval_int_list(
            data.sequence_ratings,
            self._padding_length,
            self._ignore_last_n,
            0,
            sampling_kept_mask=sampling_kept_mask,
        )
        movie_timestamps, timestamps_len = eval_int_list(
            data.sequence_timestamps,
            self._padding_length,
            self._ignore_last_n,
            0,
            sampling_kept_mask=sampling_kept_mask,
        )
        assert (
            movie_history_len == timestamps_len
        ), f"history len {movie_history_len} differs from timestamp len {timestamps_len}."
        assert (
            movie_history_len == ratings_len
        ), f"history len {movie_history_len} differs from ratings len {ratings_len}."

        def _truncate_or_pad_seq(
            y: List[int], target_len: int, chronological: bool
        ) -> List[int]:
            y_len = len(y)
            if y_len < target_len:
                y = y + [0] * (target_len - y_len)
            else:
                if not chronological:
                    y = y[:target_len]
                else:
                    y = y[-target_len:]
            assert len(y) == target_len
            return y

        historical_ids = movie_history[1:]
        historical_ratings = movie_history_ratings[1:]
        historical_timestamps = movie_timestamps[1:]
        target_ids = movie_history[0]
        target_ratings = movie_history_ratings[0]
        target_timestamps = movie_timestamps[0]
        if self._chronological:
            historical_ids.reverse()
            historical_ratings.reverse()
            historical_timestamps.reverse()

        max_seq_len = self._padding_length - 1
        history_length = min(len(historical_ids), max_seq_len)
        historical_ids = _truncate_or_pad_seq(
            historical_ids,
            max_seq_len,
            self._chronological,
        )
        historical_ratings = _truncate_or_pad_seq(
            historical_ratings,
            max_seq_len,
            self._chronological,
        )
        historical_timestamps = _truncate_or_pad_seq(
            historical_timestamps,
            max_seq_len,
            self._chronological,
        )
        # moved to features.py
        # if self._chronological:
        #     historical_ids.append(0)
        #     historical_ratings.append(0)
        #     historical_timestamps.append(0)
        # print(historical_ids, historical_ratings, historical_timestamps, target_ids, target_ratings, target_timestamps)
        ret = {
            "user_id": user_id,
            "historical_ids": torch.tensor(historical_ids, dtype=torch.int64),
            "historical_ratings": torch.tensor(historical_ratings, dtype=torch.int64),
            "historical_timestamps": torch.tensor(
                historical_timestamps, dtype=torch.int64
            ),
            "history_lengths": history_length,
            "target_ids": target_ids,
            "target_ratings": target_ratings,
            "target_timestamps": target_timestamps,
        }
        return ret


class MultiFileDatasetV2(DatasetV2, torch.utils.data.Dataset):
    def __init__(
        self,
        file_prefix: str,
        num_files: int,
        padding_length: int,
        ignore_last_n: int,  # used for creating train/valid/test sets
        shift_id_by: int = 0,
        chronological: bool = False,
        sample_ratio: float = 1.0,
    ) -> None:
        torch.utils.data.Dataset().__init__()
        self._file_prefix: str = file_prefix
        self._num_files: int = num_files
        with open(f"{file_prefix}_users.csv", "r") as file:
            reader = csv.reader(file)
            self.users_cumsum: List[int] = np.cumsum(
                [int(row[1]) for row in reader]
            ).tolist()
        self._padding_length: int = padding_length
        self._ignore_last_n: int = ignore_last_n
        self._shift_id_by: int = shift_id_by
        self._chronological: bool = chronological
        self._sample_ratio: float = sample_ratio

    def __len__(self) -> int:
        return self.users_cumsum[-1]

    def _process_line(self, line: str) -> pd.Series:
        reader = csv.reader([line])
        parsed_line = next(reader)
        user_id = int(parsed_line[0])
        sequence_item_ids = parsed_line[1]
        sequence_ratings = parsed_line[2]
        return pd.Series(
            data={
                "user_id": user_id,
                "sequence_item_ids": sequence_item_ids,
                "sequence_ratings": sequence_ratings,
                "sequence_timestamps": sequence_item_ids,  # placeholder
            }
        )

    def __getitem__(self, idx) -> Dict[str, torch.Tensor]:
        assert idx < self.users_cumsum[-1]
        file_idx: int = 0
        while self.users_cumsum[file_idx] <= idx:
            file_idx += 1
        if file_idx == 0:
            local_idx = idx
        else:
            local_idx = idx - self.users_cumsum[file_idx - 1]
        line = linecache.getline(f"{self._file_prefix}_{file_idx}.csv", local_idx + 1)
        data = self._process_line(line)
        sample = self.load_item(data)
        return sample

</content>

<content full_path="generative_recommenders/data/eval.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# pyre-unsafe

import logging
import sys
from dataclasses import dataclass
from typing import Callable, Dict, List, Optional, Set, Union

import torch
import torch.distributed as dist

from generative_recommenders.indexing.candidate_index import CandidateIndex, TopKModule
from generative_recommenders.modeling.sequential.features import SequentialFeatures
from generative_recommenders.rails.similarities.module import SimilarityModule
from torch.utils.tensorboard import SummaryWriter


logging.basicConfig(stream=sys.stdout, level=logging.INFO)


@dataclass
class EvalState:
    all_item_ids: Set[int]
    candidate_index: CandidateIndex
    top_k_module: TopKModule


def get_eval_state(
    model: SimilarityModule,
    all_item_ids: List[int],  # [X]
    negatives_sampler: torch.nn.Module,
    top_k_module_fn: Callable[[torch.Tensor, torch.Tensor], TopKModule],
    device: int,
    float_dtype: Optional[torch.dtype] = None,
) -> EvalState:
    # Exhaustively eval all items (incl. seen ids).
    eval_negatives_ids = torch.as_tensor(all_item_ids).to(device).unsqueeze(0)  # [1, X]
    # pyre-fixme[29]: `Union[Tensor, Module]` is not a function.
    eval_negative_embeddings = negatives_sampler.normalize_embeddings(
        # pyre-fixme[29]: `Union[Tensor, Module]` is not a function.
        model.get_item_embeddings(eval_negatives_ids)
    )
    if float_dtype is not None:
        eval_negative_embeddings = eval_negative_embeddings.to(float_dtype)
    candidates = CandidateIndex(
        ids=eval_negatives_ids,
        embeddings=eval_negative_embeddings,
    )
    return EvalState(
        all_item_ids=set(all_item_ids),
        candidate_index=candidates,
        top_k_module=top_k_module_fn(eval_negative_embeddings, eval_negatives_ids),
    )


@torch.inference_mode  # pyre-ignore [56]
def eval_metrics_v2_from_tensors(
    eval_state: EvalState,
    model: SimilarityModule,
    seq_features: SequentialFeatures,
    target_ids: torch.Tensor,  # [B, 1]
    min_positive_rating: int = 4,
    target_ratings: Optional[torch.Tensor] = None,  # [B, 1]
    epoch: Optional[str] = None,
    filter_invalid_ids: bool = True,
    user_max_batch_size: Optional[int] = None,
    dtype: Optional[torch.dtype] = None,
) -> Dict[str, Union[float, torch.Tensor]]:
    """
    Args:
        eval_negatives_ids: Optional[Tensor]. If not present, defaults to eval over
            the entire corpus (`num_items`) excluding all the items that users have
            seen in the past (historical_ids, target_ids). This is consistent with
            papers like SASRec and TDM but may not be fair in practice as retrieval
            modules don't have access to read state during the initial fetch stage.
        filter_invalid_ids: bool. If true, filters seen ids by default.
    Returns:
        keyed metric -> list of values for each example.
    """
    B, _ = target_ids.shape
    device = target_ids.device

    for target_id in target_ids:
        target_id = int(target_id)
        if target_id not in eval_state.all_item_ids:
            print(f"missing target_id {target_id}")

    # computes ro- part exactly once.
    # pyre-fixme[29]: `Union[Tensor, Module]` is not a function.
    shared_input_embeddings = model.encode(
        past_lengths=seq_features.past_lengths,
        past_ids=seq_features.past_ids,
        # pyre-fixme[29]: `Union[Tensor, Module]` is not a function.
        past_embeddings=model.get_item_embeddings(seq_features.past_ids),
        past_payloads=seq_features.past_payloads,
    )
    if dtype is not None:
        shared_input_embeddings = shared_input_embeddings.to(dtype)

    MAX_K = 2500
    k = min(MAX_K, eval_state.candidate_index.ids.size(1))
    user_max_batch_size = user_max_batch_size or shared_input_embeddings.size(0)
    num_batches = (
        shared_input_embeddings.size(0) + user_max_batch_size - 1
    ) // user_max_batch_size
    eval_top_k_ids_all = []
    eval_top_k_prs_all = []
    for mb in range(num_batches):
        eval_top_k_ids, eval_top_k_prs, _ = (
            eval_state.candidate_index.get_top_k_outputs(
                query_embeddings=shared_input_embeddings[
                    mb * user_max_batch_size : (mb + 1) * user_max_batch_size, ...
                ],
                top_k_module=eval_state.top_k_module,
                k=k,
                invalid_ids=(
                    seq_features.past_ids[
                        mb * user_max_batch_size : (mb + 1) * user_max_batch_size, :
                    ]
                    if filter_invalid_ids
                    else None
                ),
                return_embeddings=False,
            )
        )
        eval_top_k_ids_all.append(eval_top_k_ids)
        eval_top_k_prs_all.append(eval_top_k_prs)

    if num_batches == 1:
        eval_top_k_ids = eval_top_k_ids_all[0]
        eval_top_k_prs = eval_top_k_prs_all[0]
    else:
        eval_top_k_ids = torch.cat(eval_top_k_ids_all, dim=0)
        eval_top_k_prs = torch.cat(eval_top_k_prs_all, dim=0)

    assert eval_top_k_ids.size(1) == k
    _, eval_rank_indices = torch.max(
        torch.cat(
            [eval_top_k_ids, target_ids],
            dim=1,
        )
        == target_ids,
        dim=1,
    )
    eval_ranks = torch.where(eval_rank_indices == k, MAX_K + 1, eval_rank_indices + 1)

    output = {
        "ndcg@1": torch.where(
            eval_ranks <= 1,
            torch.div(1.0, torch.log2(eval_ranks + 1)),
            torch.zeros(1, dtype=torch.float32, device=device),
        ),
        "ndcg@10": torch.where(
            eval_ranks <= 10,
            torch.div(1.0, torch.log2(eval_ranks + 1)),
            torch.zeros(1, dtype=torch.float32, device=device),
        ),
        "ndcg@50": torch.where(
            eval_ranks <= 50,
            torch.div(1.0, torch.log2(eval_ranks + 1)),
            torch.zeros(1, dtype=torch.float32, device=device),
        ),
        "ndcg@100": torch.where(
            eval_ranks <= 100,
            torch.div(1.0, torch.log2(eval_ranks + 1)),
            torch.zeros(1, dtype=torch.float32, device=device),
        ),
        "ndcg@200": torch.where(
            eval_ranks <= 200,
            torch.div(1.0, torch.log2(eval_ranks + 1)),
            torch.zeros(1, dtype=torch.float32, device=device),
        ),
        "hr@1": (eval_ranks <= 1),
        "hr@10": (eval_ranks <= 10),
        "hr@50": (eval_ranks <= 50),
        "hr@100": (eval_ranks <= 100),
        "hr@200": (eval_ranks <= 200),
        "hr@500": (eval_ranks <= 500),
        "hr@1000": (eval_ranks <= 1000),
        "mrr": torch.div(1.0, eval_ranks),
    }
    if target_ratings is not None:
        target_ratings = target_ratings.squeeze(1)  # [B]
        output["ndcg@10_>=4"] = torch.where(
            eval_ranks[target_ratings >= 4] <= 10,
            torch.div(1.0, torch.log2(eval_ranks[target_ratings >= 4] + 1)),
            torch.zeros(1, dtype=torch.float32, device=device),
        )
        output[f"hr@10_>={min_positive_rating}"] = (
            eval_ranks[target_ratings >= min_positive_rating] <= 10
        )
        output[f"hr@50_>={min_positive_rating}"] = (
            eval_ranks[target_ratings >= min_positive_rating] <= 50
        )
        output[f"mrr_>={min_positive_rating}"] = torch.div(
            1.0, eval_ranks[target_ratings >= min_positive_rating]
        )

    return output  # pyre-ignore [7]


def eval_recall_metrics_from_tensors(
    eval_state: EvalState,
    model: SimilarityModule,
    seq_features: SequentialFeatures,
    user_max_batch_size: Optional[int] = None,
    dtype: Optional[torch.dtype] = None,
) -> Dict[str, torch.Tensor]:
    target_ids = seq_features.past_ids[:, -1].unsqueeze(1)
    filtered_past_ids = seq_features.past_ids.detach().clone()
    filtered_past_ids[:, -1] = torch.zeros_like(target_ids.squeeze(1))
    return eval_metrics_v2_from_tensors(
        eval_state=eval_state,
        model=model,
        seq_features=SequentialFeatures(
            past_lengths=seq_features.past_lengths - 1,
            past_ids=filtered_past_ids,
            past_embeddings=seq_features.past_embeddings,
            past_payloads=seq_features.past_payloads,
        ),
        target_ids=target_ids,
        user_max_batch_size=user_max_batch_size,
        dtype=dtype,
    )


def _avg(x: torch.Tensor, world_size: int) -> torch.Tensor:
    _sum_and_numel = torch.tensor(
        [x.sum(), x.numel()], dtype=torch.float32, device=x.device
    )
    if world_size > 1:
        dist.all_reduce(_sum_and_numel, op=dist.ReduceOp.SUM)
    return _sum_and_numel[0] / _sum_and_numel[1]


def add_to_summary_writer(
    writer: Optional[SummaryWriter],
    batch_id: int,
    metrics: Dict[str, torch.Tensor],
    prefix: str,
    world_size: int,
) -> None:
    for key, values in metrics.items():
        avg_value = _avg(values, world_size)
        if writer is not None:
            writer.add_scalar(f"{prefix}/{key}", avg_value, batch_id)

</content>

<content full_path="generative_recommenders/trainer/data_loader.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# pyre-unsafe

import os
from typing import Optional, Tuple

import gin
import torch


@gin.configurable  # Allows this function to be configured via gin
def create_data_loader(
    dataset: torch.utils.data.Dataset,  # Input dataset to load
    batch_size: int,                    # Number of samples per batch
    world_size: int,                    # Total number of distributed processes
    rank: int,                          # Current process rank in distributed setup
    shuffle: bool,                      # Whether to shuffle the dataset
    prefetch_factor: int = 128,         # Number of samples loaded in advance by each worker
    num_workers: Optional[int] = os.cpu_count(),  # Number of subprocesses for data loading
    drop_last: bool = False,            # Whether to drop the last incomplete batch
) -> Tuple[
    Optional[torch.utils.data.distributed.DistributedSampler[torch.utils.data.Dataset]],
    torch.utils.data.DataLoader,
]:
    """Creates a DataLoader with optional distributed sampling support.

    This function sets up a PyTorch DataLoader with distributed training capabilities.
    When shuffle is True, it uses DistributedSampler to handle data partitioning
    across multiple processes in distributed training.

    Args:
        dataset: The dataset to load
        batch_size: How many samples per batch to load
        world_size: Number of processes participating in distributed training
        rank: Process rank within distributed training
        shuffle: If True, data is shuffled and DistributedSampler is used
        prefetch_factor: Number of batches to prefetch per worker
        num_workers: Number of worker processes for data loading
        drop_last: If True, drop the last incomplete batch

    Returns:
        A tuple containing:
        - The DistributedSampler if shuffle is True, None otherwise
        - The configured DataLoader instance
          
    References:
        https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader
            - Main PyTorch data loading utility that combines a dataset and sampler
            - Provides single/multi-process data loading with customizable loading order
            - Supports automatic batching, memory pinning, and custom collation
        https://pytorch.org/docs/stable/data.html#torch.utils.data.distributed.DistributedSampler
            - Sampler that restricts data loading to a subset of the dataset in distributed training
            - Automatically partitions dataset across processes/GPUs for parallel training
            - Ensures each process sees different data when shuffling is enabled
    """
    if shuffle:
        # Create a distributed sampler when shuffling is requested
        # This ensures each process gets a different slice of the dataset
        sampler = torch.utils.data.distributed.DistributedSampler(
            dataset,
            num_replicas=world_size,  # Total number of processes
            rank=rank,                # Current process ID
            shuffle=True,             # Shuffle data within each process
            seed=0,                   # Fixed seed for reproducibility
            drop_last=drop_last,      # Whether to drop last incomplete batch
        )
        # A sampler is needed for distributed training to:
        # 1. Partition data across processes - ensures each GPU/process works on different data
        # 2. Maintain balanced workload - each process gets roughly equal amount of samples
        # 3. Avoid data duplication - prevents multiple processes from processing same samples
        # 4. Enable proper shuffling - coordinates shuffling across processes to prevent overlap
        #
        # The sampler achieves these goals by:
        # a. Using num_replicas to divide dataset size by total processes
        # b. Using rank to determine which subset this process handles
        # c. Maintaining internal indices to track data partitioning
        # d. Coordinating shuffling across processes with a shared seed
        # e. Optionally dropping remainder samples to keep batches balanced
else:
        # No sampler needed when not shuffling because:
        # 1. Data order is deterministic - each process can use simple indexing
        # 2. Each process can calculate its subset using rank and world_size directly
        # 3. No need to coordinate random shuffling across processes
        # 4. Basic sequential access is sufficient for non-shuffled distributed training
        sampler = None

    # Create and return the DataLoader
    #
    # The DataLoader is responsible for:
    # 1. Batching individual samples from the dataset into mini-batches (groups of samples
    #    processed together to balance computational efficiency and memory usage)
    # 2. Shuffling the data if specified (via sampler in distributed mode)
    # 3. Loading data in parallel using multiple worker processes
    # 4. Prefetching batches to optimize GPU utilization
    # 5. Handling the data pipeline from dataset to training loop efficiently
    data_loader = torch.utils.data.DataLoader(
        dataset,
        batch_size=batch_size,
        # shuffle=True cannot be used with sampler
        num_workers=num_workers or 0,  # Fall back to 0 if num_workers is None
        sampler=sampler,
        # Number of batches to prefetch per worker process
        # Controls how many batches each worker loads ahead of time to reduce I/O bottlenecks
        # Higher values increase memory usage but can improve throughput by overlapping data loading with training
        prefetch_factor=prefetch_factor,
    )
    return sampler, data_loader

</content>

<content full_path="generative_recommenders/trainer/train.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# pyre-unsafe

import logging
import os
import random

import time

from datetime import date
from typing import Dict, Optional

import gin

import torch
import torch.distributed as dist

from generative_recommenders.data.eval import (
    _avg,
    add_to_summary_writer,
    eval_metrics_v2_from_tensors,
    get_eval_state,
)

from generative_recommenders.data.reco_dataset import get_reco_dataset
from generative_recommenders.indexing.utils import get_top_k_module
from generative_recommenders.modeling.sequential.autoregressive_losses import (
    BCELoss,
    InBatchNegativesSampler,
    LocalNegativesSampler,
)
from generative_recommenders.modeling.sequential.embedding_modules import (
    EmbeddingModule,
    LocalEmbeddingModule,
)
from generative_recommenders.modeling.sequential.encoder_utils import (
    get_sequential_encoder,
)
from generative_recommenders.modeling.sequential.features import (
    movielens_seq_features_from_row,
)
from generative_recommenders.modeling.sequential.input_features_preprocessors import (
    LearnablePositionalEmbeddingInputFeaturesPreprocessor,
)
from generative_recommenders.modeling.sequential.losses.sampled_softmax import (
    SampledSoftmaxLoss,
)
from generative_recommenders.modeling.sequential.output_postprocessors import (
    L2NormEmbeddingPostprocessor,
    LayerNormEmbeddingPostprocessor,
)
from generative_recommenders.modeling.similarity_utils import get_similarity_function
from generative_recommenders.trainer.data_loader import create_data_loader
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.tensorboard import SummaryWriter

"""
Main training script for generative recommender systems. This module implements a distributed training
pipeline for sequential recommendation models with various embedding, loss, and sampling strategies.

Key components:
- Distributed training setup using PyTorch DDP
- Configurable model architecture (SASRec etc.)
- Multiple negative sampling strategies
- Evaluation pipeline with metrics like NDCG, HR, MRR
- TensorBoard logging and checkpoint saving
"""

def setup(rank: int, world_size: int, master_port: int) -> None:
    """
    Initialize distributed training environment.
    
    Args:
        rank: Process rank/GPU ID
        world_size: Total number of processes/GPUs
        master_port: Port for distributed coordination
    
    References:
        https://pytorch.org/docs/stable/distributed.html
    """
    os.environ["MASTER_ADDR"] = "localhost"
    # Port for distributed coordination between processes
    # References:
        # https://pytorch.org/docs/stable/distributed.html#tcp-initialization
        # Port used for TCP communication between processes during distributed training
        # Should be a free port on the machine that won't conflict with other services
    os.environ["MASTER_PORT"] = str(master_port)

    # initialize the process group
    # https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group
    dist.init_process_group("nccl", rank=rank, world_size=world_size)


def cleanup() -> None:
    """
    Cleanup distributed training environment.
    
    References:
        https://pytorch.org/docs/stable/distributed.html#torch.distributed.destroy_process_group
"""
    dist.destroy_process_group()


@gin.configurable
def get_weighted_loss(
    main_loss: torch.Tensor,
    aux_losses: Dict[str, torch.Tensor],
    weights: Dict[str, float],
) -> torch.Tensor:
    """
    Compute weighted combination of main loss and auxiliary losses.
    
    Args:
        main_loss: Primary training loss
        aux_losses: Dictionary of auxiliary loss terms
        weights: Weight coefficients for auxiliary losses
    
    Returns:
        Combined weighted loss
    """
    weighted_loss = main_loss
    for key, weight in weights.items():
        cur_weighted_loss = aux_losses[key] * weight
        weighted_loss = weighted_loss + cur_weighted_loss
    return weighted_loss


@gin.configurable
def train_fn(
    # Process rank in distributed training setup (0 to world_size-1)
    # References:
        # https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group
        # Process rank in distributed training setup (0 to world_size-1)
    rank: int,
    # Total number of processes participating in distributed training
    # References:
        # https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group
        # Total number of processes participating in distributed training
    world_size: int,
    # Port used for distributed process coordination
    # References:
        # https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group
        # Port used for distributed process coordination
        # Should be a free port on the machine that won't conflict with other services
    master_port: int,
    # Name of dataset to use for training (e.g. "ml-20m" for MovieLens 20M)
    dataset_name: str = "ml-20m",
    # Maximum length of input sequence to consider
    # References:
        # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html
        # Maximum sequence length is needed to:
        # 1. Control memory usage - longer sequences require more memory for attention
        # 2. Improve efficiency - shorter sequences are faster to process
        # 3. Handle variable length data - provides consistent truncation point
        # 4. Match model architecture - transformer models have position embedding limits
    max_sequence_length: int = 200,
    # Ratio for sampling positions in sequence during training
    # References:
# https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html
        # Positional sampling ratio < 1.0 can be beneficial for:
        # 1. Memory efficiency - sampling fewer positions reduces memory usage during training
        # 2. Training speed - processing fewer positions per sequence is faster
        # 3. Regularization - random sampling of positions acts as a form of data augmentation
        # 4. Handling long sequences - allows training on longer sequences by sampling subset of positions
        # 5. Curriculum learning - can gradually increase ratio to help model learn position dependencies
        # While still allowing model to learn sequential patterns, just from a subset of positions
        # 
        # Positional sampling ratio = 1.0 means all positions are sampled
        # Positional sampling ratio < 1.0 means fewer positions are sampled
        # Positional sampling ratio > 1.0 means more positions are sampled
    positional_sampling_ratio: float = 1.0,
    # Batch size per GPU/process
    # References:
        # Batch size per GPU/process
        # Called "local" because in distributed training each GPU/process has its own batch size
        # The effective global batch size = local_batch_size * world_size (number of GPUs)
        # Should be chosen based on:
        # 1. Individual GPU memory capacity since each GPU processes this many samples
        # 2. Model complexity and parameters per GPU
        # 3. Training speed and efficiency per GPU
        # 4. Regularization effects of per-GPU batch size
        # 5. Data loading and processing capabilities per GPU
    local_batch_size: int = 128,
    # Batch size to use during evaluation
    # References:
        # Batch size to use during evaluation
    eval_batch_size: int = 128,
    # Maximum batch size for user evaluation, optional limit
    # References:
        # Maximum batch size for user evaluation, optional limit
        # Used to limit the number of user evaluations processed in a single batch
        # This is useful for debugging and ensuring that the evaluation process is manageable
        # If not specified, the default behavior is to process all users in a single batch
    eval_user_max_batch_size: Optional[int] = None,
    # Model architecture to use ("SASRec" or "HSTU")
    main_module: str = "SASRec",
    # Whether to use bfloat16 precision for main module
    # References:
        # https://pytorch.org/docs/stable/generated/torch.bfloat16.html
        # Bfloat16 is a 16-bit floating-point format that provides a good compromise between precision and performance
        # It is faster than FP32 but less precise than FP16
    main_module_bf16: bool = False,
    # Probability of dropping units during training
    # References:
        # https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        # Dropout is a regularization technique that randomly drops units (neurons) in a neural network during training
        # This helps prevent overfitting by forcing the network to learn more robust features
        # The dropout rate is the fraction of units that are dropped
    dropout_rate: float = 0.2,
    # Type of normalization to apply to user embeddings
    # References:
        # https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html
        # https://pytorch.org/docs/stable/generated/torch.nn.L2Norm.html
        # User embeddings are normalized to prevent issues with large values
        # L2 normalization is used to scale the embeddings to unit length
        # Layer normalization is used to scale the embeddings to unit length
        # Both methods help with numerical stability and training stability
    user_embedding_norm: str = "l2_norm",
    # Strategy for sampling negative examples
    # References:
        # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html
        # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html
        # Sampling negative examples is a technique used to train models to distinguish between positive and negative examples
        # In a typical recommendation system, positive examples are the items that the user has interacted with (e.g. clicked, rated, etc.)
        # Negative examples are items that the user has not interacted with (e.g. items that the user has not clicked, rated, etc.)
        # The goal is to learn a model that can predict the likelihood of a user interacting with an item based on their historical interactions
        # The sampling strategy determines how negative examples are sampled
        # In-batch sampling is a simple strategy where negative examples are sampled from the same batch as positive examples
        # Local sampling is a more complex strategy where negative examples are sampled from the entire dataset
        # The choice of sampling strategy can impact the model's ability to learn and generalize
        #
        # More details on in-batch sampling:
        # - For each user in a batch, other items in that batch are treated as negative examples
        # - Example: In a batch with users u1,u2,u3 and their positive items i1,i2,i3:
        #   * For user u1: items i2,i3 become negative examples
        #   * For user u2: items i1,i3 become negative examples  
        #   * For user u3: items i1,i2 become negative examples
        # - This is computationally efficient as it reuses items already loaded in memory
        # - Also provides implicit regularization since popular items appear more as negatives
        # - Alternative is sampling from full item set which may give better negatives but is more expensive
    sampling_strategy: str = "in-batch",
    # Loss function module to use for training
    # References:
        # Loss function module to use for training
        #
        # The loss_module parameter is used in train.py around line 500 to determine which loss function to use:
        # - If "BCELoss": Uses binary cross entropy loss
        # - If "SampledSoftmaxLoss": Uses sampled softmax with negative sampling
        #
        # SampledSoftmaxLoss is a loss function that approximates the full softmax by sampling negative examples
        # Instead of computing probabilities over all items (which is computationally expensive), it:
        # 1. Takes the positive (true) item for each user
        # 2. Samples a fixed number of negative items (controlled by num_negatives parameter)
        # 3. Computes softmax only over this smaller subset of items
        # 4. Applies temperature scaling (controlled by temperature parameter) to control prediction sharpness
        #
        # Key benefits:
        # - Much faster training compared to full softmax over all items
        # - Memory efficient since only needs to store embeddings for sampled items
        # - Still provides good model quality with sufficient negative samples
        # - Temperature scaling helps balance exploration vs exploitation
        #
        # The loss can optionally use activation checkpointing to save memory at the cost of recomputation
        #
        # BCELoss is a simpler alternative that uses binary cross entropy to train the model
        # It treats each user-item interaction as an independent binary classification problem
        #
        # Advantages:
        # - Simple and stable training objective
        # - Works well with in-batch negative sampling
        # - Memory efficient as it doesn't require storing full item embedding matrix
        #
        # Disadvantages:
        # - May be less effective than softmax for ranking
        # - Limited to binary decisions rather than full probability distribution
        # - Requires careful negative sampling strategy
    loss_module: str = "SampledSoftmaxLoss",
    # Optional weights for different loss components
    # References:
        # Optional weights for different loss components
        #
        # The loss_weights parameter is used in train.py around line 500 to determine the weights for different loss components
        # This allows for a weighted combination of losses to improve training stability and convergence
        # The weights are a dictionary where the keys are the names of the loss components and the values are the weights
        # The weights are used to scale the loss contributions of each component
        # The sum of the weights should equal 1.0 to ensure that the total loss is a valid probability distribution
    loss_weights: Optional[Dict[str, float]] = {},
    # Number of negative samples per positive example
    # References:
        # Number of negative samples per positive example
        #
        # The num_negatives parameter is used in train.py around line 500 to determine the number of negative samples to use for each positive example
        # This is a hyperparameter that controls the balance between positive and negative examples in the training process
        # A higher number of negative samples can help the model learn to distinguish between positive and negative examples
        # However, too many negative samples can lead to overfitting and poor generalization
        # The optimal number of negative samples depends on the dataset and model complexity
        # In practice, a value between 1 and 10 is often used
    num_negatives: int = 1,
    # Whether to use activation checkpointing in loss computation
    # References:
        # Whether to use activation checkpointing in loss computation
        #
        # Activation checkpointing is a technique used to save memory during training by recomputing activations instead of storing them
        # This is useful for models with a large number of parameters and activations, as it reduces memory usage
        # The loss_activation_checkpoint parameter is used in train.py around line 500 to determine whether to use activation checkpointing in the loss computation
        # This is a boolean parameter that defaults to False
        # If set to True, activation checkpointing will be used in the loss computation
        # If set to False, activation checkpointing will not be used in the loss computation
    loss_activation_checkpoint: bool = False,
    # Whether to apply L2 normalization to item embeddings
    item_l2_norm: bool = False,
    # Temperature scaling factor for loss computation
    # References:
        # The temperature parameter is used in train.py around line 500 to determine the temperature scaling factor for the loss computation
        # This is a hyperparameter that controls the sharpness of the model's predictions
        # A higher temperature results in more uniform predictions, while a lower temperature results in more peaked predictions
        # The optimal temperature depends on the dataset and model complexity
        # In practice, a value between 0.01 and 1.0 is often used
    temperature: float = 0.05,
    # Total number of training epochs
    # References:
        # Total number of training epochs
        #
        # The num_epochs parameter is used in train.py around line 500 to determine the total number of training epochs
        # This is a hyperparameter that controls the number of times the entire dataset is passed through the model during training
        # A higher number of epochs can help the model learn more complex patterns in the data
        # However, too many epochs can lead to overfitting and poor generalization
        # The optimal number of epochs depends on the dataset and model complexity
        #
        # Values used in .gin config files: 100, 101 or 201
        #
        # The value of 101 epochs was chosen empirically to ensure convergence while avoiding overfitting
    num_epochs: int = 101,
    # Base learning rate for optimization
    # References:
        # Base learning rate for optimization
        #
        # The learning_rate parameter is used in train.py around line 500 to determine the base learning rate for the optimizer
        # This is a hyperparameter that controls the step size for updating the model's parameters during training
        # A higher learning rate can help the model learn more quickly, but too high a rate can lead to unstable training
        # A lower learning rate can help the model learn more slowly, but too low a rate can lead to slow convergence
        # The optimal learning rate depends on the dataset and model complexity
        # In practice, a value between 1e-3 and 1e-1 is often used
    learning_rate: float = 1e-3,
    # Number of steps for learning rate warmup
    # References:
        # Number of steps for learning rate warmup
        #
        # The num_warmup_steps parameter is used in train.py around line 500 to determine the number of steps for learning rate warmup
        # This is a hyperparameter that controls the number of steps for the learning rate to increase from 0 to the base learning rate
        # Warmup is used to prevent the learning rate from being too high at the start of training, which can lead to unstable training
        # The optimal number of warmup steps depends on the dataset and model complexity
        # In practice, a value between 1000 and 10000 is often used
    num_warmup_steps: int = 0,
    # L2 regularization coefficient
    # References:
        # L2 regularization coefficient
        #
        # The weight_decay parameter is used in train.py around line 500 to determine the L2 regularization coefficient for the optimizer
        # This is a hyperparameter that controls the strength of L2 regularization
        # L2 regularization adds a penalty term of 0.5 * weight_decay * ||w||^2 to the loss function
        # where ||w||^2 is the squared L2 norm of all model parameters
        # This is implemented in PyTorch's AdamW optimizer at line 1000 in torch/optim/adamw.py
        # References:
            # https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html
        # This penalty encourages the model's parameters to be small to prevent overfitting
        # The optimal weight decay depends on the dataset and model complexity
        # In practice, a value between 1e-3 and 1e-1 is often used
        #
    weight_decay: float = 1e-3,
    # Method for computing top-k predictions
    top_k_method: str = "MIPSBruteForceTopK",
    # Number of steps between evaluation runs
    # References:
        # Number of steps between evaluation runs
        #
        # The eval_interval parameter is used in train.py around line 500 to determine the number of steps between evaluation runs
        # This is a hyperparameter that controls the frequency of evaluation during training
        # Evaluating the model periodically can help detect overfitting and improve generalization
        # The optimal eval_interval depends on the dataset and model complexity
        # In practice, a value between 100 and 1000 is often used
    eval_interval: int = 100,
    # Number of epochs between full evaluations
    # References:
        # Number of epochs between full evaluations
        #
        # The full_eval_every_n parameter is used in train.py around line 500 to determine the number of epochs between full evaluations
        # This is a hyperparameter that controls the frequency of full evaluations during training
        # Full evaluations can help detect overfitting and improve generalization
        # The optimal full_eval_every_n depends on the dataset and model complexity
        # In practice, a value between 1 and 10 is often used
    full_eval_every_n: int = 1,
    # Number of steps between saving checkpoints
    # References:
        # Number of steps between saving checkpoints
        #
        # The save_ckpt_every_n parameter is used in train.py around line 500 to determine the number of steps between saving checkpoints
        # This is a hyperparameter that controls the frequency of checkpoint saving during training
        # Checkpoints are used to save the model's state and restore it later
        # The optimal save_ckpt_every_n depends on the dataset and model complexity
        # In practice, a value between 1000 and 10000 is often used
    save_ckpt_every_n: int = 1000,
    # Number of iterations for partial evaluation
    # References:
        # Number of iterations for partial evaluation
        #
        # The partial_eval_num_iters parameter is used in train.py around line 500 to determine the number of iterations for partial evaluation
        # This is a hyperparameter that controls the number of iterations for partial evaluation during training
        # Partial evaluation can help detect overfitting and improve generalization
        # The optimal partial_eval_num_iters depends on the dataset and model complexity
        # In practice, a value between 10 and 100 is often used
    partial_eval_num_iters: int = 32,
    # Type of embedding module to use
    # References:
        # Type of embedding module to use
        #
        # The embedding_module_type parameter is used in train.py around line 500 to determine the type of embedding module to use
        # This is a hyperparameter that controls the type of embedding module used in the model
        # The embedding module is responsible for transforming the input data into dense vectors
        # The embedding module can impact the model's ability to learn and generalize
        # The optimal embedding_module_type depends on the dataset and model complexity
        # In practice, a value between "local" and "distributed" is often used
    embedding_module_type: str = "local",
    # Dimension of item embedding vectors
    # References:
        # Dimension of item embedding vectors
        #
        # The item_embedding_dim parameter is used in train.py around line 500 to determine the dimension of item embedding vectors
        # This is a hyperparameter that controls the dimensionality of the item embeddings
        # The item embeddings are used to represent the items in the dataset
        # The item embedding dimension can impact the model's ability to learn and generalize
        # The optimal item_embedding_dim depends on the dataset and model complexity
        # In practice, a value between 100 and 1000 is often used
    item_embedding_dim: int = 240,
    # Type of interaction module to use
    interaction_module_type: str = "",
    # Length of output sequence for generative models
    # References:
        # Length of output sequence for generative models
        #
        # The gr_output_length parameter is used in train.py around line 500 to determine the length of the output sequence for generative models
        # This is a hyperparameter that controls the length of the output sequence for the generative model
        # The output sequence is used to generate the next item in the sequence
        # The optimal gr_output_length depends on the dataset and model complexity
        # In practice, a value between 1 and 10 is often used
    gr_output_length: int = 10,
    # Small constant for numerical stability in L2 norm
    # References:
        # Small constant for numerical stability in L2 norm
        #
        # The l2_norm_eps parameter is used in train.py around line 500 to determine the small constant for numerical stability in L2 norm
        # This is a hyperparameter that controls the small constant for numerical stability in the L2 norm
        # The L2 norm is used to normalize the item embeddings
        # The optimal l2_norm_eps depends on the dataset and model complexity
        # In practice, a value between 1e-6 and 1e-3 is often used
    l2_norm_eps: float = 1e-6,
    # Whether to enable TensorFloat-32 precision
    # References:
        # Whether to enable TensorFloat-32 precision
        #
        # The enable_tf32 parameter is used in train.py around line 500 to determine whether to enable TensorFloat-32 precision
        # This is a hyperparameter that controls whether to enable TensorFloat-32 precision
        # TensorFloat-32 (TF32) is a precision mode for NVIDIA GPUs that provides a balance between performance and accuracy
        # Enabling TF32 can improve the model's performance and accuracy
        # The optimal enable_tf32 depends on the dataset and model complexity
        # In practice, a value between True and False is often used
    enable_tf32: bool = False,
    # Random seed for reproducibility
    # References:
        # Random seed for reproducibility
        #
        # The random_seed parameter is used in train.py around line 500 to determine the random seed for reproducibility
        # This is a hyperparameter that controls the random seed for the random number generator
        # Setting a random seed ensures that the same random initialization is used for each run
        # This can help with reproducibility and debugging
        # The optimal random_seed depends on the dataset and model complexity
        # In practice, a value between 0 and 1000 is often used
    random_seed: int = 42,
) -> None:
    # to enable more deterministic results.
    """
    Main training function implementing the complete training pipeline.
    
    Key steps:
    1. Initialize distributed training
    2. Set up dataset and dataloaders
    3. Configure model components:
       - Embedding module
       - Interaction module
       - Input/output processors
       - Loss functions
       - Negative samplers
    4. Training loop with:
       - Forward/backward passes
       - Periodic evaluation
       - Checkpoint saving
       - TensorBoard logging
    
    Args:
        rank: Process rank for distributed training
        world_size: Total number of processes
        master_port: Port for distributed coordination
        dataset_name: Name of dataset to train on
        max_sequence_length: Maximum length of input sequences
        positional_sampling_ratio: Ratio for positional sampling
        local_batch_size: Per-GPU batch size
        eval_batch_size: Batch size during evaluation
        eval_user_max_batch_size: Maximum batch size for user evaluation
        main_module: Model architecture (e.g. "SASRec")
        main_module_bf16: Whether to use bfloat16 precision
        dropout_rate: Dropout probability
        user_embedding_norm: Type of user embedding normalization
        sampling_strategy: Strategy for negative sampling
        loss_module: Loss function module to use
        loss_weights: Optional weights for different loss components
        num_negatives: Number of negative samples per positive
        loss_activation_checkpoint: Whether to use activation checkpointing
        item_l2_norm: Whether to use L2 normalization on item embeddings
        temperature: Temperature parameter for loss scaling
        num_epochs: Number of training epochs
        learning_rate: Learning rate for optimization
        num_warmup_steps: Steps for learning rate warmup
        weight_decay: L2 regularization factor
        top_k_method: Method for computing top-k predictions
        eval_interval: How often to run evaluation
        full_eval_every_n: Epochs between full evaluations
        save_ckpt_every_n: Steps between checkpoints
        partial_eval_num_iters: Number of iterations for partial eval
        embedding_module_type: Type of embedding module
        item_embedding_dim: Dimension of item embeddings
        interaction_module_type: Type of interaction module
        gr_output_length: Output sequence length for generative model
        l2_norm_eps: Epsilon for L2 normalization
        enable_tf32: Whether to enable TF32 precision
        random_seed: Random seed for reproducibility
    """
    
    # === Initialize training environment ===
    random.seed(random_seed)
    torch.backends.cuda.matmul.allow_tf32 = enable_tf32
    torch.backends.cudnn.allow_tf32 = enable_tf32
    logging.info(f"cuda.matmul.allow_tf32: {enable_tf32}")
    logging.info(f"cudnn.allow_tf32: {enable_tf32}")
    logging.info(f"Training model on rank {rank}.")
    setup(rank, world_size, master_port)

    # === Dataset setup ===
    # Get the recommendation dataset with specified parameters:
    # - dataset_name: Name of the dataset to load (e.g. MovieLens)
    # - max_sequence_length: Maximum length of user interaction sequences
    # - chronological: Whether to maintain temporal order of interactions
    # - positional_sampling_ratio: Controls sampling of positions in sequences
    dataset = get_reco_dataset(
        dataset_name=dataset_name,
        max_sequence_length=max_sequence_length,
        chronological=True,
        positional_sampling_ratio=positional_sampling_ratio,
    )

    # === Create data loaders ===
    # Training loader with shuffling enabled
    train_data_sampler, train_data_loader = create_data_loader(
        dataset.train_dataset,
        batch_size=local_batch_size,
        world_size=world_size,
        rank=rank,
        shuffle=True,
        drop_last=world_size > 1,
    )
    # Evaluation loader
    eval_data_sampler, eval_data_loader = create_data_loader(
        dataset.eval_dataset,
        batch_size=eval_batch_size,
        world_size=world_size,
        rank=rank,
        shuffle=True,  # needed for partial eval
        drop_last=world_size > 1,
    )

    model_debug_str = main_module
    # === Model Configuration ===
    # Initialize the embedding module based on the specified type (local or distributed)
    # Currently only supports "local" embeddings stored on a single device
    # The embedding module maps item IDs to dense vector representations
    if embedding_module_type == "local":
        # LocalEmbeddingModule creates embeddings stored on a single device
        # num_items: Total number of unique items in the dataset
        # item_embedding_dim: Dimension of the embedding vectors for each item
        embedding_module: EmbeddingModule = LocalEmbeddingModule(
            # References:
                # LocalEmbeddingModule
                #
                # This is a module that creates embeddings stored on a single device
                # The embedding module maps item IDs to dense vector representations
                # The embedding module is used in the model to represent the items in the dataset
                # The embedding module is created with the following parameters:
                # - num_items: Total number of unique items in the dataset
                # - item_embedding_dim: Dimension of the embedding vectors for each item
            num_items=dataset.max_item_id,
            item_embedding_dim=item_embedding_dim,
        )
    else:
        # Future support for distributed embeddings could be added here
        raise ValueError(f"Unknown embedding_module_type {embedding_module_type}")
    
    # Add embedding module details to model debug string for logging
    model_debug_str += f"-{embedding_module.debug_str()}"

    # Set up interaction module for computing similarities between items
    # The interaction module computes similarity scores between query and item embeddings
    # References:
        # The interaction module is used to compute similarity scores between:
        # - Query embeddings: Represent the user's current context/interests
        # - Item embeddings: Represent candidate items to recommend
        # Common similarity functions include dot product and cosine similarity
        # The similarity scores are used to rank items for recommendations
    interaction_module, interaction_module_debug_str = get_similarity_function(
        # Type of similarity function to use (e.g. "DotProduct", "MoL")
        module_type=interaction_module_type,
        # Dimension of query embeddings (user/context representations)
        query_embedding_dim=item_embedding_dim,
        # Dimension of item embeddings (must match query dimension)
        item_embedding_dim=item_embedding_dim,
    )

    assert (
        user_embedding_norm == "l2_norm" or user_embedding_norm == "layer_norm"
    ), f"Not implemented for {user_embedding_norm}"
    # Configure embedding normalization
    # References:
        # Configure embedding normalization
        #
        # The output_postproc_module is used in train.py around line 631 to configure the embedding normalization
        # This is a hyperparameter that controls the normalization method used on the output embeddings
        # The output embeddings are used to represent the items in the dataset
        # The normalization method can impact the model's ability to learn and generalize
        # The optimal normalization method depends on the dataset and model complexity
        # In practice, a value between "l2_norm" and "layer_norm" is often used
    output_postproc_module = (
        L2NormEmbeddingPostprocessor(
            # References:
                # L2NormEmbeddingPostprocessor
                #
                # This is a module that normalizes the output embeddings using L2 norm
                # The L2 norm is a common normalization method used in generative recommender models
                # The L2 norm is computed as the square root of the sum of the squares of the embedding values
                # The L2 norm is used to ensure that the output embeddings have a consistent scale
                # The L2 norm is applied to the output embeddings to prevent overfitting and improve generalization
            embedding_dim=item_embedding_dim,
            eps=1e-6,
        )
        if user_embedding_norm == "l2_norm"
        else LayerNormEmbeddingPostprocessor(
            # References:
                # LayerNormEmbeddingPostprocessor
                #
                # This is a module that normalizes the output embeddings using layer normalization
                # Layer normalization is a common normalization method used in generative recommender models
                # Layer normalization transforms each embedding to have zero mean and unit variance
                # This ensures each embedding has a consistent scale where values typically fall within [-2,2]
                # By normalizing to this consistent scale, we avoid embeddings with very large or small magnitudes
                # 
                # Layer normalization is applied to the output embeddings to prevent overfitting and improve generalization
            embedding_dim=item_embedding_dim,
            eps=1e-6,
        )
    )

    # Set up positional embeddings
    # References:
        # Set up positional embeddings
        #
        # input_preproc_module contains a LearnablePositionalEmbeddingInputFeaturesPreprocessor that:
        # 1. Takes item embeddings as input and adds learnable positional embeddings
        # 2. Has parameters:
        #    - max_sequence_len: Maximum length of input sequences + output length + 1
        #    - embedding_dim: Dimension of item embeddings
        #    - dropout_rate: Probability of dropout for regularization
        # 3. Forward pass:
        #    - Scales item embeddings by sqrt(embedding_dim) to control variance
        #      This scaling helps maintain stable gradients during training by keeping
        #      the variance of the dot products between embeddings roughly constant
        #      as embedding dimension increases. Without scaling, larger embedding
        #      dimensions would lead to larger dot products and gradient instability.
        #    - Adds position embeddings from learned embedding table
        #    - Applies dropout to combined embeddings
        #    - Creates mask for valid (non-padding) positions
        #    - Returns sequence lengths, processed embeddings, and attention mask
        # 4. Initialization:
        #    - Creates embedding table of size [max_sequence_len x embedding_dim]
        #    - Initializes embeddings using truncated normal distribution
        #    - Sets up dropout layer with specified rate
        #
        # This module is crucial for the transformer architecture to understand sequence order,
        # since self-attention alone has no inherent way to capture position information.
    input_preproc_module = LearnablePositionalEmbeddingInputFeaturesPreprocessor(
        # References:
            # LearnablePositionalEmbeddingInputFeaturesPreprocessor
            #
            # This is a module that takes item embeddings as input and adds learnable positional embeddings
            # The positional embeddings are used to capture the order of items in the sequence
            # The positional embeddings are learned during training and added to the item embeddings
            # The positional embeddings are used to help the model understand the sequence structure
        max_sequence_len=dataset.max_sequence_length + gr_output_length + 1,
        embedding_dim=item_embedding_dim,
        dropout_rate=dropout_rate,
    )

    # Initialize main sequential encoder model
    # References:
        # The sequential encoder is the core model that:
        # 1. Takes preprocessed item sequences as input
        # 2. Processes them through transformer layers
        # 3. Generates predictions for next items
        #
        # Key components:
        # - module_type: Type of transformer architecture (e.g. HSTU, FALCON)
        # - max_sequence_length: Maximum length of input sequences
        # - max_output_length: Maximum length of output predictions (gr_output_length + 1)
        # - embedding_module: Handles item ID to embedding conversion
        # - interaction_module: Core transformer layers for sequence processing
        # - input_preproc_module: Adds positional embeddings to inputs
        # - output_postproc_module: Normalizes output embeddings
        #
        # The model processes sequences by:
        # 1. Converting item IDs to embeddings via embedding_module
        # 2. Adding positional information via input_preproc_module
        # 3. Passing through transformer layers in interaction_module
        # 4. Normalizing outputs via output_postproc_module
        # 5. Computing loss against target items
    model = get_sequential_encoder(
        module_type=main_module,
        max_sequence_length=dataset.max_sequence_length,
        max_output_length=gr_output_length + 1,
        embedding_module=embedding_module,
        interaction_module=interaction_module,
        input_preproc_module=input_preproc_module,
        output_postproc_module=output_postproc_module,
        verbose=True,
    )
    # Get debug string representation of model architecture
    model_debug_str = model.debug_str()

    # loss
    # === Loss and Sampling Setup ===
    # Configure loss function (BCE or Sampled Softmax)
    loss_debug_str = loss_module
    if loss_module == "BCELoss":
        # Binary Cross Entropy Loss for Generative Recommenders
        # 
        # BCELoss treats recommendation as a binary classification problem:
        # - For each item in the sequence, predict whether it will be the next item (1) or not (0)
        # - Loss is computed between model predictions and ground truth labels
        # - Temperature parameter scales logits before sigmoid (fixed at 1.0 for BCE)
        #
        # Advantages:
        # - Simple and stable training objective
        # - Works well with in-batch negative sampling
        # - Memory efficient as it doesn't require storing full item embedding matrix
        #
        # Disadvantages: 
        # - May be less effective than softmax for ranking
        # - Limited to binary decisions rather than full probability distribution
        # - Requires careful negative sampling strategy
        #
        # Implementation:
        # - Uses PyTorch's binary_cross_entropy_with_logits under the hood
        #   References:
        #   - https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html
        # - Handles both positive (target items) and negative samples
        # - Automatically applies sigmoid activation to model outputs
        loss_debug_str = loss_debug_str[:-4]
        assert temperature == 1.0
        ar_loss = BCELoss(temperature=temperature, model=model)
        # References:
            # BCELoss
            #
            # This is a loss function that computes the binary cross entropy loss between model predictions and ground truth labels
            # The loss is computed between model predictions and ground truth labels
            # The temperature parameter scales logits before sigmoid (fixed at 1.0 for BCE)
            #
    elif loss_module == "SampledSoftmaxLoss":
        # Sampled Softmax Loss for Generative Recommenders
        # 
        # Sampled Softmax Loss is a variant of the softmax loss function:
        # - Instead of using the full item embedding matrix, it samples negative items
        # - This reduces memory usage and computational complexity
        # - The loss is computed between model predictions and ground truth labels
        # - Temperature parameter scales logits before softmax (fixed at 1.0 for SSL)
        #
        # While BCELoss treats recommendation as binary classification (item relevant/not),
        # Sampled Softmax Loss models a full probability distribution over items:
        # - Treats each position as multi-class classification over all possible items
        # - Better captures relative preferences between items vs just binary relevance
        # - More suitable when we want to rank items by probability/preference
        #
        # Key differences from BCE:
        # - BCE: p(item is relevant) for each item independently 
        # - SSL: p(item|context) as distribution over all items
        #
        # Context in Sampled Softmax Loss:
        #
        # The context refers to the sequence of previous items and any other relevant 
        # features that help predict the next item. Specifically:
        #
        # - Previous items in the sequence provide temporal context
        # - User features/embeddings provide user preference context
        # - Item features/embeddings provide item characteristic context
        #
        # The model uses this context to compute p(item|context), which represents
        # the probability of each item being the next one given the context.
        #
        # Key differences from BCE:
        # - BCE: p(item is relevant) for each item independently 
        # - SSL: p(item|context) as distribution over all items
        # - SSL provides more fine-grained signal for ranking items
        # - SSL better models competition between items for user attention
        #
        # We offer both loss functions since they have different tradeoffs:
        # - BCE: Faster, simpler, good for binary relevance
        # - SSL: Better for ranking, models full distribution, but more complex
        #
        # Advantages:
        # - More effective for ranking tasks
        # - Can handle larger datasets with fewer negative samples
        # - Memory efficient as it doesn't require storing full item embedding matrix
        #
        # Disadvantages:
        # - Requires careful negative sampling strategy
        # - May be slower than BCELoss for small datasets
        #
        # Implementation:
        # - Uses PyTorch's cross_entropy under the hood
        # - Handles both positive (target items) and negative samples
        # - Automatically applies softmax activation to model outputs
        #
        loss_debug_str = "ssl"
        if temperature != 1.0:
            loss_debug_str += f"-t{temperature}"
        ar_loss = SampledSoftmaxLoss(
            # AR = Autoregressive, since SSL is used for next-item prediction
            # in an autoregressive fashion (predicting next item based on sequence)
            num_to_sample=num_negatives,
            softmax_temperature=temperature,
            model=model,
            activation_checkpoint=loss_activation_checkpoint,
        )
        loss_debug_str += (
            f"-n{num_negatives}{'-ac' if loss_activation_checkpoint else ''}"
        )
    else:
        raise ValueError(f"Unrecognized loss module {loss_module}.")

    # sampling
    # Set up negative sampling strategy
    if sampling_strategy == "in-batch":
        negatives_sampler = InBatchNegativesSampler(
            l2_norm=item_l2_norm,
            l2_norm_eps=l2_norm_eps,
            dedup_embeddings=True,
        )
        sampling_debug_str = (
            f"in-batch{f'-l2-eps{l2_norm_eps}' if item_l2_norm else ''}-dedup"
        )
    elif sampling_strategy == "local":
        negatives_sampler = LocalNegativesSampler(
            num_items=dataset.max_item_id,
            item_emb=model._embedding_module._item_emb,
            all_item_ids=dataset.all_item_ids,
            l2_norm=item_l2_norm,
            l2_norm_eps=l2_norm_eps,
        )
    else:
        raise ValueError(f"Unrecognized sampling strategy {sampling_strategy}.")
    sampling_debug_str = negatives_sampler.debug_str()

    # === Training Setup ===
    # Move model to GPU and wrap with DistributedDataParallel (DDP)
    # 
    # DDP wrapping is needed to:
    # 1. Enable distributed training across multiple GPUs
    # 2. Automatically synchronize gradients between processes
    # 3. Handle gradient reduction and parameter broadcast
    # 4. Optimize communication between GPUs for better performance
    #
    # Without DDP, each GPU would train independently without sharing updates,
    # leading to divergent model parameters across processes.
    device = rank
    if main_module_bf16:
        model = model.to(torch.bfloat16)
    model = model.to(device)
    ar_loss = ar_loss.to(device)
    negatives_sampler = negatives_sampler.to(device)
    model = DDP(model, device_ids=[rank], broadcast_buffers=False)

    # TODO: wrap in create_optimizer.
    # Initialize optimizer
    opt = torch.optim.AdamW(
        # References:
            # Optimizer
            #
            # The optimizer is used in train.py around line 500 to initialize the optimizer for the model
            # This is a hyperparameter that controls the optimization algorithm used to update the model's parameters during training
            # AdamW is a variant of the Adam optimizer that uses weight decay regularization
            # AdamW is a popular choice for training deep learning models, including generative recommender models
            #
            # Parameters:
            # - model.parameters(): This returns an iterator over all the parameters of the model
            # - lr: The learning rate for the optimizer
            # - betas: The beta1 and beta2 parameters for the AdamW optimizer
            # - weight_decay: The weight decay coefficient for the optimizer
        model.parameters(),
        lr=learning_rate,
        betas=(0.9, 0.98),
        weight_decay=weight_decay,
    )

    # === Logging Setup ===
    # Configure TensorBoard writer and logging paths
    date_str = date.today().strftime("%Y-%m-%d")
    model_subfolder = f"{dataset_name}-l{max_sequence_length}"
    model_desc = (
        f"{model_subfolder}"
        + f"/{model_debug_str}_{interaction_module_debug_str}_{sampling_debug_str}_{loss_debug_str}"
        + f"{f'-ddp{world_size}' if world_size > 1 else ''}-b{local_batch_size}-lr{learning_rate}-wu{num_warmup_steps}-wd{weight_decay}{'' if enable_tf32 else '-notf32'}-{date_str}"
    )
    if full_eval_every_n > 1:
        model_desc += f"-fe{full_eval_every_n}"
    if positional_sampling_ratio is not None and positional_sampling_ratio < 1:
        model_desc += f"-d{positional_sampling_ratio}"
    # creates subfolders.
    os.makedirs(f"./exps/{model_subfolder}", exist_ok=True)
    os.makedirs(f"./ckpts/{model_subfolder}", exist_ok=True)
    log_dir = f"./exps/{model_desc}"
    if rank == 0:
        writer = SummaryWriter(log_dir=log_dir)
        logging.info(f"Rank {rank}: writing logs to {log_dir}")
    else:
        writer = None
        logging.info(f"Rank {rank}: disabling summary writer")

    last_training_time = time.time()
    torch.autograd.set_detect_anomaly(True)

    batch_id = 0
    epoch = 0
    for epoch in range(num_epochs):
        # Set epoch for samplers
        if train_data_sampler is not None:
            train_data_sampler.set_epoch(epoch)
        if eval_data_sampler is not None:
            eval_data_sampler.set_epoch(epoch)
        model.train()
        for row in iter(train_data_loader):
            # === Process batch ===
            # seq_features contains the user's past interactions with items, including:
            # - past_ids: Tensor of item IDs the user has interacted with
            # - past_ratings: Tensor of ratings the user gave to those items
            # - past_timestamps: Tensor of timestamps when those interactions occurred
            # target_ids and target_ratings are the next items and ratings we want to predict
            seq_features, target_ids, target_ratings = movielens_seq_features_from_row(
                row,
                device=device,
                max_output_length=gr_output_length + 1,
            )

            # === Periodic Evaluation ===
            if (batch_id % eval_interval) == 0:
                model.eval()

                # Compute evaluation metrics
                eval_state = get_eval_state(
                    model=model.module,
                    all_item_ids=dataset.all_item_ids,
                    negatives_sampler=negatives_sampler,
                    top_k_module_fn=lambda item_embeddings, item_ids: get_top_k_module(
                        top_k_method=top_k_method,
                        model=model.module,
                        item_embeddings=item_embeddings,
                        item_ids=item_ids,
                    ),
                    device=device,
                    float_dtype=torch.bfloat16 if main_module_bf16 else None,
                )
                eval_dict = eval_metrics_v2_from_tensors(
                    eval_state,
                    model.module,
                    seq_features,
                    target_ids=target_ids,
                    target_ratings=target_ratings,
                    user_max_batch_size=eval_user_max_batch_size,
                    dtype=torch.bfloat16 if main_module_bf16 else None,
                )
                # Log results
                add_to_summary_writer(
                    writer, batch_id, eval_dict, prefix="eval", world_size=world_size
                )
                logging.info(
                    f"rank {rank}:  batch-stat (eval): iter {batch_id} (epoch {epoch}): "
                    + f"NDCG@10 {_avg(eval_dict['ndcg@10'], world_size):.4f}, "
                    f"HR@10 {_avg(eval_dict['hr@10'], world_size):.4f}, "
                    f"HR@50 {_avg(eval_dict['hr@50'], world_size):.4f}, "
                    + f"MRR {_avg(eval_dict['mrr'], world_size):.4f} "
                )
                model.train()

            # If we don't append the target item ID to the end of each sequence:
            # 1. The model won't have supervision signal for training, since it needs to know what item
            #    comes next in order to learn sequential patterns
            # 2. The loss function won't be able to compare the model's predictions against the actual
            #    next item that the user interacted with
            # 3. The model won't learn the temporal relationships between items in the sequence
            #
            # For example, if a user watched movies [1,2,3,4] in that order, we need to tell the model
            # that after [1,2,3], movie 4 came next. Without appending movie 4, the model would have
            # no way to learn this pattern.
            #
            B, N = seq_features.past_ids.shape  # B=batch size, N=sequence length
            seq_features.past_ids.scatter_(
                # Reference: https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html
                #
                # Append target item ID to end of each user's sequence by:
                # - dim=1: Operate along sequence length dimension
                # - index: Use past_lengths to identify end position for each sequence
                # - src: Target IDs to insert at those positions
                # This gives model supervision signal by showing what item came next
                dim=1,
                index=seq_features.past_lengths.view(-1, 1),  # Position to insert at
                src=target_ids.view(-1, 1),  # Target item IDs to insert
            )

            # === Training Step ===
            # Forward pass
            # Zero out gradients from previous backward pass
            opt.zero_grad()

            # Get embeddings for all items in the sequence by:
            # 1. Accessing the embedding module through model.module._embedding_module
            # 2. Calling get_item_embeddings() to lookup embeddings for each item ID
            # Shape: [batch_size, seq_len, embedding_dim]
            #
            # Reference: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html
            #
            input_embeddings = model.module._embedding_module.get_item_embeddings(seq_features.past_ids)

            # Pass sequence through model to get contextual embeddings
            # Input shape: [batch_size, seq_len]
            # Output shape: [batch_size, seq_len, embedding_dim]
            seq_embeddings = model(
                # The model here converts past_length, past_ids, past_embeddings and past_payloads into contextual embeddings to:
                #
                # 1. Capture sequential patterns and dependencies between items in the user's history
                #    - The model learns how previous items influence what comes next
                #    - Items earlier in sequence provide context for later items
                #
                # 2. Create a unified representation that combines:
                #    - past_length: How many items are in the sequence
                #    - past_ids: The actual item IDs in sequence order  
                #    - past_embeddings: The raw item embeddings
                #    - past_payloads: Additional metadata about each interaction
                #
                # 3. Generate embeddings that are "contextualized" by:
                #    - Position of item in sequence (temporal ordering)
                #    - Other items that came before/after
                #    - User's overall interaction patterns
                #
                # This contextual representation helps the model make better predictions
                # by considering the full sequential context rather than just individual items
                past_lengths=seq_features.past_lengths,
                past_ids=seq_features.past_ids,
                past_embeddings=input_embeddings,
                past_payloads=seq_features.past_payloads,
            )  # [B, X]

            supervision_ids = seq_features.past_ids

            if sampling_strategy == "in-batch":
                # get_item_embeddings currently assume 1-d tensor.
                in_batch_ids = supervision_ids.view(-1)
                negatives_sampler.process_batch(
                    ids=in_batch_ids,
                    presences=(in_batch_ids != 0),
                    embeddings=model.module.get_item_embeddings(in_batch_ids),
                )
            else:
                # pyre-fixme[16]: `InBatchNegativesSampler` has no attribute
                #  `_item_emb`.
                negatives_sampler._item_emb = model.module._embedding_module._item_emb

            # Create a mask for valid items in the sequence (non-zero IDs)
            # This mask excludes padding tokens (id=0) from loss computation
            # Ensures training only occurs on actual sequence items
            # Shape of supervision_ids: [batch_size, seq_len]
            #
            # We start from index 1: because in autoregressive training, we predict the next item
            # given the previous items. For each position i, we use items [0:i] to predict item [i].
            # So supervision_ids[:,1:] contains the target items to predict, while
            # seq_embeddings[:,:-1,:] contains the corresponding input contexts.
            #
            ar_mask = supervision_ids[:, 1:] != 0
            loss, aux_losses = ar_loss(
                # In autoregressive training, we predict the next item given previous items
                # For each position i, we use items [0:i] to predict item [i]
                # 
                # [0:i] in PyTorch is array slicing that takes elements from index 0 up to (but not including) index i
                # For example, if tensor=[1,2,3,4] then tensor[0:2] gives [1,2]
                #
                # :-1 slicing on output_embeddings removes the last position since we can't predict beyond sequence
                # 1: slicing on supervision removes first position since we need at least 1 previous item to predict
                #
                # Example for sequence [A, B, C]:
                # output_embeddings[:, :-1] gives [A, B] - use these to predict
                # supervision_ids[:, 1:] gives [B, C] - these are the targets to predict
                lengths=seq_features.past_lengths,  # [B],
                output_embeddings=seq_embeddings[:, :-1, :],  # [B, N-1, D]
                supervision_ids=supervision_ids[:, 1:],  # [B, N-1]
                supervision_embeddings=input_embeddings[:, 1:, :],  # [B, N - 1, D]
                supervision_weights=ar_mask.float(),
                negatives_sampler=negatives_sampler,
                **seq_features.past_payloads,
            )  # [B, N]

            # After this operation, we have 2 loss objects:
            # 1. loss - The original loss tensor with gradients attached
            # 2. main_loss - A detached clone of loss without gradients
            # Reference: https://pytorch.org/docs/stable/generated/torch.Tensor.detach.html
            #
            # We detach() and clone() the loss here to track the original AR loss value
            # before adding auxiliary losses. This allows us to:
            # 1. detach() returns a new Tensor, detached from the current graph, and breaks gradient flow
            # so we can track raw loss without affecting backprop
            # 2. clone() creates a separate copy to avoid modifying the original loss tensor
            # This separated value is used for logging/monitoring the main AR loss independently
            #
            # loss.detach().clone() is functionally the same as loss.clone().detach().
            # Both operations result in a new tensor that:
            # - Shares the same values as the original tensor (loss).
            # - Is detached from the computational graph (no gradient tracking).
            # - Has a separate copy of the data (due to clone()).
            #
            main_loss = loss.detach().clone()

            # Add auxiliary losses to the main loss
            # This allows us to:
            # 1. Combine multiple loss components (e.g. AR loss + auxiliary losses)
            # 2. Track each loss separately for logging/monitoring
            # 3. Gradients are only computed for the main loss, not auxiliary losses
            # 4. We can adjust weights of each loss component if needed
            # 5. This helps in understanding the contribution of each loss term to the total loss
            # 6. Allows us to optimize the main loss while keeping auxiliary losses in check
            # 7. Helps in debugging and understanding the model's behavior
            #
            # Weights for each loss component can be specified in loss_weights dictionary
            # If no weights are provided, default to equal weighting
            loss = get_weighted_loss(loss, aux_losses, weights=loss_weights or {})

            if rank == 0:
                assert writer is not None
                writer.add_scalar("losses/ar_loss", loss, batch_id)
                writer.add_scalar("losses/main_loss", main_loss, batch_id)

            # Backward pass and optimization
            loss.backward()

            # Optional linear warmup.
            if batch_id < num_warmup_steps:
                # Linearly increase learning rate from 0 to learning_rate over num_warmup_steps
                lr_scalar = min(1.0, float(batch_id + 1) / num_warmup_steps)
                for pg in opt.param_groups:
                    pg["lr"] = lr_scalar * learning_rate
                lr = lr_scalar * learning_rate
            else:
                lr = learning_rate

            if (batch_id % eval_interval) == 0:
                logging.info(
                    f" rank: {rank}, batch-stat (train): step {batch_id} "
                    f"(epoch {epoch} in {time.time() - last_training_time:.2f}s): {loss:.6f}"
                )
                last_training_time = time.time()
                if rank == 0:
                    assert writer is not None
                    writer.add_scalar("loss/train", loss, batch_id)
                    writer.add_scalar("lr", lr, batch_id)

            opt.step()

            batch_id += 1

        def is_full_eval(epoch: int) -> bool:
            return (epoch % full_eval_every_n) == 0

        # eval per epoch
        # === Epoch-level Evaluation ===
        model.eval()
        eval_dict_all = None
        eval_start_time = time.time()
        model.eval()
        eval_state = get_eval_state(
            model=model.module,
            all_item_ids=dataset.all_item_ids,
            negatives_sampler=negatives_sampler,
            top_k_module_fn=lambda item_embeddings, item_ids: get_top_k_module(
                top_k_method=top_k_method,
                model=model.module,
                item_embeddings=item_embeddings,
                item_ids=item_ids,
            ),
            device=device,
            float_dtype=torch.bfloat16 if main_module_bf16 else None,
        )
        for eval_iter, row in enumerate(iter(eval_data_loader)):
            seq_features, target_ids, target_ratings = movielens_seq_features_from_row(
                row, device=device, max_output_length=gr_output_length + 1
            )
            eval_dict = eval_metrics_v2_from_tensors(
                eval_state,
                model.module,
                seq_features,
                target_ids=target_ids,
                target_ratings=target_ratings,
                user_max_batch_size=eval_user_max_batch_size,
                dtype=torch.bfloat16 if main_module_bf16 else None,
            )

            if eval_dict_all is None:
                eval_dict_all = {}
                for k, v in eval_dict.items():
                    eval_dict_all[k] = []

            for k, v in eval_dict.items():
                eval_dict_all[k] = eval_dict_all[k] + [v]
            del eval_dict

            if (eval_iter + 1 >= partial_eval_num_iters) and (not is_full_eval(epoch)):
                logging.info(
                    f"Truncating epoch {epoch} eval to {eval_iter + 1} iters to save cost.."
                )
                break

        assert eval_dict_all is not None
        for k, v in eval_dict_all.items():
            eval_dict_all[k] = torch.cat(v, dim=-1)

        ndcg_10 = _avg(eval_dict_all["ndcg@10"], world_size=world_size)
        ndcg_50 = _avg(eval_dict_all["ndcg@50"], world_size=world_size)
        hr_10 = _avg(eval_dict_all["hr@10"], world_size=world_size)
        hr_50 = _avg(eval_dict_all["hr@50"], world_size=world_size)
        mrr = _avg(eval_dict_all["mrr"], world_size=world_size)

        add_to_summary_writer(
            writer,
            batch_id=epoch,
            metrics=eval_dict_all,
            prefix="eval_epoch",
            world_size=world_size,
        )
        if full_eval_every_n > 1 and is_full_eval(epoch):
            add_to_summary_writer(
                writer,
                batch_id=epoch,
                metrics=eval_dict_all,
                prefix="eval_epoch_full",
                world_size=world_size,
            )
        if rank == 0 and epoch > 0 and (epoch % save_ckpt_every_n) == 0:
            torch.save(
                {
                    "epoch": epoch,
                    "model_state_dict": model.state_dict(),
                    "optimizer_state_dict": opt.state_dict(),
                },
                f"./ckpts/{model_desc}_ep{epoch}",
            )

        logging.info(
            f"rank {rank}: eval @ epoch {epoch} in {time.time() - eval_start_time:.2f}s: "
            f"NDCG@10 {ndcg_10:.4f}, NDCG@50 {ndcg_50:.4f}, HR@10 {hr_10:.4f}, HR@50 {hr_50:.4f}, MRR {mrr:.4f}"
        )
        last_training_time = time.time()

    if rank == 0:
        # Save final checkpoint and clean up
        # At rank 0 (main process), we:
        # 1. Flush and close the summary writer to ensure metrics are saved
        # 2. Save the final model checkpoint with:
        #    - Current epoch number
        #    - Model state dict (weights/parameters)
        #    - Optimizer state dict (momentum buffers etc)
        # This allows resuming training from this point if needed
        if writer is not None:
            writer.flush()
            writer.close()

        torch.save(
            {
                "epoch": epoch,
                "model_state_dict": model.state_dict(),
                "optimizer_state_dict": opt.state_dict(),
            },
            f"./ckpts/{model_desc}_ep{epoch}",
        )

    cleanup()

</content>

<content full_path="generative_recommenders/rails/similarities/dot_product_similarity_fn.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from typing import Dict, Tuple

import torch

from generative_recommenders.rails.similarities.module import SimilarityModule


class DotProductSimilarity(SimilarityModule):
    def __init__(
        self,
    ) -> None:
        super().__init__()

    def debug_str(self) -> str:
        return "dp"

    def forward(
        self,
        query_embeddings: torch.Tensor,
        item_embeddings: torch.Tensor,
        **kwargs,
    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        Args:
            query_embeddings: (B, D,) or (B * r, D) x float.
            item_embeddings: (1, X, D) or (B, X, D) x float.

        Returns:
            (B, X) x float.
        """

        B_I, X, D = item_embeddings.size()
        if B_I == 1:
            # [B, D] x ([1, X, D] -> [D, X]) => [B, X]
            return (
                torch.mm(query_embeddings, item_embeddings.squeeze(0).t()),
                {},
            )  # [B, X]
        elif query_embeddings.size(0) != B_I:
            # (B * r, D) x (B, X, D).
            return (
                torch.bmm(
                    query_embeddings.view(B_I, -1, D),
                    item_embeddings.permute(0, 2, 1),
                ).view(-1, X),
                {},
            )
        else:
            # [B, X, D] x ([B, D] -> [B, D, 1]) => [B, X, 1] -> [B, X]
            return (
                torch.bmm(item_embeddings, query_embeddings.unsqueeze(2)).squeeze(2),
                {},
            )

</content>

<content full_path="generative_recommenders/rails/similarities/module.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import abc
from typing import Dict, Tuple

import torch


class SimilarityModule(torch.nn.Module):
    """
    Interface enabling interfacing with various similarity functions.

    While the discussions in our initial ICML'24 paper are based on inner products
    for simplicity, we provide this interface (SimilarityModule) to support various
    learned similarities at the retrieval stage, such as MLPs, Factorization Machines
    (FMs), and Mixture-of-Logits (MoL), which we discussed in
    - Revisiting Neural Retrieval on Accelerators (KDD'23), and
    - Retrieval with Learned Similarities (https://arxiv.org/abs/2407.15462).
    """

    @abc.abstractmethod
    def forward(
        self,
        query_embeddings: torch.Tensor,
        item_embeddings: torch.Tensor,
        **kwargs,
    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        Args:
            query_embeddings: (B, input_embedding_dim) x float.
            item_embeddings: (1/B, X, item_embedding_dim) x float.
            **kwargs: Implementation-specific keys/values (e.g.,
                item ids / sideinfo, etc.)

        Returns:
            A tuple of (
                (B, X,) similarity values,
                keyed outputs representing auxiliary losses at training time.
            ).
        """
        pass

</content>

<content full_path="generative_recommenders/rails/similarities/layers.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Defines network architectures used in constructing various learned similarities.

Forked from bailuding/rails @ 664fdb9.
"""

import torch
import torch.nn.functional as F


class GeGLU(torch.nn.Module):
    def __init__(
        self,
        in_features: int,
        out_features: int,
    ) -> None:
        super().__init__()

        self._in_features = in_features
        self._out_features = out_features
        self._w = torch.nn.Parameter(
            torch.empty((in_features, out_features * 2)).normal_(mean=0, std=0.02),
        )
        self._b = torch.nn.Parameter(
            torch.zeros((1, out_features * 2)),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        bs = x.size()[:-1]
        lhs, rhs = torch.split(
            torch.mm(x.reshape(-1, self._in_features), self._w) + self._b,
            [self._out_features, self._out_features],
            dim=-1,
        )
        return (F.gelu(lhs) * rhs).reshape(bs + (self._out_features,))


class SwiGLU(torch.nn.Module):
    """
    SwiGLU from https://arxiv.org/abs/2002.05202.
    """

    def __init__(
        self,
        in_features: int,
        out_features: int,
    ) -> None:
        super().__init__()

        self._in_features = in_features
        self._out_features = out_features
        self._w = torch.nn.Parameter(
            torch.empty((in_features, out_features * 2)).normal_(mean=0, std=0.02),
        )
        self._b = torch.nn.Parameter(
            torch.zeros((1, out_features * 2)),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        bs = x.size()[:-1]
        lhs, rhs = torch.split(
            torch.mm(x.reshape(-1, self._in_features), self._w) + self._b,
            [self._out_features, self._out_features],
            dim=-1,
        )
        return (F.silu(lhs) * rhs).reshape(bs + (self._out_features,))

</content>

<content full_path="generative_recommenders/rails/similarities/mol/item_embeddings_fn.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Defines functions to generate item-side embeddings for MoL.

Forked from bailuding/rails @ 664fdb9.
"""

from typing import Callable, Dict, Tuple

import torch

from generative_recommenders.rails.similarities.mol.embeddings_fn import MoLEmbeddingsFn


def init_mlp_xavier_weights_zero_bias(m) -> None:
    if isinstance(m, torch.nn.Linear):
        torch.nn.init.xavier_uniform_(m.weight)
        if getattr(m, "bias", None) is not None:
            m.bias.data.fill_(0.0)


class RecoMoLItemEmbeddingsFn(MoLEmbeddingsFn):
    """
    Generates P_X query-side embeddings for MoL based on input embeddings and other
    optional tensors for recommendation models. Tested for sequential retrieval
    scenarios.
    """

    def __init__(
        self,
        item_embedding_dim: int,
        item_dot_product_groups: int,
        dot_product_dimension: int,
        dot_product_l2_norm: bool,
        proj_fn: Callable[[int, int], torch.nn.Module],
        eps: float,
    ) -> None:
        super().__init__()

        self._item_emb_based_dot_product_groups: int = item_dot_product_groups
        self._item_emb_proj_module: torch.nn.Module = proj_fn(
            item_embedding_dim,
            dot_product_dimension * self._item_emb_based_dot_product_groups,
        )
        self._dot_product_dimension: int = dot_product_dimension
        self._dot_product_l2_norm: bool = dot_product_l2_norm
        self._eps: float = eps

    def forward(
        self,
        input_embeddings: torch.Tensor,
        **kwargs,
    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        Args:
            input_embeddings: (B, item_embedding_dim,) x float where B is the batch size.
            kwargs: str-keyed tensors. Implementation-specific.

        Returns:
            Tuple of (
                (B, item_dot_product_groups, dot_product_embedding_dim) x float,
                str-keyed aux_losses,
            ).
        """
        split_item_embeddings = self._item_emb_proj_module(input_embeddings).reshape(
            input_embeddings.size()[:-1]
            + (
                self._item_emb_based_dot_product_groups,
                self._dot_product_dimension,
            )
        )

        if self._dot_product_l2_norm:
            split_item_embeddings = split_item_embeddings / torch.clamp(
                torch.linalg.norm(
                    split_item_embeddings,
                    ord=None,
                    dim=-1,
                    keepdim=True,
                ),
                min=self._eps,
            )
        return split_item_embeddings, {}

</content>

<content full_path="generative_recommenders/rails/similarities/mol/embeddings_fn.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Defines interface for generating query- and item-side embeddings for MoL.

Forked from bailuding/rails @ 664fdb9.
"""

import abc
from typing import Dict, Tuple

import torch


class MoLEmbeddingsFn(torch.nn.Module):
    """
    Generates K_Q query-side (K_I item-side) embeddings for MoL based on
    input embeddings and other optional implementation-specific tensors.
    """

    @abc.abstractmethod
    def forward(
        self,
        input_embeddings: torch.Tensor,
        **kwargs,
    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        Args:
            input_embeddings: (B, ...) x float where B is the batch size.
            kwargs: implementation-specific.

        Returns:
            Tuple of (
                (B, query_dot_product_groups/item_dot_product_groups, dot_product_embedding_dim) x float,
                str-keyed auxiliary losses.
            ).
        """
        pass

</content>

<content full_path="generative_recommenders/rails/similarities/mol/query_embeddings_fn.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Defines functions to generate query-side embeddings for MoL.

Forked from bailuding/rails @ 664fdb9.
"""

from typing import Callable, Dict, List, Optional, Tuple

import torch
import torch.nn.functional as F

from generative_recommenders.rails.similarities.mol.embeddings_fn import MoLEmbeddingsFn


def init_mlp_xavier_weights_zero_bias(m) -> None:
    if isinstance(m, torch.nn.Linear):
        torch.nn.init.xavier_uniform_(m.weight)
        if getattr(m, "bias", None) is not None:
            m.bias.data.fill_(0.0)


class RecoMoLQueryEmbeddingsFn(MoLEmbeddingsFn):
    """
    Generates P_Q query-side embeddings for MoL based on input embeddings and other
    optional tensors for recommendation models. Tested for sequential retrieval
    scenarios.

    The current implementation accesses user_ids associated with the query from
    `user_ids' in kwargs.
    """

    def __init__(
        self,
        query_embedding_dim: int,
        query_dot_product_groups: int,
        dot_product_dimension: int,
        dot_product_l2_norm: bool,
        proj_fn: Callable[[int, int], torch.nn.Module],
        eps: float,
        uid_embedding_hash_sizes: Optional[List[int]] = None,
        uid_dropout_rate: float = 0.0,
        uid_embedding_level_dropout: bool = False,
    ) -> None:
        super().__init__()
        self._uid_embedding_hash_sizes: List[int] = uid_embedding_hash_sizes or []
        self._query_emb_based_dot_product_groups: int = query_dot_product_groups - len(
            self._uid_embedding_hash_sizes
        )
        self._query_emb_proj_module: torch.nn.Module = proj_fn(
            query_embedding_dim,
            dot_product_dimension * self._query_emb_based_dot_product_groups,
        )
        self._dot_product_dimension: int = dot_product_dimension
        self._dot_product_l2_norm: bool = dot_product_l2_norm
        if len(self._uid_embedding_hash_sizes) > 0:
            for i, hash_size in enumerate(self._uid_embedding_hash_sizes):
                setattr(
                    self,
                    f"_uid_embeddings_{i}",
                    torch.nn.Embedding(
                        hash_size + 1, dot_product_dimension, padding_idx=0
                    ),
                )
        self._uid_dropout_rate: float = uid_dropout_rate
        self._uid_embedding_level_dropout: bool = uid_embedding_level_dropout
        self._eps: float = eps

    def forward(
        self,
        input_embeddings: torch.Tensor,
        **kwargs,
    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        Args:
            input_embeddings: (B, query_embedding_dim,) x float where B is the batch size.
            kwargs: str-keyed tensors. Implementation-specific.

        Returns:
            Tuple of (
                (B, query_dot_product_groups, dot_product_embedding_dim) x float,
                str-keyed aux_losses,
            ).
        """
        split_query_embeddings = self._query_emb_proj_module(input_embeddings).reshape(
            (
                input_embeddings.size(0),
                self._query_emb_based_dot_product_groups,
                self._dot_product_dimension,
            )
        )

        aux_losses: Dict[str, torch.Tensor] = {}

        if len(self._uid_embedding_hash_sizes) > 0:
            all_uid_embeddings = []
            for i, hash_size in enumerate(self._uid_embedding_hash_sizes):
                # TODO: decouple this from MoLQueryEmbeddingFn.
                uid_embeddings = getattr(self, f"_uid_embeddings_{i}")(
                    (kwargs["user_ids"] % hash_size) + 1
                )
                if self.training:
                    l2_norm = (uid_embeddings * uid_embeddings).sum(-1).mean()
                    if i == 0:
                        aux_losses["uid_embedding_l2_norm"] = l2_norm
                    else:
                        aux_losses["uid_embedding_l2_norm"] = (
                            aux_losses["uid_embedding_l2_norm"] + l2_norm
                        )

                if self._uid_dropout_rate > 0.0:
                    if self._uid_embedding_level_dropout:
                        # conditionally dropout the entire embedding.
                        if self.training:
                            uid_dropout_mask = (
                                torch.rand(
                                    uid_embeddings.size()[:-1],
                                    device=uid_embeddings.device,
                                )
                                > self._uid_dropout_rate
                            )
                            uid_embeddings = (
                                uid_embeddings
                                * uid_dropout_mask.unsqueeze(-1)
                                / (1.0 - self._uid_dropout_rate)
                            )
                    else:
                        uid_embeddings = F.dropout(
                            uid_embeddings,
                            p=self._uid_dropout_rate,
                            training=self.training,
                        )
                all_uid_embeddings.append(uid_embeddings.unsqueeze(1))
            split_query_embeddings = torch.cat(
                [split_query_embeddings] + all_uid_embeddings, dim=1
            )

        if self._dot_product_l2_norm:
            split_query_embeddings = split_query_embeddings / torch.clamp(
                torch.linalg.norm(
                    split_query_embeddings,
                    ord=None,
                    dim=-1,
                    keepdim=True,
                ),
                min=self._eps,
            )
        return split_query_embeddings, aux_losses

</content>

<content full_path="generative_recommenders/rails/similarities/mol/similarity_fn.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Implements MoL (Mixture-of-Logits) with load balancing regularization loss, as discussed in:
- Revisiting Neural Retrieval on Accelerators (https://arxiv.org/abs/2306.04039, KDD'23).
- Retrieval with Learned Similarities (https://arxiv.org/abs/2407.15462).

Forked from bailuding/rails @ 664fdb9.
"""

from typing import Callable, Dict, Optional, Tuple

import torch
import torch.nn.functional as F

from generative_recommenders.rails.similarities.module import SimilarityModule
from generative_recommenders.rails.similarities.mol.embeddings_fn import MoLEmbeddingsFn


@torch.compile(dynamic=True)
def _softmax_dropout_combiner_fn(
    x: torch.Tensor,
    y: torch.Tensor,
    dropout_pr: float,
    eps: float,
    training: bool,
) -> torch.Tensor:
    """
    Computes (_softmax_dropout_fn(x) * y).sum(-1).
    """
    x = F.softmax(x, dim=-1)
    if dropout_pr > 0.0:
        x = F.dropout(x, p=dropout_pr, training=training)
        x = x / torch.clamp(x.sum(-1, keepdims=True), min=eps)
    return x, (x * y).sum(-1)


@torch.compile
def _load_balancing_mi_loss_fn(
    gating_prs: torch.Tensor,
    eps: float,
) -> torch.Tensor:
    """
    See Retrieval with Learned Similarities (RAILS, https://arxiv.org/abs/2407.15462) for discussions.
    """
    B, X, E = gating_prs.size()
    expert_util_prs = gating_prs.view(B * X, E).sum(0, keepdim=False) / (1.0 * B * X)
    expert_util_entropy = -(expert_util_prs * torch.log(expert_util_prs + eps)).sum()
    per_example_expert_entropy = -(gating_prs * torch.log(gating_prs + eps)).sum() / (
        1.0 * B * X
    )
    return -expert_util_entropy + per_example_expert_entropy


class SoftmaxDropoutCombiner(torch.nn.Module):
    def __init__(
        self,
        dropout_rate: float,
        eps: float,
    ) -> None:
        super().__init__()

        self._dropout_rate: float = dropout_rate
        self._eps: float = eps

    def forward(
        self,
        gating_weights: torch.Tensor,
        x: torch.Tensor,
    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        gating_prs, combined_logits = _softmax_dropout_combiner_fn(
            x=gating_weights,
            y=x,
            dropout_pr=self._dropout_rate,
            eps=self._eps,
            training=self.training,
        )

        aux_losses = {}
        if self.training:
            aux_losses["mi_loss"] = _load_balancing_mi_loss_fn(
                gating_prs, eps=self._eps
            )

        return combined_logits, aux_losses


class MoLGatingFn(torch.nn.Module):
    """
    Implements the gating function for MoL, used to compute pi_p(q, x) for a given (p, x) pair.
    """

    def __init__(
        self,
        num_logits: int,
        query_embedding_dim: int,
        item_embedding_dim: int,
        query_only_partial_fn: Optional[Callable[[int, int], torch.nn.Module]],
        item_only_partial_fn: Optional[Callable[[int, int], torch.nn.Module]],
        qi_partial_fn: Optional[Callable[[int, int], torch.nn.Module]],
        combination_type: str,
        normalization_fn: Callable[[int], torch.nn.Module],
    ) -> None:
        super().__init__()

        self._query_only_partial_module: Optional[torch.nn.Module] = (
            query_only_partial_fn(query_embedding_dim, num_logits)
            if query_only_partial_fn
            else None
        )
        self._item_only_partial_module: Optional[torch.nn.Module] = (
            item_only_partial_fn(item_embedding_dim, num_logits)
            if item_only_partial_fn
            else None
        )
        self._qi_partial_module: Optional[torch.nn.Module] = (
            qi_partial_fn(
                num_logits,
                num_logits,
            )
            if qi_partial_fn is not None
            else None
        )
        if (
            self._query_only_partial_module is None
            and self._item_only_partial_module is None
            and self._qi_partial_module is None
        ):
            raise ValueError(
                "At least one of query_only_partial_fn, item_only_partial_fn, "
                "and qi_partial_fn must not be None."
            )
        self._num_logits: int = num_logits
        self._combination_type: str = combination_type
        self._normalization_fn: torch.nn.Module = normalization_fn(num_logits)

    def forward(
        self,
        logits: torch.Tensor,
        query_embeddings: torch.Tensor,
        item_embeddings: torch.Tensor,
    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        Args:
            logits: (B, X, P_Q * P_X) x float;
            query_embeddings: (B, D) x float;
            item_embeddings: (1/B, X, D') x float;

        Returns:
            (B, X) x float, Dict[str, Tensor] representing auxiliary losses.
        """
        B, X, _ = logits.size()
        # [B, 1, F], [1/B, X, F], [B, X, F]
        query_partial_inputs, item_partial_inputs, qi_partial_inputs = None, None, None
        if self._query_only_partial_module is not None:
            query_partial_inputs = self._query_only_partial_module(
                query_embeddings
            ).unsqueeze(1)
        if self._item_only_partial_module is not None:
            item_partial_inputs = self._item_only_partial_module(item_embeddings)
        if self._qi_partial_module is not None:
            qi_partial_inputs = self._qi_partial_module(logits)

        if self._combination_type == "glu_silu":
            gating_inputs = (
                query_partial_inputs * item_partial_inputs + qi_partial_inputs
            )
            gating_weights = gating_inputs * F.sigmoid(gating_inputs)
        elif self._combination_type == "glu_silu_ln":
            gating_inputs = (
                query_partial_inputs * item_partial_inputs + qi_partial_inputs
            )
            gating_weights = gating_inputs * F.sigmoid(
                F.layer_norm(gating_inputs, normalized_shapes=[self._num_logits])
            )
        elif self._combination_type == "none":
            gating_inputs = query_partial_inputs
            if gating_inputs is None:
                gating_inputs = item_partial_inputs
            elif item_partial_inputs is not None:
                gating_inputs += item_partial_inputs
            if gating_inputs is None:
                gating_inputs = qi_partial_inputs
            elif qi_partial_inputs is not None:
                gating_inputs += qi_partial_inputs
            gating_weights = gating_inputs
        else:
            raise ValueError(f"Unknown combination_type {self._combination_type}")

        return self._normalization_fn(gating_weights, logits)


class MoLSimilarity(SimilarityModule):
    def __init__(
        self,
        query_embedding_dim: int,
        item_embedding_dim: int,
        dot_product_dimension: int,
        query_dot_product_groups: int,
        item_dot_product_groups: int,
        temperature: float,
        dot_product_l2_norm: bool,
        query_embeddings_fn: MoLEmbeddingsFn,
        item_embeddings_fn: Optional[MoLEmbeddingsFn],
        gating_query_only_partial_fn: Optional[Callable[[int, int], torch.nn.Module]],
        gating_item_only_partial_fn: Optional[Callable[[int, int], torch.nn.Module]],
        gating_qi_partial_fn: Optional[Callable[[int], torch.nn.Module]],
        gating_combination_type: str,
        gating_normalization_fn: Callable[[int], torch.nn.Module],
        eps: float,
        apply_query_embeddings_fn: bool = True,
        apply_item_embeddings_fn: bool = True,
        autocast_bf16: bool = False,
    ) -> None:
        """
        Args:
            apply_query_embeddings_fn: bool. If true, compute query_embeddings_fn
                to input during forward(). Otherwise, we assume the caller will
                invoke get_query_component_embeddings() separately before
                calling forward().
            apply_item_embeddings_fn: bool. If true, compute item_embeddings_fn
                to input during forward(). Otherwise, we assume the caller will
                invoke get_item_component_embeddings() separately before
                calling forward().
        """
        super().__init__()

        self._gating_fn: MoLGatingFn = MoLGatingFn(
            num_logits=query_dot_product_groups * item_dot_product_groups,
            query_embedding_dim=query_embedding_dim,
            item_embedding_dim=item_embedding_dim,
            query_only_partial_fn=gating_query_only_partial_fn,
            item_only_partial_fn=gating_item_only_partial_fn,
            qi_partial_fn=gating_qi_partial_fn,
            combination_type=gating_combination_type,
            normalization_fn=gating_normalization_fn,
        )
        self._query_embeddings_fn: MoLEmbeddingsFn = query_embeddings_fn
        self._item_embeddings_fn: MoLEmbeddingsFn = item_embeddings_fn
        self._apply_query_embeddings_fn: bool = apply_query_embeddings_fn
        self._apply_item_embeddings_fn: bool = apply_item_embeddings_fn
        self._dot_product_l2_norm: bool = dot_product_l2_norm
        self._query_dot_product_groups: int = query_dot_product_groups
        self._item_dot_product_groups: int = item_dot_product_groups
        self._dot_product_dimension: int = dot_product_dimension
        self._temperature: float = temperature
        self._eps: float = eps
        self._autocast_bf16: bool = autocast_bf16

    def get_query_component_embeddings(
        self,
        input_embeddings: torch.Tensor,
        decoupled_inference: bool = False,
        **kwargs,
    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        Args:
            input_embeddings: (B, self._input_embedding_dim,) x float
                or (B, P_Q, self._dot_product_dimension) x float.
            decoupled_inference: bool. If true, the call represents an attempt to run
                forward() in decoupled mode at inference time (e.g., to pre-compute
                component-level query embeddings for filtering, etc.). We simulate
                the logic in forward() in this case (e.g., if forward() doesn't apply
                query_embeddings_fn, then this call won't either).
            kwargs: additional implementation-specific arguments.

        Returns:
            (B, query_dot_product_groups, dot_product_embedding_dim) x float.
        """
        if decoupled_inference and not self._apply_query_embeddings_fn:
            return input_embeddings, {}
        return self._query_embeddings_fn(input_embeddings, **kwargs)

    def get_item_component_embeddings(
        self,
        input_embeddings: torch.Tensor,
        decoupled_inference: bool = False,
        **kwargs,
    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        Args:
            input_embeddings: (..., self._input_embedding_dim,) x float
                or (..., P_X, self._dot_product_dimension) x float.
            decoupled_inference: bool. If true, the call represents an attempt to run
                forward() in decoupled mode at inference time (e.g., to pre-compute
                component-level item embeddings for filtering, etc.). We simulate
                the logic in forward() in this case (e.g., if forward() doesn't apply
                item_embeddings_fn, then this call won't either).
            kwargs: additional implementation-specific arguments.

        Returns:
            (..., item_dot_product_groups, dot_product_embedding_dim) x float.
        """
        if decoupled_inference and not self._apply_item_embeddings_fn:
            return input_embeddings, {}

        return self._item_embeddings_fn(input_embeddings, **kwargs)

    def forward(
        self,
        query_embeddings: torch.Tensor,
        item_embeddings: torch.Tensor,
        **kwargs,
    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        Args:
            query_embeddings: (B, self._input_embedding_dim) x float or
                (B, P_Q, self._dot_product_dimension) x float (when query_embeddings_fn
                is applied externally).
            item_embeddings: (1/B, X, self._item_embedding_dim) x float or
                (1/B, X, P_X, self._dot_product_dimension) x float (when item_embeddings_fn
                is applied externally).
            kwargs: additional implementation-specific arguments.

        Returns:
            (B, X) x float, Dict[str, Tensor] representing auxiliary losses.
        """
        with torch.autocast(
            enabled=self._autocast_bf16, dtype=torch.bfloat16, device_type="cuda"
        ):
            B = query_embeddings.size(0)
            B_prime = item_embeddings.shape[0]  # 1 or B
            X = item_embeddings.shape[1]

            if self._apply_query_embeddings_fn:
                (
                    split_query_embeddings,
                    query_aux_losses,
                ) = self.get_query_component_embeddings(
                    query_embeddings,
                    **kwargs,
                )
            else:
                split_query_embeddings, query_aux_losses = query_embeddings, {}

            if self._apply_item_embeddings_fn:
                (
                    split_item_embeddings,
                    item_aux_losses,
                ) = self.get_item_component_embeddings(
                    input_embeddings=item_embeddings,
                    **kwargs,
                )
            else:
                split_item_embeddings, item_aux_losses = item_embeddings, {}

            if B_prime == 1:
                logits = torch.einsum(
                    "bnd,xmd->bxnm",
                    split_query_embeddings,
                    split_item_embeddings.squeeze(0),
                ).reshape(
                    B, X, self._query_dot_product_groups * self._item_dot_product_groups
                )
            else:
                logits = torch.einsum(
                    "bnd,bxmd->bxnm", split_query_embeddings, split_item_embeddings
                ).reshape(
                    B, X, self._query_dot_product_groups * self._item_dot_product_groups
                )

            gated_outputs, gating_aux_losses = self._gating_fn(
                logits=logits / self._temperature,  # [B, X, L]
                query_embeddings=query_embeddings,  # [B, D]
                item_embeddings=item_embeddings,  # [1/B, X, D']
            )
            return gated_outputs, {
                **gating_aux_losses,
                **query_aux_losses,
                **item_aux_losses,
            }

</content>

<content full_path="generative_recommenders/rails/indexing/mol_top_k.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# pyre-unsafe

"""
Defines exact- and approximate- Top-K modules for Mixture-of-Logits (MoL),
discussed in Retrieval with Learned Similarities (https://arxiv.org/abs/2407.15462).

Forked from bailuding/rails @ 664fdb9.
"""

from typing import Tuple

import torch

from generative_recommenders.rails.indexing.candidate_index import TopKModule
from generative_recommenders.rails.similarities.mol.similarity_fn import MoLSimilarity


class MoLTopKModule(TopKModule):
    def __init__(
        self,
        mol_module: MoLSimilarity,
        item_embeddings: torch.Tensor,
        item_ids: torch.Tensor,
        flatten_item_ids_and_embeddings: bool,
        keep_component_level_item_embeddings: bool,
        component_level_item_embeddings_dtype: torch.dtype = torch.bfloat16,
    ) -> None:
        """
        Args:
            mol_module: MoLSimilarity.
            item_embeddings: (1, X, D) if mol_module._apply_item_embeddings_fn is True,
                (1, X, P_X, D_P) otherwise.
            item_ids: (1, X,) representing the item ids.
            flatten_item_ids_and_embeddings: bool. If true, do not keep the extra (1,)
                dimension at size(0).
            keep_component_level_item_embeddings: bool. If true, keep P_x component-level
                embeddings in `self._mol_item_embeddings` for downstream applications.
            component_level_item_embeddings_dtype: torch.dtype. If set, the dtype
                to keep component-level item embeddings in. By default we use bfloat16.
        """
        super().__init__()

        self._mol_module: MoLSimilarity = mol_module
        self._item_embeddings: torch.Tensor = (
            item_embeddings
            if not flatten_item_ids_and_embeddings
            else item_embeddings.squeeze(0)
        )

        if keep_component_level_item_embeddings:
            self._mol_item_embeddings: torch.Tensor = (
                mol_module.get_item_component_embeddings(
                    (
                        self._item_embeddings.squeeze(0)
                        if not flatten_item_ids_and_embeddings
                        else self._item_embeddings
                    ),
                    decoupled_inference=True,
                )[0]  # (X, D) -> (X, P_X, D_P)
            ).to(component_level_item_embeddings_dtype)

        self._item_ids: torch.Tensor = (
            item_ids if not flatten_item_ids_and_embeddings else item_ids.squeeze(0)
        )

    @property
    def mol_module(self) -> MoLSimilarity:
        return self._mol_module


class MoLBruteForceTopK(MoLTopKModule):
    def __init__(
        self,
        mol_module: MoLSimilarity,
        item_embeddings: torch.Tensor,
        item_ids: torch.Tensor,
    ) -> None:
        super().__init__(
            mol_module=mol_module,
            item_embeddings=item_embeddings,
            item_ids=item_ids,
            flatten_item_ids_and_embeddings=False,
            keep_component_level_item_embeddings=False,
        )

    def forward(
        self,
        query_embeddings: torch.Tensor,
        k: int,
        sorted: bool = True,
        **kwargs,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            query_embeddings: (B, X, D) if mol_module._apply_query_embeddings_fn is True,
                (B, X, P_Q, D_P) otherwise.
            k: int. final top-k to return.
            sorted: bool. whether to sort final top-k results or not.
            **kwargs: Implementation-specific keys/values.

        Returns:
            Tuple of (top_k_scores x float, top_k_ids x int), both of shape (B, K,)
        """
        # (B, X,)
        all_logits, _ = self.mol_module(
            query_embeddings,
            self._item_embeddings,
            **kwargs,
        )
        top_k_logits, top_k_indices = torch.topk(
            all_logits,
            dim=1,
            k=k,
            sorted=sorted,
            largest=True,
        )  # (B, k,)
        return top_k_logits, self._item_ids.squeeze(0)[top_k_indices]

</content>

<content full_path="generative_recommenders/rails/indexing/candidate_index.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# pyre-unsafe

import abc
from typing import Tuple

import torch


class TopKModule(torch.nn.Module):
    @abc.abstractmethod
    def forward(
        self,
        query_embeddings: torch.Tensor,
        k: int,
        sorted: bool = True,
        **kwargs,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            query_embeddings: (B, X, ...). Implementation-specific.
            k: int. top k to return.
            sorted: bool.

        Returns:
            Tuple of (top_k_scores, top_k_ids), both of shape (B, K,)
        """
        pass

</content>

<content full_path="generative_recommenders/rails/indexing/mips_top_k.py">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# pyre-unsafe

from typing import Tuple

import torch

from generative_recommenders.rails.indexing.candidate_index import TopKModule


class MIPSTopKModule(TopKModule):
    def __init__(
        self,
        item_embeddings: torch.Tensor,
        item_ids: torch.Tensor,
    ) -> None:
        """
        Args:
            item_embeddings: (1, X, D)
            item_ids: (1, X,)
        """
        super().__init__()

        self._item_embeddings: torch.Tensor = item_embeddings
        self._item_ids: torch.Tensor = item_ids


class MIPSBruteForceTopK(MIPSTopKModule):
    def __init__(
        self,
        item_embeddings: torch.Tensor,
        item_ids: torch.Tensor,
    ) -> None:
        super().__init__(
            item_embeddings=item_embeddings,
            item_ids=item_ids,
        )
        del self._item_embeddings
        self._item_embeddings_t: torch.Tensor = item_embeddings.permute(
            2, 1, 0
        ).squeeze(2)

    def forward(
        self,
        query_embeddings: torch.Tensor,
        k: int,
        sorted: bool = True,
        **kwargs,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            query_embeddings: (B, ...). Implementation-specific.
            k: int. final top-k to return.
            sorted: bool. whether to sort final top-k results or not.

        Returns:
            Tuple of (top_k_scores x float, top_k_ids x int), both of shape (B, K,)
        """
        # (B, X,)
        all_logits = torch.mm(query_embeddings, self._item_embeddings_t)
        top_k_logits, top_k_indices = torch.topk(
            all_logits,
            dim=1,
            k=k,
            sorted=sorted,
            largest=True,
        )  # (B, k,)
        return top_k_logits, self._item_ids.squeeze(0)[top_k_indices]

</content>

<content full_path="configs/ml-20m/hstu-sampled-softmax-n128-large-final.gin">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Frozen config, validated on 04/12/2024.
# Based on HSTU-large results in
# Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations (https://arxiv.org/abs/2402.17152).
#
# Run this as:
# mkdir -p logs/ml-20m-l200/
# CUDA_VISIBLE_DEVICES=0 PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True python3 main.py --gin_config_file=configs/ml-20m/hstu-sampled-softmax-n128-large-final.gin --master_port=12345 2>&1 | tee logs/ml-20m-l200/hstu-sampled-softmax-n128-large-final.log

train_fn.dataset_name = "ml-20m"
train_fn.max_sequence_length = 200
train_fn.local_batch_size = 128

train_fn.main_module = "HSTU"
train_fn.dropout_rate = 0.2
train_fn.user_embedding_norm = "l2_norm"
train_fn.num_epochs = 101
train_fn.item_embedding_dim = 256

hstu_encoder.num_blocks = 16
hstu_encoder.num_heads = 8
hstu_encoder.dv = 32
hstu_encoder.dqk = 32
hstu_encoder.linear_dropout_rate = 0.2

train_fn.learning_rate = 1e-3
train_fn.weight_decay = 0
train_fn.num_warmup_steps = 0

train_fn.interaction_module_type = "DotProduct"
train_fn.top_k_method = "MIPSBruteForceTopK"

train_fn.loss_module = "SampledSoftmaxLoss"
train_fn.num_negatives = 128

train_fn.sampling_strategy = "local"
train_fn.temperature = 0.05
train_fn.item_l2_norm = True
train_fn.l2_norm_eps = 1e-6

train_fn.enable_tf32 = True

create_data_loader.prefetch_factor = 128
create_data_loader.num_workers = 8

</content>

<content full_path="configs/ml-20m/sasrec-sampled-softmax-n128-final.gin">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Frozen config, validated on 04/12/2024.
# Based on baseline settings in Revisiting Neural Retrieval on Accelerators (https://arxiv.org/abs/2306.04039, KDD'23).
#
# Run this as:
# mkdir -p logs/ml-20m-l200/
# CUDA_VISIBLE_DEVICES=0 python3 main.py --gin_config_file=configs/ml-20m/sasrec-sampled-softmax-n128-final.gin --master_port=12345 2>&1 | tee logs/ml-20m-l200/sasrec-sampled-softmax-n128-final.log

train_fn.dataset_name = "ml-20m"
train_fn.max_sequence_length = 200
train_fn.local_batch_size = 128

train_fn.main_module = "SASRec"
train_fn.dropout_rate = 0.2
train_fn.user_embedding_norm = "l2_norm"
train_fn.num_epochs = 101
train_fn.item_embedding_dim = 256

sasrec_encoder.num_blocks = 4
sasrec_encoder.num_heads = 4
sasrec_encoder.ffn_dropout_rate = 0.2
sasrec_encoder.ffn_hidden_dim = 256
sasrec_encoder.ffn_activation_fn = "relu"

train_fn.learning_rate = 1e-3
train_fn.weight_decay = 0
train_fn.num_warmup_steps = 0

train_fn.top_k_method = "MIPSBruteForceTopK"
train_fn.interaction_module_type = "DotProduct"

train_fn.loss_module = "SampledSoftmaxLoss"
train_fn.num_negatives = 128

train_fn.sampling_strategy = "local"
train_fn.temperature = 0.05
train_fn.item_l2_norm = True
train_fn.l2_norm_eps = 1e-6

train_fn.enable_tf32 = True

create_data_loader.prefetch_factor = 128
create_data_loader.num_workers = 8

</content>

<content full_path="configs/ml-20m/hstu-sampled-softmax-n128-final.gin">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Frozen config, validated on 04/12/2024.
# Based on HSTU results (w/ identical configurations as a SotA Transformer baseline) in
# Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations (https://arxiv.org/abs/2402.17152).
#
# Run this as:
# mkdir -p logs/ml-20m-l200/
# CUDA_VISIBLE_DEVICES=0 python3 main.py --gin_config_file=configs/ml-20m/hstu-sampled-softmax-n128-final.gin --master_port=12345 2>&1 | tee logs/ml-20m-l200/hstu-sampled-softmax-n128-final.log

train_fn.dataset_name = "ml-20m"
train_fn.max_sequence_length = 200
train_fn.local_batch_size = 128

train_fn.main_module = "HSTU"
train_fn.dropout_rate = 0.2
train_fn.user_embedding_norm = "l2_norm"
train_fn.num_epochs = 101
train_fn.item_embedding_dim = 256

hstu_encoder.num_blocks = 4
hstu_encoder.num_heads = 4
hstu_encoder.dv = 64
hstu_encoder.dqk = 64
hstu_encoder.linear_dropout_rate = 0.2

train_fn.learning_rate = 1e-3
train_fn.weight_decay = 0
train_fn.num_warmup_steps = 0

train_fn.interaction_module_type = "DotProduct"
train_fn.top_k_method = "MIPSBruteForceTopK"

train_fn.loss_module = "SampledSoftmaxLoss"
train_fn.num_negatives = 128

train_fn.sampling_strategy = "local"
train_fn.temperature = 0.05
train_fn.item_l2_norm = True
train_fn.l2_norm_eps = 1e-6

train_fn.enable_tf32 = True

create_data_loader.prefetch_factor = 128
create_data_loader.num_workers = 8

</content>

<content full_path="configs/ml-1m/hstu-sampled-softmax-n128-large-final.gin">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Frozen config, validated on 04/11/2024.
# Based on HSTU-large results in
# Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations (https://arxiv.org/abs/2402.17152).
#
# Run this as:
# mkdir -p logs/ml-1m-l200/
# CUDA_VISIBLE_DEVICES=1 python3 main.py --gin_config_file=configs/ml-1m/hstu-sampled-softmax-n128-large-final.gin --master_port=12346 2>&1 | tee logs/ml-1m-l200/hstu-sampled-softmax-n128-large-final.log

train_fn.dataset_name = "ml-1m"
train_fn.max_sequence_length = 200
train_fn.local_batch_size = 128

train_fn.main_module = "HSTU"
train_fn.dropout_rate = 0.2
train_fn.user_embedding_norm = "l2_norm"
train_fn.num_epochs = 101
train_fn.item_embedding_dim = 50

hstu_encoder.num_blocks = 8
hstu_encoder.num_heads = 2
hstu_encoder.dqk = 25
hstu_encoder.dv = 25
hstu_encoder.linear_dropout_rate = 0.2

train_fn.learning_rate = 1e-3
train_fn.weight_decay = 0
train_fn.num_warmup_steps = 0

train_fn.interaction_module_type = "DotProduct"
train_fn.top_k_method = "MIPSBruteForceTopK"

train_fn.loss_module = "SampledSoftmaxLoss"
train_fn.num_negatives = 128

train_fn.sampling_strategy = "local"
train_fn.temperature = 0.05
train_fn.item_l2_norm = True
train_fn.l2_norm_eps = 1e-6

train_fn.enable_tf32 = True

create_data_loader.prefetch_factor = 128
create_data_loader.num_workers = 8

</content>

<content full_path="configs/ml-1m/sasrec-sampled-softmax-n128-final.gin">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Frozen config, validated on 04/11/2024.
# Based on baseline settings in Revisiting Neural Retrieval on Accelerators (https://arxiv.org/abs/2306.04039, KDD'23).
#
# Run this as:
# mkdir -p logs/ml-1m-l200/
# CUDA_VISIBLE_DEVICES=0 python3 main.py --gin_config_file=configs/ml-1m/sasrec-sampled-softmax-n128-final.gin --master_port=12345 2>&1 | tee logs/ml-1m-l200/sasrec-sampled-softmax-n128-final.log

train_fn.dataset_name = "ml-1m"
train_fn.max_sequence_length = 200
train_fn.local_batch_size = 128

train_fn.main_module = "SASRec"
train_fn.dropout_rate = 0.2
train_fn.user_embedding_norm = "l2_norm"
train_fn.num_epochs = 101
train_fn.item_embedding_dim = 50

sasrec_encoder.num_blocks = 2
sasrec_encoder.num_heads = 1
sasrec_encoder.ffn_dropout_rate = 0.2
sasrec_encoder.ffn_hidden_dim = 50
sasrec_encoder.ffn_activation_fn = "relu"

train_fn.learning_rate = 1e-3
train_fn.weight_decay = 0
train_fn.num_warmup_steps = 0

train_fn.top_k_method = "MIPSBruteForceTopK"
train_fn.interaction_module_type = "DotProduct"

train_fn.loss_module = "SampledSoftmaxLoss"
train_fn.num_negatives = 128

train_fn.sampling_strategy = "local"
train_fn.temperature = 0.05
train_fn.item_l2_norm = True
train_fn.l2_norm_eps = 1e-6

train_fn.enable_tf32 = True

create_data_loader.prefetch_factor = 128
create_data_loader.num_workers = 8

</content>

<content full_path="configs/ml-1m/hstu-sampled-softmax-n128-final.gin">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Frozen config, validated on 04/11/2024.
# Based on HSTU results (w/ identical configurations as a SotA Transformer baseline) in
# Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations (https://arxiv.org/abs/2402.17152).
#
# Run this as:
# mkdir -p logs/ml-1m-l200/
# CUDA_VISIBLE_DEVICES=0 python3 main.py --gin_config_file=configs/ml-1m/hstu-sampled-softmax-n128-final.gin --master_port=12345 2>&1 | tee logs/ml-1m-l200/hstu-sampled-softmax-n128-final.log

train_fn.dataset_name = "ml-1m"
train_fn.max_sequence_length = 200
train_fn.local_batch_size = 128

train_fn.main_module = "HSTU"
train_fn.dropout_rate = 0.2
train_fn.user_embedding_norm = "l2_norm"
train_fn.num_epochs = 101
train_fn.item_embedding_dim = 50

hstu_encoder.num_blocks = 2
hstu_encoder.num_heads = 1
hstu_encoder.dqk = 50
hstu_encoder.dv = 50
hstu_encoder.linear_dropout_rate = 0.2

train_fn.learning_rate = 1e-3
train_fn.weight_decay = 0
train_fn.num_warmup_steps = 0

train_fn.interaction_module_type = "DotProduct"
train_fn.top_k_method = "MIPSBruteForceTopK"

train_fn.loss_module = "SampledSoftmaxLoss"
train_fn.num_negatives = 128

train_fn.sampling_strategy = "local"
train_fn.temperature = 0.05
train_fn.item_l2_norm = True
train_fn.l2_norm_eps = 1e-6

train_fn.enable_tf32 = True

create_data_loader.prefetch_factor = 128
create_data_loader.num_workers = 8

</content>

<content full_path="configs/amzn-books/hstu-sampled-softmax-n512-large-final.gin">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Frozen config, validated on 04/12/2024.
# Based on HSTU-large results in
# Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations (https://arxiv.org/abs/2402.17152).
#
# Run this as:
# mkdir -p logs/amzn-books-l50/
# CUDA_VISIBLE_DEVICES=1 python3 main.py --gin_config_file=configs/amzn-books/hstu-sampled-softmax-n512-large-final.gin --master_port=12346 2>&1 | tee logs/amzn-books-l50/hstu-sampled-softmax-n512-large-final2.log

train_fn.dataset_name = "amzn-books"
train_fn.max_sequence_length = 50
train_fn.local_batch_size = 128
train_fn.eval_batch_size = 128

train_fn.main_module = "HSTU"
train_fn.dropout_rate = 0.5
train_fn.user_embedding_norm = "l2_norm"
train_fn.item_embedding_dim = 64

hstu_encoder.num_blocks = 16
hstu_encoder.num_heads = 8
hstu_encoder.dv = 8
hstu_encoder.dqk = 8
hstu_encoder.linear_dropout_rate = 0.5

train_fn.eval_interval = 4000
train_fn.num_epochs = 201
train_fn.learning_rate = 1e-3
train_fn.weight_decay = 0
train_fn.num_warmup_steps = 0

train_fn.interaction_module_type = "DotProduct"
train_fn.top_k_method = "MIPSBruteForceTopK"

train_fn.loss_module = "SampledSoftmaxLoss"
train_fn.num_negatives = 512

train_fn.sampling_strategy = "local"
train_fn.temperature = 0.05
train_fn.item_l2_norm = True
train_fn.l2_norm_eps = 1e-6

train_fn.enable_tf32 = True
train_fn.full_eval_every_n = 5
train_fn.partial_eval_num_iters = 64

create_data_loader.prefetch_factor = 1024
create_data_loader.num_workers = 8

</content>

<content full_path="configs/amzn-books/sasrec-sampled-softmax-n512-final.gin">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Frozen config, validated on 04/12/2024.
# Based on baseline settings in Revisiting Neural Retrieval on Accelerators (https://arxiv.org/abs/2306.04039, KDD'23).
#
# Run this as:
# mkdir -p logs/amzn-books-l50/
# CUDA_VISIBLE_DEVICES=1 python3 main.py --gin_config_file=configs/amzn-books/sasrec-sampled-softmax-n512-final.gin  --master_port=12346 2>&1 | tee logs/amzn-books-l50/sasrec-sampled-softmax-n512-final.log

train_fn.dataset_name = "amzn-books"
train_fn.max_sequence_length = 50
train_fn.local_batch_size = 128
train_fn.eval_batch_size = 128

train_fn.main_module = "SASRec"
train_fn.dropout_rate = 0.5
train_fn.user_embedding_norm = "l2_norm"
train_fn.item_embedding_dim = 64

sasrec_encoder.num_blocks = 4
sasrec_encoder.num_heads = 4
sasrec_encoder.ffn_dropout_rate = 0.5
sasrec_encoder.ffn_hidden_dim = 64
sasrec_encoder.ffn_activation_fn = "relu"

train_fn.eval_interval = 4000
train_fn.num_epochs = 201
train_fn.learning_rate = 1e-3
train_fn.weight_decay = 0
train_fn.num_warmup_steps = 0

train_fn.save_ckpt_every_n = 10

train_fn.interaction_module_type = "DotProduct"
train_fn.top_k_method = "MIPSBruteForceTopK"

train_fn.loss_module = "SampledSoftmaxLoss"
train_fn.num_negatives = 512

train_fn.sampling_strategy = "local"
train_fn.temperature = 0.05
train_fn.item_l2_norm = True
train_fn.l2_norm_eps = 1e-6

train_fn.enable_tf32 = True
train_fn.full_eval_every_n = 5
train_fn.partial_eval_num_iters = 64

create_data_loader.prefetch_factor = 1024
create_data_loader.num_workers = 8

</content>

<content full_path="configs/amzn-books/hstu-sampled-softmax-n512-final.gin">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Frozen config, validated on 04/12/2024.
# Based on HSTU results (w/ identical configurations as a SotA Transformer baseline) in
# Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations (https://arxiv.org/abs/2402.17152).
#
# Run this as:
# mkdir -p logs/amzn-books-l50/
# CUDA_VISIBLE_DEVICES=1 python3 main.py --gin_config_file=configs/amzn-books/hstu-sampled-softmax-n512-final.gin --master_port=12346 2>&1 | tee logs/amzn-books-l50/hstu-sampled-softmax-n512-final.log

train_fn.dataset_name = "amzn-books"
train_fn.max_sequence_length = 50
train_fn.local_batch_size = 128
train_fn.eval_batch_size = 128

train_fn.main_module = "HSTU"
train_fn.dropout_rate = 0.5
train_fn.user_embedding_norm = "l2_norm"
train_fn.item_embedding_dim = 64

hstu_encoder.num_blocks = 4
hstu_encoder.num_heads = 4
hstu_encoder.dv = 16
hstu_encoder.dqk = 16
hstu_encoder.linear_dropout_rate = 0.5

train_fn.eval_interval = 4000
train_fn.num_epochs = 201
train_fn.learning_rate = 1e-3
train_fn.weight_decay = 0
train_fn.num_warmup_steps = 0

train_fn.interaction_module_type = "DotProduct"
train_fn.top_k_method = "MIPSBruteForceTopK"

train_fn.loss_module = "SampledSoftmaxLoss"
train_fn.num_negatives = 512

train_fn.sampling_strategy = "local"
train_fn.temperature = 0.05
train_fn.item_l2_norm = True
train_fn.l2_norm_eps = 1e-6

train_fn.enable_tf32 = True
train_fn.full_eval_every_n = 5
train_fn.partial_eval_num_iters = 64

create_data_loader.prefetch_factor = 1024
create_data_loader.num_workers = 8

</content>

<content full_path="configs/ml-3b/sasrec-sampled-softmax-n96-seqlen500-final.gin">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Run this as:
# mkdir -p logs/ml-3b-l500/
# CUDA_VISIBLE_DEVICES=0 python3 main.py --gin_config_file=configs/ml-3b/sasrec-sampled-softmax-n96-seqlen500-final.gin --master_port=12345 2>&1 | tee logs/ml-3b-l500/sasrec-sampled-softmax-n96-seqlen500-final.log

train_fn.dataset_name = "ml-3b"
train_fn.max_sequence_length = 500
train_fn.local_batch_size = 96
train_fn.eval_batch_size = 96

train_fn.main_module = "SASRec"
train_fn.dropout_rate = 0.2
train_fn.user_embedding_norm = "l2_norm"
train_fn.num_epochs = 100
train_fn.item_embedding_dim = 256

sasrec_encoder.num_blocks = 4
sasrec_encoder.num_heads = 4
sasrec_encoder.ffn_dropout_rate = 0.2
sasrec_encoder.ffn_hidden_dim = 256
sasrec_encoder.ffn_activation_fn = "relu"

train_fn.learning_rate = 1e-3
train_fn.weight_decay = 0
train_fn.num_warmup_steps = 0

train_fn.top_k_method = "MIPSBruteForceTopK"
train_fn.interaction_module_type = "DotProduct"

train_fn.loss_module = "SampledSoftmaxLoss"
train_fn.num_negatives = 128

train_fn.sampling_strategy = "local"
train_fn.temperature = 0.05
train_fn.item_l2_norm = True
train_fn.l2_norm_eps = 1e-6

train_fn.enable_tf32 = True

create_data_loader.prefetch_factor = 128
create_data_loader.num_workers = 8

</content>

<content full_path="configs/ml-3b/hstu-sampled-softmax-n96-seqlen500-final.gin">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Run this as:
# mkdir -p logs/ml-3b-l500/
# CUDA_VISIBLE_DEVICES=0 python3 main.py --gin_config_file=configs/ml-3b/hstu-sampled-softmax-n96-seqlen500-final.gin --master_port=12345 2>&1 | tee logs/ml-3b-l500/hstu-sampled-softmax-n96-seqlen500-final.log

train_fn.dataset_name = "ml-3b"
train_fn.max_sequence_length = 500
train_fn.local_batch_size = 96
train_fn.eval_batch_size = 96

train_fn.main_module = "HSTU"
train_fn.dropout_rate = 0.2
train_fn.user_embedding_norm = "l2_norm"
train_fn.num_epochs = 100
train_fn.item_embedding_dim = 256

hstu_encoder.num_blocks = 4
hstu_encoder.num_heads = 4
hstu_encoder.dv = 64
hstu_encoder.dqk = 64
hstu_encoder.linear_dropout_rate = 0.2

train_fn.learning_rate = 1e-3
train_fn.weight_decay = 0
train_fn.num_warmup_steps = 0

train_fn.interaction_module_type = "DotProduct"
train_fn.top_k_method = "MIPSBruteForceTopK"

train_fn.loss_module = "SampledSoftmaxLoss"
train_fn.num_negatives = 128

train_fn.sampling_strategy = "local"
train_fn.temperature = 0.05
train_fn.item_l2_norm = True
train_fn.l2_norm_eps = 1e-6

train_fn.enable_tf32 = True

create_data_loader.prefetch_factor = 128
create_data_loader.num_workers = 8

</content>

<content full_path="configs/ml-3b/hstu-sampled-softmax-n96-seqlen500-large-final.gin">
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Run this as:
# mkdir -p logs/ml-3b-l500/
# CUDA_VISIBLE_DEVICES=0 python3 main.py --gin_config_file=configs/ml-3b/hstu-sampled-softmax-n96-seqlen500-large-final.gin --master_port=12345 2>&1 | tee logs/ml-3b-l500/hstu-sampled-softmax-n96-seqlen500-large-final.log

train_fn.dataset_name = "ml-3b"
train_fn.max_sequence_length = 500
train_fn.local_batch_size = 96
train_fn.eval_batch_size = 96

train_fn.main_module = "HSTU"
train_fn.dropout_rate = 0.2
train_fn.user_embedding_norm = "l2_norm"
train_fn.num_epochs = 100
train_fn.item_embedding_dim = 256

hstu_encoder.num_blocks = 16
hstu_encoder.num_heads = 8
hstu_encoder.dv = 32
hstu_encoder.dqk = 32
hstu_encoder.linear_dropout_rate = 0.2

train_fn.learning_rate = 1e-3
train_fn.weight_decay = 0
train_fn.num_warmup_steps = 0

train_fn.interaction_module_type = "DotProduct"
train_fn.top_k_method = "MIPSBruteForceTopK"

train_fn.loss_module = "SampledSoftmaxLoss"
train_fn.num_negatives = 128

train_fn.sampling_strategy = "local"
train_fn.temperature = 0.05
train_fn.item_l2_norm = True
train_fn.l2_norm_eps = 1e-6

train_fn.enable_tf32 = True

create_data_loader.prefetch_factor = 128
create_data_loader.num_workers = 8

</content>

</repo-to-text>
